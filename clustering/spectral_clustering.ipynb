{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Spectral Clustering\n",
    "Here I will derive the mathematical basics of why does spectral clustering work. I will break them into four parts. The first three parts will lay the required groundwork for the mathematics behind spectral clustering. The final part will be piecing everything together and show that why that spectral clustering works as intended.\n",
    "\n",
    "## Part 1 - Vector Derivative of a Matrix\n",
    "Given matrices `A` and `x`, how to compute the following?\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial x} x^{T} A x\n",
    "$$\n",
    "\n",
    "For example:\n",
    "\n",
    "$$\n",
    "x = [x_{0}, x_{1}]\n",
    "$$\n",
    "\n",
    "$$\n",
    "A = \\begin{bmatrix} a_{00} & a_{01} \\\\ a_{10} & a_{11} \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Multiply them out and get the following expression:\n",
    "\n",
    "$$\n",
    "x^{T}Ax = a_{00}x_{0}^{2} + a_{01}x_{0}x_{1} + a_{10}x_{0}x_{1} + a_{11}x_{1}^{2}\n",
    "$$\n",
    "\n",
    "Think of taking derivative of `x.T * A * x` with respect to a vector as an element wise operation:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial}{\\partial \\vec{x}} x^{T}Ax &= \\begin{bmatrix} \\frac{\\partial}{\\partial x_{0}} \\\\ \\frac{\\partial}{\\partial x_{1}} \\end{bmatrix} x^{T}Ax \\\\\n",
    "&= \\begin{bmatrix}\n",
    "2a_{00}x_{0} + a_{01}x_{1} + a_{10}x_{1} \\\\\n",
    "a_{01}x_{0} + a_{10}x_{0} + 2a_{11}x_{1}\n",
    "\\end{bmatrix}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Which is equivalent to:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "2a_{00} & a_{01} + a_{10} \\\\\n",
    "a_{01} + a_{10} & 2a_{11}\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix} x_{0} \\\\ x_{1} \\end{bmatrix}\n",
    "= \\left( A + A^{T} \\right) \\vec{x}\n",
    "$$\n",
    "\n",
    "Thus, the answer is:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial x} x^{T} A x = \\left( A + A^{T} \\right) \\vec{x}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Part 2 - Lagrange Multiplier\n",
    "Lagrange multiplier is frequently used in classical mechanics to solve function optimization under constraints. Lagrangian mechanic is often used in non-relativistic quantum mechanics for particle physics, however that would require knowledge in path integral. In this section, I am sticking to the plain old Lagrange multipler to solve a simple constraint problem as an example. \n",
    "\n",
    "Suppose we want to minimize or maximize a function `f(x, y, z)` while subjecting to a constraint function `g(x, y, z) = k`. The easier example one can think of is that you want to a fence around your house, but the constraint is that you have limited materials from Home Depot. You want to maximize the area enclosed by your fence. Then you can use Lagrange Multiplier to perform the calculation and find the opitmal solution. \n",
    "\n",
    "### Formulation\n",
    "We define Lagrangian as follows:\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = f(x, y, z) - \\lambda\\left(g(x, y, z) - k\\right)\n",
    "$$\n",
    "\n",
    "The objective is to solve the following function, which I forgot what was the formal name for it:\n",
    "\n",
    "$$\n",
    "\\nabla_{x, y, z, \\lambda} \\mathcal{L}(x, y, z, \\lambda) = \\vec{0}\n",
    "$$\n",
    "\n",
    "We will have four equations and four unknowns.\n",
    "\n",
    "### Example\n",
    "Let's use a simple 2 dimensional example with only two variables, `x` and `y`:\n",
    "\n",
    "$$\n",
    "f(x, y) = 6x^{2} + 12y^{2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "g(x, y) = x + y = 90\n",
    "$$\n",
    "\n",
    "Then the Lagrangian for this example is:\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = 6x^{2} + 12y^{2} - \\lambda\\left(x + y - 90\\right)\n",
    "$$\n",
    "\n",
    "Compute gradient for our Lagrangian and set it to equal to zero:\n",
    "\n",
    "$$\n",
    "\\nabla_{x, y, \\lambda} = \\begin{bmatrix}\n",
    "12x - \\lambda \\\\\n",
    "24y - \\lambda \\\\\n",
    "90 - x - y\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "0 \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Then the solution is clear:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} x \\\\ y \\\\ \\lambda \\end{bmatrix}=\n",
    "\\begin{bmatrix} 60 \\\\ 30 \\\\ 720 \\end{bmatrix}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Part 3 - Minimize $x^{T}Ax$\n",
    "The objective here is to combine what we know from Part 1 and Part 2 to achieve the following:\n",
    "\n",
    "$$\n",
    "argmin_{\\vec{x}} \\; x^{T}Ax\n",
    "$$\n",
    "\n",
    "We want to minimize the expression `x * A.T * x` under the following constraints: \n",
    "\n",
    "$$\n",
    "\\text{x is normalized} \\quad x^{T}x = 1\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{symmetric} \\quad A = A^{T}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{A is positive semidefinite} \\quad x^{T}Ax \\geq 0\n",
    "$$\n",
    "\n",
    "Being positive semidefinite is an important quality, because if a matrix is definite or semidefinite positive, the vector, at which derivative of the expression is zero, has to be the solution for minimization. Now we have our constraints, we are ready to use Lagrange multiplier to minimize this expression.\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = x^{T}Ax - \\lambda\\left(x^{T}x - 1\\right)\n",
    "$$\n",
    "\n",
    "We will make an important assumption here, that `A` is a symmetric matrix. You will see that this is indeed the case later on. \n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial x} = 2Ax - 2\\lambda x = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\lambda} = 1 - x^{T}x = 0\n",
    "$$\n",
    "\n",
    "Now solve for `x` and I will begin to use the vector notation here in case we forget that `x` is a vector, and it has always been a vector. I omitted the vector symbol to type less LaTeX code on my end but I must include it here to illustrate a point:\n",
    "\n",
    "$$\n",
    "A\\vec{x} = \\lambda\\vec{x}\n",
    "$$\n",
    "\n",
    "The constraint equation gave us that\n",
    "\n",
    "$$\n",
    "x^{T}x = 1\n",
    "$$\n",
    "\n",
    "So what does this mean? It means that if you want to minimize the expression $x^{T}Ax$, `x` must be the eigenvectors of `A`! Here are couple important properties:\n",
    "\n",
    "* `A` is positive if all eigenvalues are positive.\n",
    "* `A` is semidefinite positive if all eigenvalues are either positive or zero.\n",
    "* All eigenvectors are the same size, they have a norm equal to 1.\n",
    "* The eigenvector corresponding to the smallest eigenvalue will give you the smallest possible value of $A\\vec{x}$\n",
    "* In converse, eigenvector corresponding to the biggest eigenvalue will give you the maximum of $A\\vec{x}$. However I am not 100% sure of this point, I need to run couple tests to verify it. \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
