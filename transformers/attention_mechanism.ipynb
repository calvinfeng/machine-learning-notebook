{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "edf4f3fe",
   "metadata": {},
   "source": [
    "# Attention Mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b3a292",
   "metadata": {},
   "source": [
    "## Additive Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc744f0",
   "metadata": {},
   "source": [
    "[Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473) is also known as the additive attention. Attention is an extension to encoder-decoder model which learns to align and translate jointly. Each time the model generates a word in a translation, it searches for a set of positions in a source sentence where the most relevant information is concentrated. The model then predicts a target word based on the context vectors associated with these source positions and all the previous generated target words.\n",
    "\n",
    "The models that came before this idea tried to encode the whole input sentence into a single fixed-length vector. The new attention mechanism encodes the input sentence into a sequence of vectors and chooses only a subset of these vectors adaptively during decoding phase."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046bd2cf",
   "metadata": {},
   "source": [
    "### RNN Encoder-Decoder\n",
    "\n",
    "Suppose we have a RNN based encoder-decoder network, the encoder reads the input sentence, a sequence of vectors\n",
    "$x = (x_1, x_2, ..., x_{T})$ into a context vector $c$. For every step in the encoding phase, the network generates a hidden state vector at step `t`.\n",
    "\n",
    "$$\n",
    "h_t = f\\,(x_t, h_{t-1})\n",
    "$$\n",
    "\n",
    "The context vector is then generated from the sequence of hidden states.\n",
    "\n",
    "$$\n",
    "c = q\\,({h_1, h_2, ..., h_T})\n",
    "$$\n",
    "\n",
    "$f$ and $q$ are some nonlinear functions that represent the network. The decoder is trained to predict the next word $y_t$ given the context vector and all the previously predicted words $(y_1, y_2, ..., y_{t-1})$. In other words, the decoder defines a probability over the translation $y$ by decomposing the joint probability into the ordered conditionals.\n",
    "\n",
    "$$\n",
    "p\\,(y) = \\prod_{t=1}^T p\\,(y_t \\mid y_1, y_2, ..., y_{t-1}, c)\n",
    "$$\n",
    "\n",
    "The $y$ represents the whole translated sentence. Each conditional probability is modeled as\n",
    "\n",
    "$$\n",
    "p\\,(y_t \\mid y_1, y_2, ..., y_{t-1}, c) = g\\,(y_{t-1}, s_t, c)\n",
    "$$\n",
    "\n",
    "where $s_t$ is the hidden state of the RNN network.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4af3fb",
   "metadata": {},
   "source": [
    "### Learning to Align\n",
    "\n",
    "The paper proposes a new model architecture where it uses a bi-directional RNN as an encoder/decoder. Each conditional probability is defined as:\n",
    "\n",
    "$$\n",
    "p\\,(y_i \\mid y_1, y_2, ..., y_{i-1}, x) = g\\,(y_{i-1}, s_{i}, c_{i})\n",
    "$$\n",
    "\n",
    "$s_i$ is the hidden state at time `i` which is computed by\n",
    "\n",
    "$$\n",
    "s_i = f\\,(s_{i-1}, y_{i-1}, c_i)\n",
    "$$\n",
    "\n",
    "Notice that the probability is conditioned on a **distinct** context vector $c_i$ for each target word output. The context vector depends on a sequence of annotations $(h_1, h_2, ..., h_T)$ to which an encoder maps the input sentence. Each annotation $h_i$ contains information about the whole input sequence with a strong focus on the parts surrounding the `i`th word of the input sequence. These are effectively the hidden states of the encoder at time step `t`.\n",
    "\n",
    "The context vector is a weighted sum of these annotations.\n",
    "\n",
    "$$\n",
    "c_i = \\Sigma_{j=1}^{T} \\alpha_{ij}h_j\n",
    "$$\n",
    "\n",
    "The weight $\\alpha_{ij}$ for each annotation $h_j$ is computed by a softmax function.\n",
    "\n",
    "$$\n",
    "\\alpha_{ij} = \\frac{\\text{exp}\\,(e_{ij})}{ \\Sigma_{k=1}^T \\text{exp}\\, (e_{ik})}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "e_{ij} = a\\,(s_{i-1}, h_j)\n",
    "$$\n",
    "\n",
    "The $a$ is called an alignment model which scores how well the inputs around position `j` and the output at position `i` match. $a$ is modeled as a feedforward fully connected neural network which is jointly trained with all the other components of the system.\n",
    "\n",
    "> So why use a bi-directional RNN?\n",
    "\n",
    "Annotation of each word should summarize not only the preceding words, but also the following words. The bi-directional allows such information to propagate from left to right and right to left.\n",
    "\n",
    "The forward RNN reads the input sequences from left to right and calculates a forward hidden state sequence. The backward RNN reads the input sequences from right to left and calculates a backward hidden state sequence. The annotation for each word is the concatenation of the forward and backward hidden states.\n",
    "\n",
    "> Why is it additive?\n",
    "\n",
    "Because the context vector is a weighted sum of annotations.\n",
    "\n",
    "![Bahdanau Attention Model](assets/bahdanau.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8acf7d",
   "metadata": {},
   "source": [
    "### Encoder and Decoder in PyTorch\n",
    "\n",
    "Source: [Practical PyTorch: Translation with a Sequence to Sequence Network and Attention](https://github.com/spro/practical-pytorch/blob/master/seq2seq-translation/seq2seq-translation.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82421e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_layers=1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers)\n",
    "        \n",
    "    def forward(self, word_inputs, hidden):\n",
    "        # Note: we run this all at once (over the whole input sequence)\n",
    "        seq_len = len(word_inputs)\n",
    "        embedded = self.embedding(word_inputs).view(seq_len, 1, -1)\n",
    "        output, hidden = self.gru(embedded, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        hidden = Variable(torch.zeros(self.n_layers, 1, self.hidden_size))\n",
    "        if USE_CUDA: hidden = hidden.cuda()\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9971b04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, n_layers=1, dropout_p=0.1):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        \n",
    "        # Define parameters\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # Define layers\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.attn = GeneralAttn(hidden_size) # Attention unit should compute the softmaxed weights aka alpha.\n",
    "        self.gru = nn.GRU(hidden_size * 2, hidden_size, n_layers, dropout=dropout_p)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, word_input, last_hidden, encoder_outputs):\n",
    "        # Note that we will only be running forward for a single decoder time step, but will use all\n",
    "        # encoder outputs\n",
    "        \n",
    "        # Get the embedding of the current input word (last output word)\n",
    "        word_embedded = self.embedding(word_input).view(1, 1, -1) # S=1 x B x N\n",
    "        word_embedded = self.dropout(word_embedded)\n",
    "        \n",
    "        # Calculate attention weights and apply to encoder outputs (which are the hidden states h)\n",
    "        attn_weights = self.attn(last_hidden[-1], encoder_outputs) \n",
    "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1)) # B x 1 x N \n",
    "        \n",
    "        # Combine embedded input word and attended context, run through RNN\n",
    "        rnn_input = torch.cat((word_embedded, context), 2)\n",
    "        output, hidden = self.gru(rnn_input, last_hidden)\n",
    "        \n",
    "        # Final output layer\n",
    "        output = output.squeeze(0) # B x N\n",
    "        output = F.log_softmax(self.out(torch.cat((output, context), 1)))\n",
    "        \n",
    "        # Return final output, hidden state, and attention weights (for visualization)\n",
    "        return output, hidden, attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46025c89",
   "metadata": {},
   "source": [
    "## Self-Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60ecc7b",
   "metadata": {},
   "source": [
    "The issue with the attention mechanism above is that the attention is applied after the hidden states are generated from running the sequence through a RNN. The sequential processing becomes a bottleneck for scalability because there is no known way to parallelize the hidden state computation in a RNN. It has to go from left to right or right to left. \n",
    "\n",
    "In the seminar paper [Attention is All You Need](https://arxiv.org/abs/1706.03762), the authors proposed that we don't need RNN to encode sequences, instead attention is all we need to compute a powerful and rich representation of an input sequence.\n",
    "\n",
    "> Self attention, sometimes called intra-attention is an attention mechanism relating different positions of a\n",
    "  single sequence in order to compute a representation of the sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0302a1b7",
   "metadata": {},
   "source": [
    "### RNN Attention vs Self Attention\n",
    "\n",
    "Using example from Andrew Ng's lecture, we have a French sentence:\n",
    "\n",
    "> Jane viste l'Afrique en septembre\n",
    "\n",
    "The sentence has length `T = 5` and each word has embedding $x^t$. \n",
    "\n",
    "- $x^1$ = `embedding('Jane')`\n",
    "- $x^2$ = `embedding('viste')`\n",
    "- ...\n",
    "- $x^5$ = `embedding('septembre')`\n",
    "\n",
    "\n",
    "The origianl attention has the form\n",
    "\n",
    "$$\n",
    "\\alpha_{ij} = \\frac{\\text{exp}\\,(e_{ij})}{ \\Sigma_{k=1}^T \\text{exp}\\, (e_{ik})}\n",
    "$$\n",
    "\n",
    "The `i` context vector is created via looping over `j`, where $j \\in [1, \\text{T}]$.\n",
    "\n",
    "$$\n",
    "c_i = \\Sigma_{j=1}^{T} \\alpha_{ij}h_j = \\Sigma_{j=1}^{T} \\frac{\\text{exp}\\,(e_{ij})}{ \\Sigma_{k=1}^T \\text{exp}\\, (e_{ik})} h_j\n",
    "$$\n",
    "\n",
    "![Self Attention](assets/self-attention.png)\n",
    "\n",
    "The self attention has a similar but different form that allows it to compute attention for all words in parallel. Let $Q$ denotes a matrix of query vectors $q^1, q^2, ..., q^5$, $K$ denotes a matrix of key vectors $k^1, k^2, ..., k^5$, and $V$ denotes a matrix of value vectors $v^1, v^2, ..., v^5$. We have queries, keys, and values.\n",
    "\n",
    "$$\n",
    "\\text{Attention}\\,(Q, K, V) = \\text{softmax}\\,(\\frac{QK^T}{\\sqrt{d_k}})V\n",
    "$$\n",
    "\n",
    "The $d_k$ is known as a scaling factor for the softmax. If we take a closer look at a single query and see what it is doing, then we have the following.\n",
    "\n",
    "$$\n",
    "A_{self}\\,(q, K, V) = \\Sigma_i \\frac{ \\text{exp}\\,(q \\cdot k^i)}{ \\Sigma_j \\text{exp}\\,(q \\cdot k^j)} v^i\n",
    "$$\n",
    "\n",
    "The queries are computed and learned by a weight parameter, same applies for keys and values.\n",
    "\n",
    "$$\n",
    "Q = W^Q X \\\\\n",
    "K = W^K X \\\\\n",
    "V = W^V X\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8adc25a4",
   "metadata": {},
   "source": [
    "### How to interpret Q, K, V?\n",
    "\n",
    "Queries, keys, and values are analogous to retrieval system. **Queries** are what we search, **keys** are the titles, attributes, and descriptions of items, and **values** are the actual items themselves.\n",
    "\n",
    "In additive attention (Bahdanau), the context vector is a weighted average of values, where values are the hidden states from the encoder RNN.  The problem is that for every output token or word in the decoder sequence, we have to go through every input token or word in the encoder sequence, resulting in `O(N*M)` runtime. \n",
    "\n",
    "In self attention, encoder sequence and decoder sequence are projected to a common space. These projection vectors are query (for decoder) and key (for encoder). The values are supposedly the hidden states but we don't have a RNN here so it's an analogy. The weight parameters perform such projection. The runtime becomes linear."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c832175",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n",
    "For example, we want to translate the French sentence into English sentence. When the decoder looks at `viste` as the query, it asks the question, \"what are the keys that are most relevantt to this word?\". The dot product and softmax will produce the probabilities to indicate which keys are most relevant. In this case, `viste` is a verb, it needs a subject and an object. The subject key will be `Jane` and the object key will be `l'Afrique`. The values are then the encoded vectors for `Jane` and `l'Afrique`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28438de9",
   "metadata": {},
   "source": [
    "## Multi-Head Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac62923",
   "metadata": {},
   "source": [
    "Each time we compute a self-attention, we call it a `head`. Suppose we use 8 heads in our architecture `H = 8`, we have a multi-head attention. Each head is responsible for asking a specific contextual question, like \"What does Africa have to do with Jane\", \"When is Jane going to visit Africa\", \"Is Africa a place or a name?\" and etc...\n",
    "\n",
    "$$\n",
    "\\text{head}_i = \\text{Attention}\\,(QW_i^Q, KW_i^K, VW_i^V) \\\\ \n",
    "\\text{MultiHead}\\,(Q, K, V) = \\text{Concat}\\,(\\text{head}_1, ..., \\text{head}_8) W^O\n",
    "$$\n",
    "\n",
    "The $W^O$ is another weight parameter for the final output.\n",
    "\n",
    "![Multi Head](assets/multi-head.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
