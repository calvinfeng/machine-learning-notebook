{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d05b3a88-7079-4c90-a5db-378d9b207ffa",
   "metadata": {},
   "source": [
    "# Backpropagation Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1dcad56-52f3-46cd-9f66-628fa1b16e42",
   "metadata": {},
   "source": [
    "## Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5afc47f5-dab0-46c0-8791-449b4842b05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.layers import Softmax, Conv2D, Dense, Input\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ecf671f-ec6e-4854-8b69-49e02af7b7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = Softmax()\n",
    "\n",
    "\n",
    "def my_softmax(y):\n",
    "    z = np.sum(np.exp(y), axis=1, keepdims=True)\n",
    "    log_probs = y - np.log(z)\n",
    "    return np.exp(log_probs)\n",
    "\n",
    "def my_better_softmax(y):\n",
    "    shifted_logits = y - np.max(y, axis=1, keepdims=True)\n",
    "    z = np.sum(np.exp(shifted_logits), axis=1, keepdims=True)\n",
    "    log_probs = shifted_logits - np.log(z)\n",
    "    return np.exp(log_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f56944-e243-449c-abc0-edab19f686bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0. 1. 0.]\n",
      " [0. 1. 0.]], shape=(2, 3), dtype=float32)\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "[[0. 1. 0.]\n",
      " [0. 1. 0.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_358000/838545724.py:5: RuntimeWarning: overflow encountered in exp\n",
      "  z = np.sum(np.exp(y), axis=1, keepdims=True)\n"
     ]
    }
   ],
   "source": [
    "y = np.array([[-200, 100, 5], [-200, 100, -5]]).astype(np.float64) * 1000\n",
    "print(softmax(y))\n",
    "print(my_softmax(y))\n",
    "print(my_better_softmax(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66bfd41-7758-4f1c-a17c-396fef5d0642",
   "metadata": {},
   "source": [
    "## Cross Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a47866c-f61e-4179-abdd-22950ad594b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6040394277047364"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = np.array([[0.3, 0.4, 0.3], [0.5, 0.5, 0.0], [1.0, 0.0, 0.0]])\n",
    "y_pred = np.array([[0.3, 0.4, 0.29], [0.5, 0.5, 0.01], [0.99, 0.01, 0.01]])\n",
    "cce = CategoricalCrossentropy()\n",
    "cce(y_true, y_pred).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33796e79-8292-4c35-9b22-3096e205d70e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6007559857537915"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def my_cross_entropy(y_true, y_pred):\n",
    "    log_probs = np.log(y_pred)\n",
    "    return -np.sum(y_true * log_probs) / y_true.shape[0]\n",
    "\n",
    "my_cross_entropy(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98052ef6-0e47-4707-b150-a869ae4eec63",
   "metadata": {},
   "source": [
    "## Combine Everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d697dbf-d6f1-4b3f-ad10-a138b34d7ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from layers.dense import Dense\n",
    "from layers.activations import ReLU\n",
    "from IPython.display import Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22a3d45c-fe2c-4013-b816-e75cb0a5fe6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>pre { line-height: 125%; }\n",
       "td.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\n",
       "td.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       ".output_html .hll { background-color: #ffffcc }\n",
       ".output_html { background: #f8f8f8; }\n",
       ".output_html .c { color: #3D7B7B; font-style: italic } /* Comment */\n",
       ".output_html .err { border: 1px solid #FF0000 } /* Error */\n",
       ".output_html .k { color: #008000; font-weight: bold } /* Keyword */\n",
       ".output_html .o { color: #666666 } /* Operator */\n",
       ".output_html .ch { color: #3D7B7B; font-style: italic } /* Comment.Hashbang */\n",
       ".output_html .cm { color: #3D7B7B; font-style: italic } /* Comment.Multiline */\n",
       ".output_html .cp { color: #9C6500 } /* Comment.Preproc */\n",
       ".output_html .cpf { color: #3D7B7B; font-style: italic } /* Comment.PreprocFile */\n",
       ".output_html .c1 { color: #3D7B7B; font-style: italic } /* Comment.Single */\n",
       ".output_html .cs { color: #3D7B7B; font-style: italic } /* Comment.Special */\n",
       ".output_html .gd { color: #A00000 } /* Generic.Deleted */\n",
       ".output_html .ge { font-style: italic } /* Generic.Emph */\n",
       ".output_html .gr { color: #E40000 } /* Generic.Error */\n",
       ".output_html .gh { color: #000080; font-weight: bold } /* Generic.Heading */\n",
       ".output_html .gi { color: #008400 } /* Generic.Inserted */\n",
       ".output_html .go { color: #717171 } /* Generic.Output */\n",
       ".output_html .gp { color: #000080; font-weight: bold } /* Generic.Prompt */\n",
       ".output_html .gs { font-weight: bold } /* Generic.Strong */\n",
       ".output_html .gu { color: #800080; font-weight: bold } /* Generic.Subheading */\n",
       ".output_html .gt { color: #0044DD } /* Generic.Traceback */\n",
       ".output_html .kc { color: #008000; font-weight: bold } /* Keyword.Constant */\n",
       ".output_html .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */\n",
       ".output_html .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */\n",
       ".output_html .kp { color: #008000 } /* Keyword.Pseudo */\n",
       ".output_html .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */\n",
       ".output_html .kt { color: #B00040 } /* Keyword.Type */\n",
       ".output_html .m { color: #666666 } /* Literal.Number */\n",
       ".output_html .s { color: #BA2121 } /* Literal.String */\n",
       ".output_html .na { color: #687822 } /* Name.Attribute */\n",
       ".output_html .nb { color: #008000 } /* Name.Builtin */\n",
       ".output_html .nc { color: #0000FF; font-weight: bold } /* Name.Class */\n",
       ".output_html .no { color: #880000 } /* Name.Constant */\n",
       ".output_html .nd { color: #AA22FF } /* Name.Decorator */\n",
       ".output_html .ni { color: #717171; font-weight: bold } /* Name.Entity */\n",
       ".output_html .ne { color: #CB3F38; font-weight: bold } /* Name.Exception */\n",
       ".output_html .nf { color: #0000FF } /* Name.Function */\n",
       ".output_html .nl { color: #767600 } /* Name.Label */\n",
       ".output_html .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */\n",
       ".output_html .nt { color: #008000; font-weight: bold } /* Name.Tag */\n",
       ".output_html .nv { color: #19177C } /* Name.Variable */\n",
       ".output_html .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */\n",
       ".output_html .w { color: #bbbbbb } /* Text.Whitespace */\n",
       ".output_html .mb { color: #666666 } /* Literal.Number.Bin */\n",
       ".output_html .mf { color: #666666 } /* Literal.Number.Float */\n",
       ".output_html .mh { color: #666666 } /* Literal.Number.Hex */\n",
       ".output_html .mi { color: #666666 } /* Literal.Number.Integer */\n",
       ".output_html .mo { color: #666666 } /* Literal.Number.Oct */\n",
       ".output_html .sa { color: #BA2121 } /* Literal.String.Affix */\n",
       ".output_html .sb { color: #BA2121 } /* Literal.String.Backtick */\n",
       ".output_html .sc { color: #BA2121 } /* Literal.String.Char */\n",
       ".output_html .dl { color: #BA2121 } /* Literal.String.Delimiter */\n",
       ".output_html .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */\n",
       ".output_html .s2 { color: #BA2121 } /* Literal.String.Double */\n",
       ".output_html .se { color: #AA5D1F; font-weight: bold } /* Literal.String.Escape */\n",
       ".output_html .sh { color: #BA2121 } /* Literal.String.Heredoc */\n",
       ".output_html .si { color: #A45A77; font-weight: bold } /* Literal.String.Interpol */\n",
       ".output_html .sx { color: #008000 } /* Literal.String.Other */\n",
       ".output_html .sr { color: #A45A77 } /* Literal.String.Regex */\n",
       ".output_html .s1 { color: #BA2121 } /* Literal.String.Single */\n",
       ".output_html .ss { color: #19177C } /* Literal.String.Symbol */\n",
       ".output_html .bp { color: #008000 } /* Name.Builtin.Pseudo */\n",
       ".output_html .fm { color: #0000FF } /* Name.Function.Magic */\n",
       ".output_html .vc { color: #19177C } /* Name.Variable.Class */\n",
       ".output_html .vg { color: #19177C } /* Name.Variable.Global */\n",
       ".output_html .vi { color: #19177C } /* Name.Variable.Instance */\n",
       ".output_html .vm { color: #19177C } /* Name.Variable.Magic */\n",
       ".output_html .il { color: #666666 } /* Literal.Number.Integer.Long */</style><div class=\"highlight\"><pre><span></span><span class=\"kn\">import</span> <span class=\"nn\">numpy</span> <span class=\"k\">as</span> <span class=\"nn\">np</span>\n",
       "\n",
       "\n",
       "<span class=\"k\">class</span> <span class=\"nc\">Dense</span><span class=\"p\">:</span>\n",
       "    <span class=\"k\">def</span> <span class=\"fm\">__init__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">):</span>\n",
       "        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>\n",
       "        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">w</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>\n",
       "        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">b</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>\n",
       "\n",
       "    <span class=\"k\">def</span> <span class=\"fm\">__call__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">w</span><span class=\"p\">,</span> <span class=\"n\">b</span><span class=\"p\">):</span>\n",
       "        <span class=\"sd\">&quot;&quot;&quot;Perform forward propagation</span>\n",
       "\n",
       "<span class=\"sd\">        Args:</span>\n",
       "<span class=\"sd\">            x (np.ndarray): Input</span>\n",
       "<span class=\"sd\">            w (np.ndarray): Kernel weights</span>\n",
       "<span class=\"sd\">            b (np.ndarray): Biases</span>\n",
       "\n",
       "<span class=\"sd\">        Returns:</span>\n",
       "<span class=\"sd\">            np.ndarray: Output</span>\n",
       "<span class=\"sd\">        &quot;&quot;&quot;</span>\n",
       "        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">x</span>\n",
       "        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">w</span> <span class=\"o\">=</span> <span class=\"n\">w</span>\n",
       "        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">b</span> <span class=\"o\">=</span> <span class=\"n\">b</span>\n",
       "        <span class=\"k\">return</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">matmul</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">w</span><span class=\"p\">)</span> <span class=\"o\">+</span> <span class=\"n\">b</span>\n",
       "\n",
       "    <span class=\"k\">def</span> <span class=\"nf\">gradients</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">grad_out</span><span class=\"p\">):</span>\n",
       "        <span class=\"sd\">&quot;&quot;&quot;Perform back propagation and return gradients with respect to upstream loss function.</span>\n",
       "\n",
       "<span class=\"sd\">        Args:</span>\n",
       "<span class=\"sd\">            grad_out (np.ndarray): Gradient of loss with respect to output.</span>\n",
       "\n",
       "<span class=\"sd\">        Returns:</span>\n",
       "<span class=\"sd\">            np.ndarray: Gradient of loss with respect to x</span>\n",
       "<span class=\"sd\">            np.ndarray: Gradient of loss with respect to w</span>\n",
       "<span class=\"sd\">            np.ndarray: Gradient of loss with respect to b</span>\n",
       "<span class=\"sd\">        &quot;&quot;&quot;</span>\n",
       "        <span class=\"k\">if</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">x</span> <span class=\"ow\">is</span> <span class=\"kc\">None</span><span class=\"p\">:</span>\n",
       "            <span class=\"k\">raise</span> <span class=\"ne\">ValueError</span><span class=\"p\">(</span><span class=\"s2\">&quot;layer must be forward propagated first&quot;</span><span class=\"p\">)</span>\n",
       "        \n",
       "        <span class=\"n\">grad_x</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">matmul</span><span class=\"p\">(</span><span class=\"n\">grad_out</span><span class=\"p\">,</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">w</span><span class=\"o\">.</span><span class=\"n\">T</span><span class=\"p\">)</span>\n",
       "        <span class=\"n\">grad_w</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">matmul</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">T</span><span class=\"p\">,</span> <span class=\"n\">grad_out</span><span class=\"p\">)</span>\n",
       "        <span class=\"n\">grad_b</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">sum</span><span class=\"p\">(</span><span class=\"n\">grad_out</span><span class=\"p\">,</span> <span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n",
       "        <span class=\"k\">return</span> <span class=\"n\">grad_x</span><span class=\"p\">,</span> <span class=\"n\">grad_w</span><span class=\"p\">,</span> <span class=\"n\">grad_b</span>\n",
       "</pre></div>\n"
      ],
      "text/latex": [
       "\\begin{Verbatim}[commandchars=\\\\\\{\\}]\n",
       "\\PY{k+kn}{import} \\PY{n+nn}{numpy} \\PY{k}{as} \\PY{n+nn}{np}\n",
       "\n",
       "\n",
       "\\PY{k}{class} \\PY{n+nc}{Dense}\\PY{p}{:}\n",
       "    \\PY{k}{def} \\PY{n+nf+fm}{\\PYZus{}\\PYZus{}init\\PYZus{}\\PYZus{}}\\PY{p}{(}\\PY{n+nb+bp}{self}\\PY{p}{)}\\PY{p}{:}\n",
       "        \\PY{n+nb+bp}{self}\\PY{o}{.}\\PY{n}{x} \\PY{o}{=} \\PY{k+kc}{None}\n",
       "        \\PY{n+nb+bp}{self}\\PY{o}{.}\\PY{n}{w} \\PY{o}{=} \\PY{k+kc}{None}\n",
       "        \\PY{n+nb+bp}{self}\\PY{o}{.}\\PY{n}{b} \\PY{o}{=} \\PY{k+kc}{None}\n",
       "\n",
       "    \\PY{k}{def} \\PY{n+nf+fm}{\\PYZus{}\\PYZus{}call\\PYZus{}\\PYZus{}}\\PY{p}{(}\\PY{n+nb+bp}{self}\\PY{p}{,} \\PY{n}{x}\\PY{p}{,} \\PY{n}{w}\\PY{p}{,} \\PY{n}{b}\\PY{p}{)}\\PY{p}{:}\n",
       "        \\PY{l+s+sd}{\\PYZdq{}\\PYZdq{}\\PYZdq{}Perform forward propagation}\n",
       "\n",
       "\\PY{l+s+sd}{        Args:}\n",
       "\\PY{l+s+sd}{            x (np.ndarray): Input}\n",
       "\\PY{l+s+sd}{            w (np.ndarray): Kernel weights}\n",
       "\\PY{l+s+sd}{            b (np.ndarray): Biases}\n",
       "\n",
       "\\PY{l+s+sd}{        Returns:}\n",
       "\\PY{l+s+sd}{            np.ndarray: Output}\n",
       "\\PY{l+s+sd}{        \\PYZdq{}\\PYZdq{}\\PYZdq{}}\n",
       "        \\PY{n+nb+bp}{self}\\PY{o}{.}\\PY{n}{x} \\PY{o}{=} \\PY{n}{x}\n",
       "        \\PY{n+nb+bp}{self}\\PY{o}{.}\\PY{n}{w} \\PY{o}{=} \\PY{n}{w}\n",
       "        \\PY{n+nb+bp}{self}\\PY{o}{.}\\PY{n}{b} \\PY{o}{=} \\PY{n}{b}\n",
       "        \\PY{k}{return} \\PY{n}{np}\\PY{o}{.}\\PY{n}{matmul}\\PY{p}{(}\\PY{n}{x}\\PY{p}{,} \\PY{n}{w}\\PY{p}{)} \\PY{o}{+} \\PY{n}{b}\n",
       "\n",
       "    \\PY{k}{def} \\PY{n+nf}{gradients}\\PY{p}{(}\\PY{n+nb+bp}{self}\\PY{p}{,} \\PY{n}{grad\\PYZus{}out}\\PY{p}{)}\\PY{p}{:}\n",
       "        \\PY{l+s+sd}{\\PYZdq{}\\PYZdq{}\\PYZdq{}Perform back propagation and return gradients with respect to upstream loss function.}\n",
       "\n",
       "\\PY{l+s+sd}{        Args:}\n",
       "\\PY{l+s+sd}{            grad\\PYZus{}out (np.ndarray): Gradient of loss with respect to output.}\n",
       "\n",
       "\\PY{l+s+sd}{        Returns:}\n",
       "\\PY{l+s+sd}{            np.ndarray: Gradient of loss with respect to x}\n",
       "\\PY{l+s+sd}{            np.ndarray: Gradient of loss with respect to w}\n",
       "\\PY{l+s+sd}{            np.ndarray: Gradient of loss with respect to b}\n",
       "\\PY{l+s+sd}{        \\PYZdq{}\\PYZdq{}\\PYZdq{}}\n",
       "        \\PY{k}{if} \\PY{n+nb+bp}{self}\\PY{o}{.}\\PY{n}{x} \\PY{o+ow}{is} \\PY{k+kc}{None}\\PY{p}{:}\n",
       "            \\PY{k}{raise} \\PY{n+ne}{ValueError}\\PY{p}{(}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{layer must be forward propagated first}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{)}\n",
       "        \n",
       "        \\PY{n}{grad\\PYZus{}x} \\PY{o}{=} \\PY{n}{np}\\PY{o}{.}\\PY{n}{matmul}\\PY{p}{(}\\PY{n}{grad\\PYZus{}out}\\PY{p}{,} \\PY{n+nb+bp}{self}\\PY{o}{.}\\PY{n}{w}\\PY{o}{.}\\PY{n}{T}\\PY{p}{)}\n",
       "        \\PY{n}{grad\\PYZus{}w} \\PY{o}{=} \\PY{n}{np}\\PY{o}{.}\\PY{n}{matmul}\\PY{p}{(}\\PY{n+nb+bp}{self}\\PY{o}{.}\\PY{n}{x}\\PY{o}{.}\\PY{n}{T}\\PY{p}{,} \\PY{n}{grad\\PYZus{}out}\\PY{p}{)}\n",
       "        \\PY{n}{grad\\PYZus{}b} \\PY{o}{=} \\PY{n}{np}\\PY{o}{.}\\PY{n}{sum}\\PY{p}{(}\\PY{n}{grad\\PYZus{}out}\\PY{p}{,} \\PY{n}{axis}\\PY{o}{=}\\PY{l+m+mi}{0}\\PY{p}{)}\n",
       "        \\PY{k}{return} \\PY{n}{grad\\PYZus{}x}\\PY{p}{,} \\PY{n}{grad\\PYZus{}w}\\PY{p}{,} \\PY{n}{grad\\PYZus{}b}\n",
       "\\end{Verbatim}\n"
      ],
      "text/plain": [
       "import numpy as np\n",
       "\n",
       "\n",
       "class Dense:\n",
       "    def __init__(self):\n",
       "        self.x = None\n",
       "        self.w = None\n",
       "        self.b = None\n",
       "\n",
       "    def __call__(self, x, w, b):\n",
       "        \"\"\"Perform forward propagation\n",
       "\n",
       "        Args:\n",
       "            x (np.ndarray): Input\n",
       "            w (np.ndarray): Kernel weights\n",
       "            b (np.ndarray): Biases\n",
       "\n",
       "        Returns:\n",
       "            np.ndarray: Output\n",
       "        \"\"\"\n",
       "        self.x = x\n",
       "        self.w = w\n",
       "        self.b = b\n",
       "        return np.matmul(x, w) + b\n",
       "\n",
       "    def gradients(self, grad_out):\n",
       "        \"\"\"Perform back propagation and return gradients with respect to upstream loss function.\n",
       "\n",
       "        Args:\n",
       "            grad_out (np.ndarray): Gradient of loss with respect to output.\n",
       "\n",
       "        Returns:\n",
       "            np.ndarray: Gradient of loss with respect to x\n",
       "            np.ndarray: Gradient of loss with respect to w\n",
       "            np.ndarray: Gradient of loss with respect to b\n",
       "        \"\"\"\n",
       "        if self.x is None:\n",
       "            raise ValueError(\"layer must be forward propagated first\")\n",
       "        \n",
       "        grad_x = np.matmul(grad_out, self.w.T)\n",
       "        grad_w = np.matmul(self.x.T, grad_out)\n",
       "        grad_b = np.sum(grad_out, axis=0)\n",
       "        return grad_x, grad_w, grad_b"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Code(filename='layers/dense.py', language='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0aa62a4-36d4-4eb9-aebd-f783b8726809",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
