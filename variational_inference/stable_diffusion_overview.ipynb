{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92ffdbad",
   "metadata": {},
   "source": [
    "# Stable Diffusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bcf8aba",
   "metadata": {},
   "source": [
    "## What is Diffusion?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb356f7a",
   "metadata": {},
   "source": [
    "This idea came from diffusion in physics, the mixing of substances caused by particle motions. Substances diffuse in another until a homogenouous mixture is achieved. For example, hot water mixes with cool water, temperature gradient will drive particle motion until temperature is identical everywhere in the mixture.\n",
    "\n",
    "![Diffusion](https://upload.wikimedia.org/wikipedia/commons/thumb/1/12/Diffusion.svg/1200px-Diffusion.svg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a98067",
   "metadata": {},
   "source": [
    "We take the same idea and apply to machine learning models. Instead of diffusing particles, we diffuse structure of data. The equilibrium state is achieved when structure of data is completely destroyed, i.e. turning a piece of data into random noise. Obviously destroying data isn't particular useful, but the opposite is very useful, i.e. generating data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6443cd61",
   "metadata": {},
   "source": [
    "## Physical Intuition of Diffusion Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd901cda",
   "metadata": {},
   "source": [
    "This idea first appeared in [Deep Unsupervised Learning using Nonequilibrium Thermodynamics](https://arxiv.org/pdf/1503.03585.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e67eef",
   "metadata": {},
   "source": [
    "### Observation 1: Diffusion Destroys Structure\n",
    "\n",
    "Imagine that we have a dye diffusing in a bottle of water. A drop of dye will follow Brownian motion and diffuse itself in the water. Let the dye density represent probability density. The goal of a diffusion model is to learn the structure of probability density.\n",
    "\n",
    "It's impossible to learn the actual process of diffusion but we can approximate it. We know the boundary conditions, i.e. data distribution (dye) will eventually reach uniform distribution (dye uniformly coloring the water)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0fff49",
   "metadata": {},
   "source": [
    "### Observation 2: Microscopic Diffusion is Time Reversible\n",
    "\n",
    "If we zoom into the microscopic process of diffusion, the time moving forward and backward is identical, just like Quantum Mechanical processes. We cannot distinguish the before and after state of such operation. Positional update of each dye molecule is sampled from small Gaussian in either time flow direction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122846df",
   "metadata": {},
   "source": [
    "### Conjecture\n",
    "\n",
    "We can learn the reversal of diffusion process by estimating the function for mean and covariance of each step in the reverse diffusion process (transition rates for discrete state spaces.) The reverse diffusion process is going to form the model of our data.\n",
    "\n",
    "If I have a `(200, 200, 3)` image, I have a positional vector in `200*200*3`  dimensional space. It is diffusing through this `200*200*3` space and makes an incremental update in position every time step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939d5e7e",
   "metadata": {},
   "source": [
    "## Forward Diffusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f804fb",
   "metadata": {},
   "source": [
    "We will use a Markov process with Gaussian transition to describe the forward diffusion process. Markov means that current stand is only dependent on previous state. Gaussian transition means that we move from one state to next state by sampling from a Gaussian distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0df25b8",
   "metadata": {},
   "source": [
    "Let's denote q  as the forward diffusion process.\n",
    "\n",
    "$$\n",
    "q\\,(x^t \\mid x^{t-1}) = \\mathbb{N}\\left[x^t\\,;x^{t-1}\\sqrt{1 - \\beta_t}\\,,I_{\\text{identity}}\\beta_t\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8532266d",
   "metadata": {},
   "source": [
    "This is saying that we will produce a distribution of `x[t]` using `x[t-1]`. The beta term is the diffusion strength. The matrix `I` is an identity matrix. It represents the covariance matrix with beta as the covariance in `x` space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffee00d3",
   "metadata": {},
   "source": [
    "Consider we have an image of shape `(64, 64, 3)`, my `x` is a vector of 12288 values. This is assuming that I treat each color channel as a dimension. My diffusion process is applying Gaussian on this vector using the mean `x * sqrt(1 - beta)` and variance `beta`.\n",
    "\n",
    "```py\n",
    "import numpy as np\n",
    "\n",
    "x_0 = np.random.randn(64, 64, 3)\n",
    "x_0 = img.reshape(-1)\n",
    "beta = np.random.uniform(0, 1, 64*64*3)\n",
    "mean = x_0 * np.sqrt(1 - beta)\n",
    "\n",
    "# Diffusion\n",
    "x_1 = np.random.normal(mean, beta)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd31313d",
   "metadata": {},
   "source": [
    "## Backward Diffusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b21bd3",
   "metadata": {},
   "source": [
    "We can describe same process in reverse using the same functional form with different parameters, i.e. the mean and variance. This is the key idea of why diffusion works in the first place."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d576e09",
   "metadata": {},
   "source": [
    "$$\n",
    "p\\,(x^{t-1} \\mid x^t) = \\mathbb{N}\\left[x^{t-1}\\,; f_{\\mu}(x^t, t)\\,,f_{\\sigma}(x^t, t)\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc2fd40",
   "metadata": {},
   "source": [
    "This is saying, given some noisy data or image in this case, we can reverse the noise process using the same Gaussian functional form but with different mean and variance at each time step for each dimension or color in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85592fa",
   "metadata": {},
   "source": [
    "Our model needs to learn the mean and the covariance of the Gaussian from time step to time step. We are turning the problem of building a density model into the problem of learning functions for the mean and the covariance of a sequence of Gaussians. This is basically a supervised regression problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3695828",
   "metadata": {},
   "source": [
    "## Diffusion Training Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbdea011",
   "metadata": {},
   "source": [
    "The math is nearly identical to that of **variational autoencoder**. We start from the premise of maixmizing the log likelihood of `p` such that it will approximate the true distribution of data. Due to the lack of closed form solution for the posterior, we have to do a couple math tricks like Jensen's inequality to arrive at an **evidence lower bound** for the loss function. I will skip the derivation steps here. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f169128",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{argmin}_{f_\\mu, f_\\sigma} = \\mathbb{E}\\left[D_{KL}\\left(q\\,(x^{t-1}\\mid x^t, x^0) || p\\,(x^{t-1} \\mid x^t) \\right)\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006d4424",
   "metadata": {},
   "source": [
    "In summary, we want to learn the mean and covariance functions of the diffusion process such that we can minimize the expected value of KL divergence of the two distributions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44908411",
   "metadata": {},
   "source": [
    "## Stable Diffusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d33333",
   "metadata": {},
   "source": [
    "Suppose I have a handwritten digit classifier, I feed handwritten digit images to it. The classifier will predict a probability for the given image. For a noisy image, I will get a very low probability. For a clear digit image, I will get a very high probability.\n",
    "\n",
    "Now I ask, what is the gradient of probability with respect to each pixel? What if I update each pixel of my noisy image such that it yields the greatest probability to be classified as a digit?\n",
    "\n",
    "> This idea is very similar to generative adversarial network but diffusion can be seen as a more guided approach \n",
    "> to the same idea.\n",
    "\n",
    "I actually re-frame the problem such that I don't even need a classifier. I train a model that accepts a noisy image as input and produces noises as output. I subtract the noise from the noisy input and I now get the original image I expected. This is the core idea of stable diffusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b627ef0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
