{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0915a290-a59d-42a8-8ac1-f6c6306af5a2",
   "metadata": {},
   "source": [
    "# KL Divergence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9449575-abf8-47f9-b63d-edf2256a78df",
   "metadata": {},
   "source": [
    "We want to quantify the difference between probability distributions for a given random variable. For example, we'd be interested in the difference between an actual and observed probability distribution.\n",
    "\n",
    "> Kullback-Leibler divergence calculates a score that measures the divergence of one probability distribution from another."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab778b4-e117-4c29-bd5c-38455bd1a7ea",
   "metadata": {},
   "source": [
    "## Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a44eb9-d785-49ae-a697-023cb930f2a5",
   "metadata": {},
   "source": [
    "The KL divergence between two distributions `Q` and `P` is often stated as follows.\n",
    "\n",
    "$$\n",
    "\\text{KL}(P \\| Q)\n",
    "$$\n",
    "\n",
    "The operator indicates `P`'s divergence from `Q`, which is not the same as `Q`'s divergence from `P`. Thus, the operation is not commutative.\n",
    "\n",
    "$$\n",
    "\\text{KL}(P \\| Q) \\neq \\text{KL}(Q \\| P)\n",
    "$$\n",
    "\n",
    "KL divergence can be calculated as the negative sum of probability of each event in `P` multiplied by the log of the probability of the event in `Q` over the probability of the event in `P` .\n",
    "\n",
    "$$\n",
    "\\text{KL}(P \\| Q) = \\Sigma_{x \\in X} P(x) \\cdot log\\left( \\frac{P(x}{Q(x)} \\right)\n",
    "$$\n",
    "\n",
    "If there is no divergence at all, the log term becomes 0 and the sum is 0. The log can be base-2 to give units in bits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8525c347-76c2-44b8-804b-9f2e1c8924b9",
   "metadata": {},
   "source": [
    "If we are attempting to approximate an unknown probability distribution, then the target probability distribution from data is P  and Q  is our approximation of the distribution. **In this case, the KL divergence summarizes the number of additional bits required to represent an event from the random variable.** The better our approximation, the less additional information is required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85aedeb-1816-4a39-af66-9b79ef52d290",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b368fe8-c242-442e-8954-909099de6cd7",
   "metadata": {},
   "source": [
    "Consider a random variable like drawing a colored marble from a bag. We have 3 colors and two different probability distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62cdc0cd-7322-459b-91b2-a06daaa512c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "events = ['red', 'green', 'blue']\n",
    "p = [0.10, 0.40, 0.50]\n",
    "q = [0.80, 0.15, 0.05]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3a9e98-5b77-4356-9d77-ae7e4d90a344",
   "metadata": {},
   "source": [
    "We can calculate the KL divergence using formula above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "142286b2-e471-4195-879d-511204695d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def kl_divergence(p, q):\n",
    "    div = 0\n",
    "    for i in range(3):\n",
    "        div += p[i] * np.log2(p[i]/q[i])\n",
    "    return div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6044cee-7fda-477c-8158-507311a4a3cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KL(P||Q) 1.9269790471552186 bits\n",
      "KL(Q||P) 2.0216479703638055 bits\n"
     ]
    }
   ],
   "source": [
    "print(\"KL(P||Q)\", kl_divergence(p, q), \"bits\")\n",
    "print(\"KL(Q||P)\", kl_divergence(q, p), \"bits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5084c683-cc04-455f-ae01-f37b2a9c2279",
   "metadata": {},
   "source": [
    "## Intuition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ebf5fca-d9f4-42c2-9cfb-00b706182a28",
   "metadata": {},
   "source": [
    "[Intuitively Understanding the KL Divergence](https://www.youtube.com/watch?v=SxGYPqCgJWM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2689a750-45b0-46fb-9adf-ac31b11f54b6",
   "metadata": {},
   "source": [
    "If I have two coins, one fair and one weighted, KL divergence tells me how much information do I need to distinguish the weighted coin from the fair coin."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
