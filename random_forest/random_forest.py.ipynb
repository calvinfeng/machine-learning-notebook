{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest\n",
    "Random forest is a popular ensemble machine learning technique. Essentially it uses a batch of decision tree and bootstrap aggregation (*bagging*) to reduce variance. A single decision tree leads to high bias. A forest of decision tree will lead to high variance. The bagging technique will address the variance problem.\n",
    "\n",
    "We can build a decision tree easily using `sklearn` and achieve >80% accuracy on MNIST dataset using all pixel values as features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.87539999999999996"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import tree\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "N, H, W = x_train.shape\n",
    "x = x_train.reshape((N,H*W)).astype('float') / 255\n",
    "y = to_categorical(y_train, num_classes=10)\n",
    "\n",
    "model = tree.DecisionTreeClassifier()\n",
    "model.fit(x, y)\n",
    "\n",
    "N, H, W = x_test.shape\n",
    "x = x_test.reshape((N,H*W)).astype('float') / 255\n",
    "y = to_categorical(y_test, num_classes=10)\n",
    "\n",
    "model.score(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forest outperforms decision tree by having 100 tree. Obviously, it will take a longer time to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.90229999999999999"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import ensemble\n",
    "\n",
    "N, H, W = x_train.shape\n",
    "x = x_train.reshape((N,H*W)).astype('float') / 255\n",
    "y = to_categorical(y_train, num_classes=10)\n",
    "\n",
    "model = ensemble.RandomForestClassifier(n_estimators=100)\n",
    "model.fit(x, y)\n",
    "\n",
    "N, H, W = x_test.shape\n",
    "x = x_test.reshape((N,H*W)).astype('float') / 255\n",
    "y = to_categorical(y_test, num_classes=10)\n",
    "\n",
    "model.score(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Construction\n",
    "### Tree Node\n",
    "There are two types of tree node in a decision tree. **Prediction node(s)** are leaf nodes of the tree. **Decidsion node(s)** are the parent nodes of the tree. Most decision tree implementations use binary tree, i.e. every decision node will split into two branches at most. Now the question is, how do we split the data at every decision node? The essence of splitting comes down to reduce impurity. We try to split until the impurity is zero, i.e. all data are homogeneous, unless there is a maximum depth restriction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create couple test data as if they are from a CSV File\n",
    "header = ['color', 'diameter', 'label']\n",
    "\n",
    "training_data = [\n",
    "    ['Green', 3, 'Apple'],\n",
    "    ['Yellow', 3, 'Apple'],\n",
    "    ['Red', 1, 'Grape'],\n",
    "    ['Red', 1, 'Grape'],\n",
    "    ['Yellow', 3, 'Lemon']\n",
    "]\n",
    "\n",
    "testing_data = [\n",
    "    ['Green', 3, 'Apple'],\n",
    "    ['Yellow', 4, 'Apple'],\n",
    "    ['Red', 2, 'Grape'],\n",
    "    ['Red', 1, 'Grape'],\n",
    "    ['Yellow', 3, 'Lemon']\n",
    "]\n",
    "\n",
    "def class_counts(rows):\n",
    "    \"\"\"Counts the number of each type of example in a dataset.\n",
    "    \"\"\"\n",
    "    counts = dict()\n",
    "    for row in rows:\n",
    "        label = row[-1]\n",
    "        if label not in counts:\n",
    "            counts[label] = 0\n",
    "        \n",
    "        counts[label] += 1\n",
    "    \n",
    "    return counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CART Gini Index\n",
    "The definition of **pure** is that if we select two items from a pure population, the probaility of them being of same class has to be one. Gini index is a measurement of how impure a population is, ranging from 0 being pure to 1 being impure. \n",
    "\n",
    "$$\n",
    "G = 1 - \\Sigma_{i}^{C} P(i)^{2}\n",
    "$$\n",
    "\n",
    "The procedure to benchmark a branching decision is.\n",
    "\n",
    "1. Calculate Gini index for left and right sub-node. \n",
    "2. Use weighted average on the two indices to decide what is the impurity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subpar split: 0.400000\n",
      "Optimal split: 0.166667\n"
     ]
    }
   ],
   "source": [
    "def gini(data_rows):\n",
    "    counts = class_counts(data_rows)\n",
    "    impurity = 1\n",
    "    for label in counts:\n",
    "        prob = counts[label] / float(len(data_rows))\n",
    "        impurity -= prob**2\n",
    "    \n",
    "    return impurity\n",
    "\n",
    "# Impurity should be 0\n",
    "left_branch_gini = gini([['Apple'], ['Apple'], ['Apple']])\n",
    "\n",
    "# Impurity should be high\n",
    "right_branch_gini = gini([['Apple'], ['Orange'], ['Banana'], ['Apple'], ['Orange']])\n",
    "\n",
    "# Using weighted average to compute total purity score\n",
    "print 'Subpar split: %f' % (left_branch_gini * (3.0/8) + right_branch_gini * (5.0/8))\n",
    "\n",
    "# Lower the impurity, the better split is\n",
    "left_branch_gini = gini([['Apple'], ['Apple'], ['Apple'], ['Apple'], ['Apple']])\n",
    "right_branch_gini = gini([['Orange'], ['Banana'], ['Orange']])\n",
    "print 'Optimal split: %f' % (left_branch_gini * (5.0/8) + right_branch_gini * (3.0/8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ID3\n",
    "The core algorithm for building decision trees is called ID3. This algorithm eploys a top-down, greedy search through the space of possible branches with no backtracking. ID3 uses *entropy* and *information gain* to construct a decision tree.\n",
    "\n",
    "Entropy is defined as follows:\n",
    "\n",
    "$$\n",
    "E = \\Sigma_{i = 1}^{C} -P(i) * log_{2} P(i)\n",
    "$$\n",
    "\n",
    "The procedure to benchmark a branching decision is.\n",
    "\n",
    "1. Calculate the entropy before the split happens.\n",
    "2. Calculate the entropy for left and right sub-branch. \n",
    "3. Using the prior entropy and weighted sum of the sub-entropies, we can come up with information gain.\n",
    "\n",
    "$$\n",
    "IG = E_{0} - \\Sigma_{i}^{2} P(i)E_{i}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subpar split: 0.240931\n",
      "Optimal split: 0.661563\n"
     ]
    }
   ],
   "source": [
    "def entropy(data_rows):\n",
    "    counts = class_counts(data_rows)\n",
    "    entropy = 0\n",
    "    for label in counts:\n",
    "        prob = counts[label] / float(len(data_rows))\n",
    "        entropy += -1 * prob * np.log(prob)\n",
    "    \n",
    "    return entropy\n",
    "\n",
    "\n",
    "def info_gain(partitions):\n",
    "    combined = []\n",
    "    for part in partitions:\n",
    "        combined += part\n",
    "    \n",
    "    gain = entropy(combined)\n",
    "    for part in partitions:\n",
    "        prob = float(len(part)) / len(combined)\n",
    "        gain -= prob * entropy(part)\n",
    "        \n",
    "    return gain\n",
    "    \n",
    "\n",
    "left_part = [['Apple'], ['Apple'], ['Apple']]\n",
    "right_part = [['Apple'], ['Orange'], ['Banana'], ['Apple'], ['Orange']]\n",
    "print 'Subpar split: %f' % info_gain([left_part, right_part])\n",
    "\n",
    "# The more information gain, the better split is.\n",
    "left_part = [['Apple'], ['Apple'], ['Apple'], ['Apple'], ['Apple']]\n",
    "right_part = [['Orange'], ['Banana'], ['Orange']]\n",
    "print 'Optimal split: %f' % info_gain([left_part, right_part])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
