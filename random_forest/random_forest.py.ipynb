{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest\n",
    "Random forest is a popular ensemble machine learning technique. Essentially it uses a batch of decision tree and bootstrap aggregation (*bagging*) to reduce variance. A single decision tree leads to high bias and low variance. A forest of decision tree will lead to high variance with low bias. The bagging technique will address the variance problem.\n",
    "\n",
    "We can build a decision tree easily using `sklearn` and achieve >80% accuracy on MNIST dataset using all pixel values as features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.87729999999999997"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import tree\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "N, H, W = x_train.shape\n",
    "x = x_train.reshape((N,H*W)).astype('float') / 255\n",
    "y = to_categorical(y_train, num_classes=10)\n",
    "\n",
    "model = tree.DecisionTreeClassifier()\n",
    "model.fit(x, y)\n",
    "\n",
    "N, H, W = x_test.shape\n",
    "x = x_test.reshape((N,H*W)).astype('float') / 255\n",
    "y = to_categorical(y_test, num_classes=10)\n",
    "\n",
    "model.score(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forest outperforms decision tree by having 100 tree. Obviously, it will take a longer time to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.90400000000000003"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import ensemble\n",
    "\n",
    "N, H, W = x_train.shape\n",
    "x = x_train.reshape((N,H*W)).astype('float') / 255\n",
    "y = to_categorical(y_train, num_classes=10)\n",
    "\n",
    "model = ensemble.RandomForestClassifier(n_estimators=100)\n",
    "model.fit(x, y)\n",
    "\n",
    "N, H, W = x_test.shape\n",
    "x = x_test.reshape((N,H*W)).astype('float') / 255\n",
    "y = to_categorical(y_test, num_classes=10)\n",
    "\n",
    "model.score(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree\n",
    "A decision tree is very intuitive. It is a direct representation of how we make decision on a day-to-day basis. Here I borrow an example from Christopher Gray. Suppose that I am deciding whether to accept a job offer. \n",
    "\n",
    "![job offer](./assets/accept_job.png)\n",
    "\n",
    "I ask myself a series of questions, e.g. is the salary above $50,000? If no, I can immediately decline the offer. If yes, then I will ask myself another question, does it require me longer than 1 hour of commute time? If yes, then I will immediately decline the offer. If not, then I will ask another question again. This process goes on until there is no more questions to ask.\n",
    "\n",
    "### Tree Node\n",
    "There are two types of tree node in a decision tree. \n",
    "\n",
    "#### Prediction node(s)\n",
    "These are the are leaf nodes of the tree. They are the final answer to the ultimate question, i.e. should I accept this job offer?\n",
    "\n",
    "#### Decision node(s)\n",
    "These are the parent nodes of the tree. They represent a question or a rule that splits the into two different outcome/branches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to Split\n",
    "Now the question is, how do we know when to branch at each node and when to stop branching? Also, how do we know what question to ask for the splitting? Well, the essence of splitting comes down to reducing impurity. In the training phase, we are given a set of data. \n",
    "\n",
    "For example, let's say we have a basket of fruits, which contains apples, grapes, and lemons. Each of them has a set of attributes or features, like color and diameter.\n",
    "\n",
    "| Color  | Diameter | Fruit |\n",
    "|--------|----------|-------|\n",
    "| Green  | 3        | Apple |\n",
    "| Red    | 3        | Apple |\n",
    "| Green  | 1        | Grape |\n",
    "| Purple | 1        | Grape |\n",
    "| Yellow | 3        | Lemon |\n",
    "\n",
    "At every **decision node**, we ask a question to split the data into two batches. For example, a question we may ask is, *is it red?* For all the fruits that are red will be grouped together, and the rest will be put in another group. \n",
    "\n",
    "| Color  | Diameter | Fruit | Batch |\n",
    "|--------|----------|-------|-------|\n",
    "| Green  | 3        | Apple | 0     |\n",
    "| Red    | 3        | Apple | 0     |\n",
    "| Green  | 1        | Grape | 1     |\n",
    "| Purple | 1        | Grape | 1     |\n",
    "| Yellow | 3        | Lemon | 0     |\n",
    "\n",
    "Now the splitting is clear. We have 2 apples and 1 lemon in one group, while we have grapes in another group. We will try to split until the impurity is zero, i.e. all data are homogeneous, unless there is a maximum depth restriction. In this case, the grape group has an impurity of zero because everything is grape in that group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CART Gini Index\n",
    "The definition of **pure** is that if we select two items from a pure population, the probaility of them being of same class has to be one. Gini index is a measurement of how impure a population is, ranging from 0 being pure to 1 being impure. \n",
    "\n",
    "$$\n",
    "G = 1 - \\Sigma_{i}^{C} P(i)^{2}\n",
    "$$\n",
    "\n",
    "The procedure to benchmark a branching decision is.\n",
    "\n",
    "1. Calculate Gini index for left and right sub-node. \n",
    "2. Use weighted average on the two indices to decide what is the impurity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subpar split: 0.400000\n",
      "Optimal split: 0.166667\n"
     ]
    }
   ],
   "source": [
    "def class_counts(rows):\n",
    "    \"\"\"Counts the number of each type of example in a dataset.\n",
    "    \"\"\"\n",
    "    counts = dict()\n",
    "    for row in rows:\n",
    "        label = row[-1]\n",
    "        if label not in counts:\n",
    "            counts[label] = 0\n",
    "        \n",
    "        counts[label] += 1\n",
    "    \n",
    "    return counts\n",
    "\n",
    "def gini(data_rows):\n",
    "    counts = class_counts(data_rows)\n",
    "    impurity = 1\n",
    "    for label in counts:\n",
    "        prob = counts[label] / float(len(data_rows))\n",
    "        impurity -= prob**2\n",
    "    \n",
    "    return impurity\n",
    "\n",
    "# Impurity should be 0\n",
    "left_branch_gini = gini([['Apple'], ['Apple'], ['Apple']])\n",
    "\n",
    "# Impurity should be high\n",
    "right_branch_gini = gini([['Apple'], ['Orange'], ['Banana'], ['Apple'], ['Orange']])\n",
    "\n",
    "# Using weighted average to compute total purity score\n",
    "print 'Subpar split: %f' % (left_branch_gini * (3.0/8) + right_branch_gini * (5.0/8))\n",
    "\n",
    "# Lower the impurity, the better split is\n",
    "left_branch_gini = gini([['Apple'], ['Apple'], ['Apple'], ['Apple'], ['Apple']])\n",
    "right_branch_gini = gini([['Orange'], ['Banana'], ['Orange']])\n",
    "print 'Optimal split: %f' % (left_branch_gini * (5.0/8) + right_branch_gini * (3.0/8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ID3\n",
    "The core algorithm for building decision trees is called ID3. This algorithm eploys a top-down, greedy search through the space of possible branches with no backtracking. ID3 uses *entropy* and *information gain* to construct a decision tree.\n",
    "\n",
    "Entropy is defined as follows:\n",
    "\n",
    "$$\n",
    "E = \\Sigma_{i = 1}^{C} -P(i) * log_{2} P(i)\n",
    "$$\n",
    "\n",
    "The procedure to benchmark a branching decision is.\n",
    "\n",
    "1. Calculate the entropy before the split happens.\n",
    "2. Calculate the entropy for left and right sub-branch. \n",
    "3. Using the prior entropy and weighted sum of the sub-entropies, we can come up with information gain.\n",
    "\n",
    "$$\n",
    "IG = E_{0} - \\Sigma_{i}^{2} P(i)E_{i}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subpar split: 0.240931\n",
      "Optimal split: 0.661563\n"
     ]
    }
   ],
   "source": [
    "def entropy(data_rows):\n",
    "    counts = class_counts(data_rows)\n",
    "    entropy = 0\n",
    "    for label in counts:\n",
    "        prob = counts[label] / float(len(data_rows))\n",
    "        entropy += -1 * prob * np.log(prob)\n",
    "    \n",
    "    return entropy\n",
    "\n",
    "\n",
    "def info_gain(partitions):\n",
    "    combined = []\n",
    "    for part in partitions:\n",
    "        combined += part\n",
    "    \n",
    "    gain = entropy(combined)\n",
    "    for part in partitions:\n",
    "        prob = float(len(part)) / len(combined)\n",
    "        gain -= prob * entropy(part)\n",
    "        \n",
    "    return gain\n",
    "    \n",
    "\n",
    "left_part = [['Apple'], ['Apple'], ['Apple']]\n",
    "right_part = [['Apple'], ['Orange'], ['Banana'], ['Apple'], ['Orange']]\n",
    "print 'Subpar split: %f' % info_gain([left_part, right_part])\n",
    "\n",
    "# The more information gain, the better split is.\n",
    "left_part = [['Apple'], ['Apple'], ['Apple'], ['Apple'], ['Apple']]\n",
    "right_part = [['Orange'], ['Banana'], ['Orange']]\n",
    "print 'Optimal split: %f' % info_gain([left_part, right_part])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "```\n",
    "TBD\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "As the name implies, a random forest is simply a collection of *randomly* generated decision tree. They are not exactly randomly generated, they are randomly assigned features to perform the splitting process described above. We specify how many tree we want through the parameter `n_estimators` in `sklearn`. There are many optional arguments to the constructor of `RandomForestClassifier`. They are hyperparameters for tuning the performance of the forest. I will briefly talk about few of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Put Everything Together\n",
    "First, create a helper function for loading iris species data from CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "\n",
    "def load_data(csv_path):\n",
    "    col_names = None\n",
    "    data = []\n",
    "\n",
    "    with open(csv_path, 'rb') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "\n",
    "        # Extract column names from header\n",
    "        header = reader.next()\n",
    "        col_names = header[1:len(header)]\n",
    "\n",
    "        for row in reader:\n",
    "            data_row = [float(el) for el in row[1:len(row)-1]] + [row[-1]]\n",
    "            data.append(data_row)\n",
    "\n",
    "    return col_names, data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then load the data and shuffle them because the original data set is sorted by iris species. We want to do a 80:20 split on the data set to get a training set and a validation set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected Iris-virginica and predicted Iris-virginica\n",
      "Expected Iris-virginica and predicted Iris-virginica\n",
      "Expected Iris-versicolor and predicted Iris-versicolor\n",
      "Expected Iris-virginica and predicted Iris-virginica\n",
      "Expected Iris-versicolor and predicted Iris-versicolor\n",
      "Expected Iris-versicolor and predicted Iris-versicolor\n",
      "Expected Iris-virginica and predicted Iris-virginica\n",
      "Expected Iris-setosa and predicted Iris-setosa\n",
      "Expected Iris-versicolor and predicted Iris-versicolor\n",
      "Expected Iris-setosa and predicted Iris-setosa\n",
      "Expected Iris-virginica and predicted Iris-versicolor\n",
      "Expected Iris-versicolor and predicted Iris-versicolor\n",
      "Expected Iris-virginica and predicted Iris-versicolor\n",
      "Expected Iris-setosa and predicted Iris-setosa\n",
      "Expected Iris-virginica and predicted Iris-virginica\n",
      "Expected Iris-virginica and predicted Iris-virginica\n",
      "Expected Iris-setosa and predicted Iris-setosa\n",
      "Expected Iris-virginica and predicted Iris-virginica\n",
      "Expected Iris-virginica and predicted Iris-virginica\n",
      "Expected Iris-setosa and predicted Iris-setosa\n",
      "Expected Iris-versicolor and predicted Iris-versicolor\n",
      "Expected Iris-setosa and predicted Iris-setosa\n",
      "Expected Iris-setosa and predicted Iris-setosa\n",
      "Expected Iris-virginica and predicted Iris-virginica\n",
      "Expected Iris-virginica and predicted Iris-virginica\n",
      "Expected Iris-setosa and predicted Iris-setosa\n",
      "Expected Iris-versicolor and predicted Iris-versicolor\n",
      "Expected Iris-setosa and predicted Iris-setosa\n",
      "Expected Iris-virginica and predicted Iris-virginica\n",
      "Expected Iris-setosa and predicted Iris-setosa\n"
     ]
    }
   ],
   "source": [
    "from forest import DecisionTree, RandomForest\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "IRIS_MAP = {\n",
    "    'Iris-setosa': 0,\n",
    "    'Iris-versicolor': 1,\n",
    "    'Iris-virginica': 2\n",
    "}\n",
    "\n",
    "# Load and shuffle the data\n",
    "col_names, data = load_data('./datasets/iris.csv')\n",
    "np.random.shuffle(data)\n",
    "\n",
    "# Use a 80:20 split for training and validation\n",
    "N = len(data)\n",
    "training_data = data[:int(0.80*N)]\n",
    "validation_data = data[int(0.80*N):]\n",
    "\n",
    "forest = RandomForest(n_samples=10, n_features=3)\n",
    "forest.fit(col_names, training_data)\n",
    "\n",
    "for row in validation_data:\n",
    "    print 'Expected %s and predicted %s' % (row[-1], forest.predict(row))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
