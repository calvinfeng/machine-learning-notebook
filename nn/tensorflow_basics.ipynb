{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Basics\n",
    "## Numpy vs TF\n",
    "Start with a simple architecture to outline how strikingly similar are numpy and tensorflow.\n",
    "\n",
    "![Computational Graph](./assets/tf_comp_graph.png)\n",
    "\n",
    "The following is forward propagation and gradient computation in numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient x: [[ 0.76103773  0.12167502  0.44386323  0.33367433]\n",
      " [ 1.49407907 -0.20515826  0.3130677  -0.85409574]\n",
      " [-2.55298982  0.6536186   0.8644362  -0.74216502]]\n",
      "Graident y: [[ 1.76405235  0.40015721  0.97873798  2.2408932 ]\n",
      " [ 1.86755799 -0.97727788  0.95008842 -0.15135721]\n",
      " [-0.10321885  0.4105985   0.14404357  1.45427351]]\n",
      "Gradient z: [[1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "N, D = 3, 4\n",
    "\n",
    "x = np.random.randn(N, D)\n",
    "y = np.random.randn(N, D)\n",
    "z = np.random.randn(N, D)\n",
    "\n",
    "# Forward prop\n",
    "a = x * y\n",
    "b = a + z\n",
    "c = np.sum(b)\n",
    "\n",
    "# Backward prop\n",
    "grad_c = 1 # Gradient of c with respect to c \n",
    "grad_b = grad_c * np.ones((N, D))\n",
    "grad_a = grad_b.copy()\n",
    "grad_z = grad_b.copy() \n",
    "grad_x = grad_a * y\n",
    "grad_y = grad_a * x\n",
    "\n",
    "print \"Gradient x:\", grad_x\n",
    "print \"Graident y:\", grad_y\n",
    "print \"Gradient z:\", grad_z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is forward propagation and gradient computation in tensorflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient x: [[ 0.7610377   0.12167501  0.44386324  0.33367434]\n",
      " [ 1.4940791  -0.20515826  0.3130677  -0.85409576]\n",
      " [-2.5529897   0.6536186   0.8644362  -0.742165  ]]\n",
      "Graident y: [[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      " [ 1.867558   -0.9772779   0.95008844 -0.1513572 ]\n",
      " [-0.10321885  0.41059852  0.14404356  1.4542735 ]]\n",
      "Gradient z: [[1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "np.random.seed(0)\n",
    "\n",
    "N, D = 3, 4\n",
    "\n",
    "x = tf.placeholder(tf.float32)\n",
    "y = tf.placeholder(tf.float32)\n",
    "z = tf.placeholder(tf.float32)\n",
    "\n",
    "a = x * y\n",
    "b = a + z \n",
    "c = tf.reduce_sum(b)\n",
    "\n",
    "grad_x, grad_y, grad_z = tf.gradients(c, [x, y, z])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    values = {\n",
    "        x: np.random.randn(N, D),\n",
    "        y: np.random.randn(N, D),\n",
    "        z: np.random.randn(N, D),\n",
    "    }\n",
    "\n",
    "    out = sess.run([c, grad_x, grad_y, grad_z], feed_dict=values)\n",
    "    c_val, grad_x_val, grad_y_val, grad_z_val = out\n",
    "\n",
    "print \"Gradient x:\", grad_x_val\n",
    "print \"Graident y:\", grad_y_val\n",
    "print \"Gradient z:\", grad_z_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedforward Neural Network\n",
    "The following is an example of building a fully connected 3-layer neural network with TensorFlow. As you can see that the code isn't that much different from numpy as like the examples above. Tensorflow shares many similar API with numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N, D, H = 64, 1000, 100\n",
    "with tf.device('/cpu:0'):\n",
    "    x = tf.placeholder(tf.float32, shape=(N, D))\n",
    "    y = tf.placeholder(tf.float32, shape=(N, D))\n",
    "    w1 = tf.placeholder(tf.float32, shape=(D, H))\n",
    "    w2 = tf.placeholder(tf.float32, shape=(H, D))\n",
    "    \n",
    "    h = tf.maximum(tf.matmul(x, w1), 0)\n",
    "    y_pred = tf.matmul(h, w2)\n",
    "    diff = y_pred - y\n",
    "    loss = tf.reduce_mean(tf.reduce_sum(diff ** 2, axis=1))\n",
    "    \n",
    "    grad_w1, grad_w2 = tf.gradients(loss, [w1, w2])\n",
    "    \n",
    "with tf.Session() as sess:\n",
    "    values = {\n",
    "        x: np.random.randn(N, D),\n",
    "        y: np.random.randn(N, D),\n",
    "        w1: np.random.randn(D, H),\n",
    "        w2: np.random.randn(H, D),\n",
    "    }\n",
    "    \n",
    "    learning_rate = 1e-5\n",
    "    for t in range(50):\n",
    "        out = sess.run([loss, grad_w1, grad_w2], feed_dict=values)\n",
    "        loss_val, grad_w1_val, grad_w2_val = out\n",
    "        \n",
    "        # Perform parameter updates and learning\n",
    "        values[w1] -= learning_rate * grad_w1_val\n",
    "        values[w2] -= learning_rate * grad_w2_val "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Graph\n",
    "The workflow in TensorFlow is splitted into two. First we define the computational graph and decide on what are the gradients we are looking for, e.g.\n",
    "```python\n",
    "grad_w1, grad_w2 = tf.gradients(loss, [w1, w2])\n",
    "```\n",
    "\n",
    "```python\n",
    "x = tf.placeholder(tf.float32, shape=(N, D))\n",
    "y = tf.placeholder(tf.float32, shape=(N, D))\n",
    "w1 = tf.placeholder(tf.float32, shape=(D, H))\n",
    "w2 = tf.placeholder(tf.float32, shape=(H, D))\n",
    "h = tf.maximum(tf.matmul(x, w1), 0)\n",
    "y_pred = tf.matmul(h, w2)\n",
    "diff = y_pred - y\n",
    "loss = tf.reduce_mean(tf.reduce_sum(diff ** 2, axis=1))\n",
    "grad_w1, grad_w2 = tf.gradients(loss, [w1, w2])\n",
    "```\n",
    "\n",
    "### Run Session\n",
    "Then we run the graph many times and perform updates on each variable/placeholder. Remember that whatever placeholder we used, we need to feed them with values using key word argument `feed_dict` in `run()` function.\n",
    "```python\n",
    "with tf.Session() as sess:\n",
    "    values = {\n",
    "        x: np.random.randn(N, D),\n",
    "        y: np.random.randn(N, D),\n",
    "        w1: np.random.randn(D, H),\n",
    "        w2: np.random.randn(H, D),\n",
    "    }\n",
    "\n",
    "    out = sess.run([loss, grad_w1, grad_w2], feed_dict=values)\n",
    "    loss_val, grad_w1_val, grad_w2_val = out\n",
    "```\n",
    "\n",
    "### Performance Bottleneck\n",
    "Notice that vanilla `numpy` only works with CPU, the output of `run()` is in numpy array. This creates a problem when we want to run our `tf` code on GPU because we are basically copying values from GPU to CPU and CPU back to GPU. This can create a bottleneck on huge dataset. \n",
    "\n",
    "There is a solution, instead of using `placeholder`, use `Variable`.\n",
    "\n",
    "```python\n",
    "w1 = tf.Variable(tf.random_normal((D, H)))\n",
    "w2 = tf.Variable(tf.random_normal((H, D)))\n",
    "``` \n",
    "\n",
    "These variables are values that live inside the computational graph and will persist throughout training. And then we need to specify how we'd like to update these variables per iteration through `session.Run()` in Tensorflow.\n",
    "\n",
    "```python\n",
    "grad_w1, grad_w2 = tf.gradients(loss, [w1, w2])\n",
    "new_w1 = w1.assign(w1 - learning_rate * grad_w1)\n",
    "new_w2 = w2.assign(w2 - learning_rate * grad_w2)\n",
    "```\n",
    "\n",
    "In order to actually compute and update these values, we need to assign them to a dummy node and tell tensor flow that we need the node to be computed in each iteration.\n",
    "\n",
    "```python\n",
    "weight_updates = tf.group(new_w1, new_w2)\n",
    "with tf.Session() as sess:\n",
    "    for t in range(50):\n",
    "        loss_val, _ = sess.run([loss, weight_updates], feed_dict=values)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow API\n",
    "### Optimizers\n",
    "Tensor flow actually gives us API to run gradient descent. The API lives inside `tf.train`. Basically what it does is similar to what's written above. It performs the updates by calling `assign` on the `tf.Variable`s and then group them into a dummy node and execute the computation per iteration.\n",
    "\n",
    "```python\n",
    "optimizer = tf.train.GradientDescentOptimizer(1e-5)\n",
    "weight_updates = optimizer.minimize(loss)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    values = { x: np.random.randn(N, D), y: np.random.randn(N, D) }\n",
    "    losses = []\n",
    "    for t in range(50):\n",
    "        loss_val, _ = sess.run([loss, weight_updates], feed_dict=values)\n",
    "        losses.append(loss_val)\n",
    "```\n",
    "\n",
    "### Losses\n",
    "If `tf` gives optimizier, it must also other conveninent functions like L1 and L2 losses. That's right, it does!\n",
    "\n",
    "```python\n",
    "loss = tf.losses.mean_squared_error(y_pred, y)\n",
    "```\n",
    "\n",
    "### Layers\n",
    "So far we have omitted all the nitty-gritty details of defining biases and performing xavier initialization for simplicity. It'd take a good amount of code to carefully piece everything together if we were to write them from scratch. And again, `tf` provides everything out of the box for us!\n",
    "\n",
    "```python \n",
    "init = tf.contrib.layers.xavier_initializer()\n",
    "h = tf.layers.dense(inputs=x, units=H, activation=tf.nn.relu, kernel_initializer=init)\n",
    "y_pred = tf.layers.dense(inputs=h, units=D, kernel_initializer=init)\n",
    "loss = tf.losses.mean_squared_error(y_pred, y)\n",
    "```\n",
    "\n",
    "This `tf.layers` library provides architectural setup for us so we don't have to create the layer manually. All the biases and weight initializations are set right out of the box!\n",
    "\n",
    "### Keras\n",
    "Keras is a wrapper around TensorFlow. It provides more readable API at the cost of flexibility. However, most projects don't actually require the flexibility of TensorFlow. Keras works well in most cases.\n",
    "```python\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "N, D, H = 64, 1000, 100\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(input_dim=D, output_dim=H))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(input_dim=H, output_dim=D))\n",
    "\n",
    "optimizer = SGD(lr=1e0)\n",
    "model.compile(loss='mean_squared_error', optimizer=optimizer)\n",
    "\n",
    "x = np.random.randn(N, D)\n",
    "y = np.random.randn(N, D)\n",
    "history = model.fit(x, y, nb_epoch=50, batch_size=N, verbose=0)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More TF Examples\n",
    "Using the provided API, we can now write a more optimized graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['w1' 'w2']\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAEDCAYAAAAVyO4LAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6af0b20f50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\"\"\"\n",
    "Two-layer ReLU fully connected network running on Tensorflow with optimized the variables, i.e. we are not going to load\n",
    "weights from CPU to GPU and GPU back to CPU. The weights will stay within the graph throughout training.\n",
    "\"\"\"\n",
    "\n",
    "N, D, H = 64, 1000, 100\n",
    "with tf.device('/cpu:0'):\n",
    "    # Define our computational graph\n",
    "    x = tf.placeholder(tf.float32, shape=(N,D))\n",
    "    y = tf.placeholder(tf.float32, shape=(N,D))\n",
    "\n",
    "    # Notice that I use Variable instead of placeholder to avoid transfering values from GPU to CPU if\n",
    "    # we were to run this with GPU\n",
    "    w1 = tf.Variable(tf.random_normal((D, H)), name=\"w1\")\n",
    "    w2 = tf.Variable(tf.random_normal((H, D)), name=\"w2\")\n",
    "\n",
    "    h = tf.maximum(tf.matmul(x, w1), 0)\n",
    "    y_pred = tf.matmul(h, w2)\n",
    "    diff = y_pred - y\n",
    "\n",
    "    # L2 Loss\n",
    "    loss = tf.reduce_mean(tf.reduce_sum(diff ** 2, axis=1))\n",
    "\n",
    "    # Telling TensorFlow the update rules for the tf Variables\n",
    "    learning_rate = 1e-5\n",
    "    grad_w1, grad_w2 = tf.gradients(loss, [w1, w2])\n",
    "    new_w1 = w1.assign(w1 - learning_rate * grad_w1)\n",
    "    new_w2 = w2.assign(w2 - learning_rate * grad_w2)\n",
    "\n",
    "    # Create a new dummy node to explicitly tell TensorFlow to NOT SKIP computing it\n",
    "    weight_updates = tf.group(new_w1, new_w2)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print sess.run(tf.report_uninitialized_variables())\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # Vectors x and y should not live in the graph as variables because we actually want to load many different\n",
    "    # mini batch into the graph for SGD.\n",
    "    values = {\n",
    "        x: np.random.randn(N, D),\n",
    "        y: np.random.randn(N, D)\n",
    "    }\n",
    "    \n",
    "    time_steps = []\n",
    "    losses = []\n",
    "    for t in range(50):\n",
    "        loss_val, _ = sess.run([loss, weight_updates], feed_dict=values)\n",
    "        time_steps.append(t)\n",
    "        losses.append(loss_val)\n",
    "    \n",
    "    plt.plot(time_steps, losses)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now using layers, we have a fully optimized example below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/cfeng/Desktop/machine-learning-notebook/neural_network/environment/local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algorithm took 6.52738213539 seconds using /cpu:0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nXxfhANwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6a9ee8f310>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from timeit import default_timer\n",
    "\n",
    "device_name, N, D, H = '/cpu:0', 1000, 1000, 1000\n",
    "\n",
    "with tf.device(device_name):\n",
    "    x = tf.placeholder(tf.float32, shape=(N, D))\n",
    "    y = tf.placeholder(tf.float32, shape=(N, D))\n",
    "\n",
    "    xavier_init = tf.contrib.layers.xavier_initializer()\n",
    "    h = tf.layers.dense(inputs=x, units=H, activation=tf.nn.relu, kernel_initializer=xavier_init)\n",
    "    y_pred = tf.layers.dense(inputs=h, units=D, kernel_initializer=xavier_init)\n",
    "    loss = tf.losses.mean_squared_error(y_pred, y)\n",
    "\n",
    "    optimizer = tf.train.GradientDescentOptimizer(7e0)\n",
    "    weight_updates = optimizer.minimize(loss)\n",
    "\n",
    "start = default_timer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    values = {\n",
    "        x: np.random.randn(N, D),\n",
    "        y: np.random.randn(N, D)\n",
    "    }\n",
    "\n",
    "    losses, iters = [], []\n",
    "    for t in range(100):\n",
    "        loss_val, _ = sess.run([loss, weight_updates], feed_dict=values)\n",
    "        losses.append(loss_val)\n",
    "        iters.append(t)\n",
    "\n",
    "duration = default_timer() - start\n",
    "print 'Algorithm took %s seconds using %s' % (duration, device_name)\n",
    "plt.plot(iters, losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch\n",
    "PyTorch is another popular framework for machine learning from Facebook. Here is a quick comparison between TensorFlow and PyTorch.\n",
    "\n",
    "## Three Levels of Abstractions\n",
    "| PyTorch | TensorFlow |\n",
    "|----------------------------------------------------------------------|------------------------------|\n",
    "|Tensor - Imperative ndarray but runs on GPU                           |Numpy array                   |\n",
    "|Variable - Node in a computational graph; stores data and gradient    |Tensor, Variable, Placeholder |\n",
    "|Module - A neural network layer; may store state or learnable weights |tf.layers, TFSlim, or TFLearn |\n",
    "\n",
    "\n",
    "### Tensors\n",
    "```python\n",
    "import torch\n",
    "\n",
    "dtype = torch.FloatTensor # Or run it on GPU by using torch.cuda.FloatTensor\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "x = torch.randn(N, D_in).type(dtype)\n",
    "y = torch.randn(N, D_out).type(dtype)\n",
    "w1 = torch.randn(D_in, H).type(dtype)\n",
    "w2 = torch.randn(H, D_out).type(dtype)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    h = x.mm(w1)\n",
    "    h_relu = h.clamp(min=0)\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    \n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.t().mm(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.mm(w2.t())\n",
    "    grad_h = grad_h_relu.clone()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.t().mm(grad_h)\n",
    "    \n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2\n",
    "```\n",
    "\n",
    "It looks just like `numpy` code!\n",
    "\n",
    "### Autograd\n",
    "A PyTorch `Variable` is a node in a computational graph. `x.data` is a `Tensor`, `x.grad` is a `Variable` of gradients with the same shape as `x.data`, and `x.grad.data` is a `Tensor` of gradients.\n",
    "\n",
    "PyTorch `Tensor` and `Variable` have the same API. `Variable`s remember how they were created for backprop.\n",
    "```python\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "x = Variable(torch.randn(N, D_in), requires_grad=False)\n",
    "y = Variable(torch.randn(N, D_out), requires_grad=False)\n",
    "w1 = Variable(torch.randn(D_in, H), requires_grad=True)\n",
    "w2 = Variable(torch.randn(H, D_out), requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    \n",
    "    if w1.grad: w1.grad.data.zero()\n",
    "    if w2.grad: w2.grad.data.zero()\n",
    "    loss.backward()\n",
    "    \n",
    "    w1.data -= learning_rate * w1.grad.data\n",
    "    w2.data -= learning_rate * w2.grad.data\n",
    "```    \n",
    "\n",
    "As the name suggests, `autograd` computes gradient for you automatically.\n",
    "\n",
    "### Define Your Own\n",
    "You can define your own autograd functions by writing forward and backward props for tensors.\n",
    "```python\n",
    "class ReLU(torch.autograd.Function):\n",
    "    def forward(self, x):\n",
    "        self.save_for_backward(x)\n",
    "        return x.clamp(min=0)\n",
    "    \n",
    "    def backward(self, grad_y):\n",
    "        x, = self.saved_tensors\n",
    "        grad_input = grad_y.clone()\n",
    "        grad_input[x < 0] = 0\n",
    "        return grad_input\n",
    "```\n",
    "\n",
    "### Optimizer\n",
    "PyTorch offers a similar package to TensorFlow's `keras`. \n",
    "\n",
    "```python\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "x = Variable(torch.randn(N, D_in))\n",
    "y = Variable(torch.randn(N, D_out), requires_grad=False)\n",
    "\n",
    "model = torch.nn.Sequential(torch.nn.Linear(D_in, H), torch.nn.ReLU(), torch.nn.Linear(H, D_out))\n",
    "loss_fn = torch.nn.MSELoss(size_average=False)\n",
    "\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for t in range(500):\n",
    "    y_pred = model(x)\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "```\n",
    "\n",
    "### Modules\n",
    "A PyTorch module is a neural network layer; it inputs and outputs `Variable`s. Modules can contain weights as `Variable` and other modules. You can define your own modules!\n",
    "\n",
    "```python\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class TwoLayerNet(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        super(TwoLayerNet, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(D_in, H)\n",
    "        self.linear2 = torch.nn.Linear(H, D_out)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h_relu = self.linear1(x).clamp(min=0)\n",
    "        y_pred = self.linear2(h_relu)\n",
    "        return y_pred\n",
    "    \n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "x = Variable(torch.randn(N, D_in))\n",
    "y = Variable(torch.randn(N, D_out), requires_grad=False)\n",
    "\n",
    "model = TwoLayerNet(D_in, H, D_out)\n",
    "\n",
    "criterion = torch.nn.MSELoss(size_average=False)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-4)\n",
    "\n",
    "for t in range(500):\n",
    "    y_pred = model(x)\n",
    "    loss = criterion(y_pred, y)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
