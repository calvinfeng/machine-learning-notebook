{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "spread-examination",
   "metadata": {},
   "source": [
    "# Loss Function Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sharing-commerce",
   "metadata": {},
   "source": [
    "For supervised learning, there are two main categories of learning objectives.\n",
    "\n",
    "- Classification (discrete prediction)\n",
    "- Regression (continuous prediction)\n",
    "\n",
    "I will use the following notations and assume indexing starts at 1 for ease of writing.\n",
    "\n",
    "- $N$ is the number of training inputs\n",
    "- $i$ is an index of a training input\n",
    "- $P$ is the number of parameters or weights in the model\n",
    "- $j$ is an index of a parameter\n",
    "- $w$ is a parameter/weight of the model\n",
    "- $y_i$ is the ground truth label for ith input\n",
    "- $\\hat{y}$ is the predicted label for ith input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compliant-certification",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Regression Losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2073639-2f93-438d-a4d5-275f5cecf57f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## L2 Loss "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "partial-facility",
   "metadata": {},
   "source": [
    "This is also called Mean Square Error (MSE) loss. It measures the average squared difference between predictions and actual observations. \n",
    "\n",
    "$$\n",
    "L = \\frac{1}{N} \\Sigma^{N}_{i=1} (y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "\n",
    "Now add a L2 regularization term.\n",
    "\n",
    "$$\n",
    "L = \\frac{1}{N} \\Sigma^{N}_{i=1} (y_i - \\hat{y}_i)^2 + \\lambda \\Sigma^{P}_{j=1} w_j^2\n",
    "$$\n",
    "\n",
    "![MSE](./assets/mse.png)\n",
    "\n",
    "Lambda is the hyperparameter for tuning L2 regularization strength. A regression model that uses L2 regularization technique is called **Ridge Regression**. \n",
    "\n",
    "**Regularization Behavior**\n",
    "\n",
    "This regularization penalizes the squared of weights, which acting as a force to remove % of weight. At any given rate, L2 regularization does not drive weights to zero.\n",
    "\n",
    "Intuitively speaking, since a L2-norm squares the error (increasing by a lot if error > 1), the model will see a much larger error ( e vs e^2 ) than the L1-norm, so the model is much more sensitive to this sample, and adjusts the model to minimize this error. If this sample is an outlier, the model will be adjusted to minimize this single outlier case, at the expense of many other common examples, since the errors of these common examples are small compared to that single outlier case. Therefore, outlier can have significant impact on the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affb974d-3202-4799-b3a9-3bdc654e149c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## L1 Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inappropriate-detective",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "This is also called Mean Absolute Error (MAE) loss. It measures the average of sum of absolute differences between predictions and actual observations.\n",
    "\n",
    "$$\n",
    "L = \\frac{1}{N}\\Sigma^{N}_{i=1} \\left | y_i - \\hat{y}_i \\right |\n",
    "$$\n",
    "\n",
    "Now add L1 regularization term.\n",
    "\n",
    "$$\n",
    "L = \\frac{1}{N}\\Sigma^{N}_{i=1} \\left | y_i - \\hat{y}_i \\right | + \\lambda\\Sigma^{P}_{i=1} \\left | w_i \\right |\n",
    "$$\n",
    "\n",
    "![MAE](./assets/mae.png)\n",
    "\n",
    "Lambda is the hyperparameter for tuning L1 regularization strength. A regression model that uses L1 regularization technique is called **Lasso Regression**.\n",
    "\n",
    "**Regularization Behavior**\n",
    "\n",
    "This regularization penalizes the absolute value of weights, which acting as a force that subtracts some constant from the weight every time. This regularization can drive weights to zero which can be useful for a sparsed inputs to minimize computational effort.\n",
    "\n",
    "In a high dimensional sparse vector, it would be nice to encourage weights to drop to exactly 0 where possible. A weight of exactly 0 essentially removes the corresponding feature from the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comfortable-layout",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## L2 vs L1 Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a8c668-c4c4-453c-a126-a94a849572fd",
   "metadata": {},
   "source": [
    "In general, L2 loss is preferred in most cases. But when there are outliers in the dataset, L2 loss will not perform well, because the error rate is magnified. One should consider using L1 or simply remove the outliers from the dataset before training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42267775-67b3-4dbe-9631-05c8a7a5e1b0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Huber Loss / Smooth Mean Absolute Error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "molecular-renaissance",
   "metadata": {
    "tags": []
   },
   "source": [
    "Huber loss function is defined as the combination of MSE and MAE loss function, using a hyperparameter called $\\delta$ to control the switch point. \n",
    "\n",
    "$$\n",
    "L_{i} = \\left\\{\\begin{matrix}\n",
    "\\text{if} \\; \\left | y_i - \\hat{y}_i \\right | \\leq \\delta \\; \\text{then} \\; \\frac{1}{2} (y_i - \\hat{y}_i)^2 \\\\\n",
    "\\text{else} \\; \\delta \\left | y_i - \\hat{y}_i \\right | - \\frac{1}{2}\\delta^2\n",
    "\\end{matrix}\\right.\n",
    "$$\n",
    "\n",
    "If the error is small and below threshold $\\delta$, the error is squared. Otherwise, the error is absolute valued. The choice of delta value is critical because it determines what you are willing to cosnider as an outlier. Hence the Huber loss could be less sensitive to outliers compared to a standard L2 loss.\n",
    "\n",
    "We can choose whichever regularization term we want from above. I am not going to write it out here.\n",
    "\n",
    "![Huber](./assets/huber.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4363a75d-98fc-4882-b391-52a4c9d1d1eb",
   "metadata": {},
   "source": [
    "### Log Cosh Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amateur-workplace",
   "metadata": {},
   "source": [
    "The log cosh loss function is defined as the log of hyperbolic cosine of the prediction error. It is much smoother than MSE loss. It has all the advantages of Huber loss and it's twice differentiable everywhere, meaning that we can easily take a second derivative of the loss.\n",
    "\n",
    "$$\n",
    "L = \\Sigma^N_{i=1} log(cosh(\\hat{y}_i - y_i))\n",
    "$$\n",
    "\n",
    "![Log Cosh](./assets/logcosh.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "general-modern",
   "metadata": {},
   "source": [
    "# Classification Losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15490b2-fca5-4fa9-b863-a637920035fb",
   "metadata": {},
   "source": [
    "## Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continent-turkish",
   "metadata": {},
   "source": [
    "This is the most common loss function for classification problems. In general entropy represents the amount of information there is in a random event using the probability of the event. For example, if a random event has a probability of 1, it is guaranteed to happen, therefore we need very little information to represent it. If a random event has low probability, it has low likelihood of happening, we need more information to represent it. Entropy loss works on the same principle except it is cross referencing two different distributions.\n",
    "\n",
    "- The ground truth distribution of labels\n",
    "- The predicted distribution of labels\n",
    "\n",
    "When the number of classes is 2, it's a binary classification problem. The predicted value of `y` is `[0, 1]`.\n",
    "\n",
    "$$\n",
    "L = \\frac{-1}{N} \\Sigma^N_i y_i \\cdot log(\\hat{y}_i) + (1 - y_i) \\cdot log(1 - \\hat{y}_i)\n",
    "$$\n",
    "\n",
    "When the number of classes is more than 2, multi-class classification\n",
    "\n",
    "$$\n",
    "L = \\frac{-1}{N} \\Sigma^N_i y_i \\cdot log(\\hat{y}_i)\n",
    "$$\n",
    "\n",
    "![Cross Entropy Loss](./assets/crossentropy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3dee5bf-43fd-4fc0-bcd1-3fc6c2fe1daf",
   "metadata": {},
   "source": [
    "### Why Log?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71615acf-9305-47ee-b1ae-880b50007590",
   "metadata": {},
   "source": [
    "There are two reasons to it.\n",
    "\n",
    "First, entropy is defined as the expected value of self information about a random variable. It's usually computed in base 2 with unit of bits.\n",
    "Cross entropy measures the amount of information needed to remove \"uncertainty\" from the prediction. That is, if prediction distribution matches ground truth distribution, you need 0 information to represent the uncertainty because there is none.\n",
    "\n",
    "$$\n",
    "H(X) = -\\Sigma_{x \\in X} p(x)\\;log_2\\;p(x) = \\mathbb{E}[ -log\\;p(X) ]\n",
    "$$\n",
    "\n",
    "Second, `log` provides numerical stability. For very small probability, `log` function will return a number that is sufficiently larger than the small probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f855d765-0a9d-4cb5-926a-8bc8c99f8f19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-13.815510557964274\n",
      "-16.11809565095832\n",
      "-18.420680743952364\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(np.log(1e-6))\n",
    "print(np.log(1e-7))\n",
    "print(np.log(1e-8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6422d7a1-7750-4b99-8af8-180006699e20",
   "metadata": {},
   "source": [
    "## Hinge Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bibliographic-suffering",
   "metadata": {},
   "source": [
    "The second most common loss function for classification problems and an alternative to cross entropy loss is hinge loss. It's primarily developed for SVM model evaluation. SVM is falling out of favor because for unstructure data, it's been outperformed by neural networks. For structured data, it's been outperformed by gradient boosted trees.\n",
    "\n",
    "$$\n",
    "L = \\frac{-1}{N} \\Sigma^N_i max(0, 1 - y_i \\cdot \\hat{y}_i)\n",
    "$$\n",
    "\n",
    "Hinge loss not only penalizes the wrong predictions but also the right predictions that are not confident. It has to be used on SVM classifiers with class labels `-1` and `1`. \n",
    "\n",
    "![Hinge Loss](./assets/hinge.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
