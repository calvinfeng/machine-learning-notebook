{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Gradients\n",
    "The problem with Q-learning is that Q-function can be very complicated. For a problem with high-dimensional state, it is hard to learn exact or accurate Q-value for every pair of state-action. However, policy can be much simpler. The question is can we learn a policy directly?\n",
    "\n",
    "\n",
    "We will define a class of parametrized policies\n",
    "\n",
    "$$\n",
    "\\prod = \\left\\{ \\pi^{\\theta}, \\theta \\in \\mathbb{R}^{m} \\right\\}\n",
    "$$\n",
    "\n",
    "For each policy, we will define its value as\n",
    "\n",
    "$$\n",
    "J(\\theta) = \\mathbb{E} \\left[ \\Sigma \\; \\gamma^{t} r_{t} \\mid \\pi^{\\theta} \\right]\n",
    "$$\n",
    "\n",
    "We want to find the optimal policy that will give us the best expected reward.\n",
    "\n",
    "$$\n",
    "\\theta^{*} = argmax_{\\theta} \\; J(\\theta)\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforce Algorithm\n",
    "Mathematically we can write\n",
    "\n",
    "$$\n",
    "J(\\theta) = \\int r(\\tau)p(\\tau;\\theta)\\; d\\tau\n",
    "$$ \n",
    "\n",
    "And $r(\\tau)$ is the reward of a state transition trajectory\n",
    "\n",
    "$$\n",
    "\\tau = (s_{0}, a_{0}, r_{0}, s_{1}, ...)\n",
    "$$\n",
    "\n",
    "We want to do gradient ascent to **maximize** the expected reward from the policy. So we need to differentiate the integral!\n",
    "\n",
    "$$\n",
    "\\nabla_{\\theta} J(\\theta) = \\int r(\\tau) \\nabla_{\\theta}\\; p(\\tau;\\theta)\\; d\\tau\n",
    "$$\n",
    "\n",
    "However, this is intractable. Gradient of an expectation value is problematic because probability depends on $\\theta$. But, personally I don't see why this is the case. **NOTE**: Figure this shit out.\n",
    "\n",
    "Here's a trick to do\n",
    "$$\n",
    "\\nabla_{\\theta} p(\\tau;\\theta) = p(\\tau;\\theta) \\frac{\\nabla_{\\theta}p(\\tau;\\theta)}{p(\\tau;\\theta)} =\n",
    "p(\\tau;\\theta)\\nabla_{\\theta}log\\;p(\\tau;\\theta)\n",
    "$$\n",
    "\n",
    "Then we inject it back into the original integral\n",
    "$$\n",
    "\\nabla_{\\theta} J(\\theta) = \\int \\left(r(\\tau) \\nabla_{\\theta}\\; log\\; p(\\tau;\\theta)\\right)p(\\tau;\\theta)\\;d\\tau\n",
    "= \\mathbb{E}\\left[ r(\\tau) \\nabla_{\\theta} log \\; p (\\tau;\\theta) \\right]\n",
    "$$\n",
    "\n",
    "We can estimate with Monte Carlo sampling."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
