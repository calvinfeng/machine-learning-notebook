{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c208bbf1",
   "metadata": {},
   "source": [
    "# Precision & Recall\n",
    "\n",
    "Precision measures the ratio of true positive to all detected positives. Recall measures the ratio of true positive to all ground truth positives.\n",
    "\n",
    "$$\n",
    "\\def\\tp{\\text{True Positives}}\n",
    "\\def\\fp{\\text{False Positives}}\n",
    "\\def\\tn{\\text{True Negatives}}\n",
    "\\def\\fn{\\text{False Negatives}}\n",
    "\\begin{aligned}\n",
    "\\text{Precision} &= \\frac{\\tp}{\\tp + \\fp} \\\\\n",
    "\\text{Recall} &= \\frac{\\tp}{\\tp + \\fn}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "In terms of trade-off, assuming that the model's true positive stays fixed.\n",
    "\n",
    "- Maximizing precision is same as minimizing false positives, maybe at the expense of more false negatives.\n",
    "- Maximizing recall is same as minimizing false negatives, maybe at the expense of more false positives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4c0069",
   "metadata": {},
   "source": [
    "# Intersection over Union (IoU)\n",
    "\n",
    "<div>\n",
    "<img src=\"./assets/iou.png\" width=\"600\"/>\n",
    "</div>\n",
    "\n",
    "IoU is used as a threshold. When area of overlap equals to the area of union, i.e. `iou == 1`, we have a perfect bounding box detection. We use this as a threshold to define true positive or false positive.\n",
    "\n",
    "<div>\n",
    "<img src=\"./assets/iou-threshold.jpeg\" width=\"600\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ef2500",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Average Precision\n",
    "\n",
    "For a set of predictions, we can adjust the IoU threshold to produce a set of precision and recall values. This will result in a **receiver operator characteristic** curve. Now the area under the curve on ROC graph is the average precision. \n",
    "\n",
    "![ROC](./assets/roc.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0888a3a6",
   "metadata": {},
   "source": [
    "# Mean Average Precision\n",
    "\n",
    "Area under the curve is the same as average precision. It _averaged_ over all thresholds of detection. The mean average precision is calculated by taking the mean of **AUC** across all classes. For a model that predicts a single class, the mean AP is the same as AP because there is only 1 class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76a3667",
   "metadata": {},
   "source": [
    "# Mean IoU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffa925f",
   "metadata": {},
   "source": [
    "In **segmentation**, we can define true/false positives or negatives directly on pixel level for each class of instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f107c94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def calculate_confusion_matrix_values(gt_mask, pred_mask):\n",
    "    \"\"\"\n",
    "    Calculates the true positive, false positive, and false negative values for\n",
    "    two binary segmentation masks.\n",
    "\n",
    "    Args:\n",
    "        gt_mask (numpy.ndarray): binary mask of shape (H, W)\n",
    "        pred_mask (numpy.ndarray): binary mask of shape (H, W)\n",
    "\n",
    "    Returns:\n",
    "        tuple: true positive, false positive, false negative values\n",
    "    \"\"\"\n",
    "    TP = np.logical_and(gt_mask, pred_mask)\n",
    "    FP = np.logical_and(np.logical_not(gt_mask), pred_mask)\n",
    "    FN = np.logical_and(gt_mask, np.logical_not(pred_mask))\n",
    "\n",
    "    return np.sum(TP), np.sum(FP), np.sum(FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e653dcec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted labels [1 1 1 2 3]\n",
      "Ground truth labels [1 1 2 1 1]\n",
      "Class = 1 prediction mask [1 1 1 0 0]\n",
      "Class = 1 ground truth mask [1 1 0 1 1]\n",
      "TP: 2, FP: 1, FN: 2\n"
     ]
    }
   ],
   "source": [
    "# Let softmax be one-hot encoding of classes for each pixel (flatten into 1D array of one-hot encodings)\n",
    "softmax = np.array([\n",
    "    [1, 0, 0, 0],\n",
    "    [1, 0, 0, 0],\n",
    "    [1, 0, 0, 0],\n",
    "    [0, 1, 0, 0],\n",
    "    [0, 0, 1, 0],\n",
    "])\n",
    "pred_labels = np.argmax(softmax, axis=1) + 1\n",
    "gt_labels = np.array([1, 1, 2, 1, 1])\n",
    "print(\"Predicted labels\", pred_labels)\n",
    "print(\"Ground truth labels\", gt_labels)\n",
    "\n",
    "# For a given class, say class = 1\n",
    "pred_mask = np.where(pred_labels == 1, 1, 0)\n",
    "gt_mask = np.where(gt_labels == 1, 1, 0)\n",
    "print(\"Class = 1 prediction mask\", pred_mask)\n",
    "print(\"Class = 1 ground truth mask\", gt_mask)\n",
    "TP, FP, FN = calculate_confusion_matrix_values(gt_mask, pred_mask)\n",
    "print(f\"TP: {TP}, FP: {FP}, FN: {FN}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825af90a",
   "metadata": {},
   "source": [
    "Then IoU has a different usage, it measures how well the model segments a scene (or image) for a given class.\n",
    "\n",
    "$$\n",
    "\\def\\tp{\\text{TP}}\n",
    "\\def\\fp{\\text{FP}}\n",
    "\\def\\fn{\\text{FN}}\n",
    "\\text{IoU} = \\frac{\\tp}{\\tp + \\fp + \\fn}\n",
    "$$\n",
    "\n",
    "Mean IoU is the average of IoU for all classes. However, there are few issues with mean IoU.\n",
    "\n",
    "1. **Class Imbalance**: In many real-world datasets, some classes may be much more common than others. If the model is very good at segmenting common classes but poor at identifying rare classes, it could still achieve very high mIoU.\n",
    "2. **Misinterpretation of Mean**: mIoU computes a simple arithmetic mean of the IoU values for each class. This means that each class is given equal weight in the final score. This shares the same vein as class imbalance.\n",
    "3. **Failure to Capture Spatial Discrepancies**: mIoU does not take into account the relative spatial distributions of predicted regions. Two predictions with same IoU could look quite different if one has errors scattered throughout the image and the other has a single consolidated region of error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7424fba",
   "metadata": {},
   "source": [
    "# Dice Coefficient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66320dd2",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{Dice} = \\frac{2 * \\text{Intersection}}{\\text{Union} + \\text{Intersection}}\n",
    "$$\n",
    "\n",
    "This is equivalent to the F1 score.\n",
    "\n",
    "$$\n",
    "\\def\\tp{\\text{TP}}\n",
    "\\def\\fp{\\text{FP}}\n",
    "\\def\\tn{\\text{TN}}\n",
    "\\def\\fn{\\text{FN}}\n",
    "\\begin{aligned}\n",
    "\\text{F1}\n",
    "&= \\frac{2 * \\text{Precision} * \\text{Recall}}{\\text{Precision} + \\text{Recall}} \\\\\n",
    "&= 2*\\frac{\\tp}{\\tp + \\fp}*\\frac{\\tp}{\\tp + \\fn} * \\frac{(\\tp + \\fp) * (\\tp + \\fn)}{\\tp * (\\tp + \\tp + \\fp + \\fn)}\\\\\n",
    "&= \\frac{2TP}{2TP + FP + FN} \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "By definition, union and intersection are\n",
    "\n",
    "$$\n",
    "\\def\\tp{\\text{TP}}\n",
    "\\def\\fp{\\text{FP}}\n",
    "\\def\\tn{\\text{TN}}\n",
    "\\def\\fn{\\text{FN}}\n",
    "\\begin{aligned}\n",
    "\\text{Union} &= \\tp + \\fp + \\fn \\\\\n",
    "\\text{Intersection} &= \\tp\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "This is equivalent to computing harmonic means of precision and recall.\n",
    "\n",
    "$$\n",
    "H(x_1, x_2, \\dots, x_n) = \\frac{n}{\\frac{1}{x_1} + \\frac{1}{x_2} + \\dots + \\frac{1}{x_n}}\n",
    "$$\n",
    "\n",
    "Harmonic mean biased toward the lowest value. Thus, F1 or Dice coefficient is biased toward the lowest value of precision or recall.\n",
    "\n",
    "> One characteristic of the harmonic mean is that it is dominated by the smallest elements of the set, more so \n",
    "  than either the arithmetic mean or the geometric mean. This makes the harmonic mean valuable in certain \n",
    "  situations where you want smaller values to have a higher influence on the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "df8f2333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Harmonic mean 1.8181818181818181\n",
      "Mean 5.5\n"
     ]
    }
   ],
   "source": [
    "def harmonic_mean(values):\n",
    "    reciprocal = 1 / values\n",
    "    return len(values) / np.sum(reciprocal)\n",
    "\n",
    "values = np.array([1.0, 10.0])\n",
    "print(\"Harmonic mean\", harmonic_mean(values))\n",
    "print(\"Mean\", values.mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
