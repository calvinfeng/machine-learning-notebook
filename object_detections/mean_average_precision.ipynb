{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aea27d9f",
   "metadata": {},
   "source": [
    "# Mean Average Precision (mAP)\n",
    "\n",
    "Mean average precision is a popular evaluation metric for object detection which includes localization and classification. Localiztion produces bounding boxes while classification labels the bounding boxes. This is a particularly confusing metric due to its naming. I would be thinking, aren't _mean_ and _average_ the same thing?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c208bbf1",
   "metadata": {},
   "source": [
    "## Precision\n",
    "\n",
    "Precision measures the ratio of true positive to all detected positives, i.e. a detection algorithm has detected N objects, how many of the N objects are correct detections? On the other hand, recall measures the ratio of true positive to all ground truth positive, i.e. a detection algorithm has detected N objects, but there are M objects to be detected, how many did it miss?\n",
    "\n",
    "High precision does not account for missing detection. A model with high precision can still miss a lot of objects it should have detected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4c0069",
   "metadata": {},
   "source": [
    "## Intersection over Union (IoU)\n",
    "\n",
    "IoU measures the overlap between the predicted bounding box and the ground truth bounding box. We need IoU to define precision and recall. \n",
    "\n",
    "![IoU](./assets/iou.png)\n",
    "\n",
    "IoU is used as a threshold. When area of overlap equals to the area of union, we have a perfect bounding box detection which is `iou == 1`. We can use this value to set a threshold to say whether we have a true positive or false positive.\n",
    "\n",
    "![IoU Threshold](./assets/iou-threshold.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ef2500",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Area Under Curve (AOC)\n",
    "\n",
    "For a set of predictions, we can adjust the IoU threshold to produce a set of precision and recall values. This will result in a **receiver operator characteristic** graph. Now the area under the curve on ROC graph is the average precision or sometimes they call it **AUC**. \n",
    "\n",
    "![ROC](./assets/roc.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0888a3a6",
   "metadata": {},
   "source": [
    "## Mean Average Precision\n",
    "\n",
    "Area under the curve is the same as average precision. It _averaged_ over all thresholds of detection. The mean average precision is calculated by taking the mean of **AUC** across all classes. For a model that predicts a single class, the mean AP is the same as AP because there is only 1 class."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
