{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# LSTM Recurrent Neural Network\n",
    "## Exploding Gradient in Vanilla RNN\n",
    "Recall that our RNN model:\n",
    "\n",
    "$$\n",
    "h_{t} = tanh(W_{hh} h_{t-1} + W_{xh}x_{t})\n",
    "$$\n",
    "\n",
    "$$\n",
    "h_{t} = tanh \\begin{pmatrix} (W_{hh} W_{xh}) \\begin{pmatrix} h_{t-1} \\\\ x_{t} \\end{pmatrix} \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "h_{t} = tanh \\begin{pmatrix} W \\begin{pmatrix} h_{t-1} \\\\ x_{t} \\end{pmatrix} \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "For every time step of a sequence, we backprogate from `h[t]` to `h[t-1]`. First the gradient will flow through the `tanh` gate and then to matrix multiplication gate. As we know, whenever we backprop into matrix multiplication gate, the upstream gradient is multiplied by the tranpose of the `W` matrix. This happens at every time step throughout the sequence. What if the sequence is very long?\n",
    "\n",
    "![rnn-gradient-flow](assets/rnn-gradient-flow.png)\n",
    "\n",
    "The final expression for gradient on `h[0]` will involve many factors of this weight matrix. This will either lead to an exploding gradient problem or vanishing gradient problem. There' a simple hack to address this problme, which is using `numpy.clip`. However, if the problem is vanishing gradient, clipping isn't going to help."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Introducing LSTM\n",
    "\n",
    "LSTM has a fancier recurrence relation than the vanilla RNN. LSTM has two states, one is being the usual hidden state `h[t]` we see in vanilla RNN and another one is called the cell state `c[t]`. Cell state is an internal vector that is not exposed to the outside world. \n",
    "\n",
    "Let's define some terminologies here: \n",
    "\n",
    "* `f` **forget gate**: whether to erase cell\n",
    "* `i` **input gate**: whether to write to cell\n",
    "* `g` **gate gate**: how much to write to cell\n",
    "* `o` **output gate**: how much to reveal cell\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix} i \\\\ f \\\\ o \\\\ g \\end{pmatrix} = \\begin{pmatrix} \\sigma \\\\ \\sigma \\\\ \\sigma \\\\ tanh \\end{pmatrix}\n",
    "W \\begin{pmatrix} h_{t - 1} \\\\ x_{t} \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "**Note** that the sigma symbol represents sigmoid activation function.\n",
    "\n",
    "$$\n",
    "c_{t} = f \\odot c_{t - 1} + i \\odot g\n",
    "$$\n",
    "\n",
    "which is equivalent to \n",
    "\n",
    "$$\n",
    "c_{t} = \\sigma(W_{hhf} h_{t-1} + W_{xhf} x_{t}) \\odot c_{t-1} + \\sigma(W_{hhi} h_{t-1} + W_{xhi} x_{t}) \\odot tanh(W_{hhg} h_{t-1} + W_{xhg} x_{t})\n",
    "$$\n",
    "\n",
    "And\n",
    "\n",
    "$$\n",
    "h_{t} = o \\odot tanh(c_{t})\n",
    "$$\n",
    "\n",
    "which is equivalent to\n",
    "\n",
    "$$\n",
    "h_{t} = \\sigma \\begin{pmatrix} W_{hho} h_{t-1} + W_{xho} x_{t} \\end{pmatrix} \\odot tanh(c_{t})\n",
    "$$\n",
    "\n",
    "We take the previous cell state and hidden state as the inputs to our LSTM cell. The previous hidden state is combined with the input vector and multiply with the weight matrix to produce `ifog`. \n",
    "\n",
    "The forget gate multiplies element-wise with the previous cell state. The input and gate gate also multiply element wise. The two results are combined through sum elemenwise to produce a new cell state. The cell state is then squashed by a `tanh` and multiplied element-wise by the output gate to produce our next hidden state.\n",
    "![lstm](assets/lstm.png)\n",
    "\n",
    "### LSTM Gradient Flow\n",
    "Backpropagating from `c[t]` to `c[t-1]` is only element-wise multiplication by the `f` gate, and there is no matrix multiplication by W. The `f` gate is different at every time step, ranged between 0 and 1 due to sigmoid property, thus we have avoided of the problem of multiplying the same thing over and over again. \n",
    "\n",
    "Backpropagating from `h[t]` to `h[t-1]` is going through only one single `tanh` nonlinearity rather than `tanh` for every single step.\n",
    "\n",
    "![cell-state-gradient-flow](assets/cell-state-gradient-flow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## LSTM Forward Propagation\n",
    "The forward propagation isn't all that different from the vanilla recurrent neural network, we just now have more variables. Suppose we take a mini-batch of data, of shape `(N, T, D)`. `N` is our batch size, `T` is the size of the sequence, and `D` is the dimension of our input. \n",
    "\n",
    "For example, I have a sentence, `\"hello world I am Calvin\"`. We can treat this sentence as one input sequence with size `T=5` because it has five words. The dimension of the input depends on how we represent each word. Before we feed the sentence into a RNN, each word of a sentence has to be converted to a word vector which has dimension of `D`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0.92316896  0.42683601  0.09736801  0.42345096  0.16077895]\n",
      "  [ 0.76913305  0.19142192  0.17194874  0.64221621  0.38024893]\n",
      "  [ 0.34509468  0.05679093  0.17179702  0.09890025  0.38915591]\n",
      "  [ 0.59768999  0.51223228  0.53420898  0.48474058  0.02789487]\n",
      "  [ 0.93754868  0.97818453  0.03263516  0.39473973  0.41731715]]\n",
      "\n",
      " [[ 0.93754868  0.97818453  0.03263516  0.39473973  0.41731715]\n",
      "  [ 0.06547563  0.62950683  0.88021683  0.52397818  0.48162677]\n",
      "  [ 0.92316896  0.42683601  0.09736801  0.42345096  0.16077895]\n",
      "  [ 0.76913305  0.19142192  0.17194874  0.64221621  0.38024893]\n",
      "  [ 0.91825584  0.85292219  0.11624048  0.21388636  0.39841289]]]\n",
      "\n",
      "Input sequence has shape (2, 5, 5)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Use null token to ensure each input sentence has the same length\n",
    "raw_txt_inputs = ['hello world i am calvin', 'calvin says hello world <null>']\n",
    "word_to_idx = dict()\n",
    "\n",
    "idx = 0\n",
    "for sentence in raw_txt_inputs:\n",
    "    for word in sentence.split():\n",
    "        if word_to_idx.get(word) is None:\n",
    "            word_to_idx[word] = idx\n",
    "            idx += 1\n",
    "\n",
    "# Create a weight matrix for mapping word to its word vector representation\n",
    "vocab_size = len(word_to_idx)\n",
    "word_vec_dim = 5\n",
    "word_embedding_weight = np.random.rand(vocab_size, word_vec_dim)\n",
    "\n",
    "# Convert raw_txt_input to tensor representation\n",
    "index_sequences = []\n",
    "for sentence in raw_txt_inputs:\n",
    "    seq = []\n",
    "    for word in sentence.split():\n",
    "        seq.append(word_to_idx[word])\n",
    "    index_sequences.append(seq)\n",
    "\n",
    "input_sequences = word_embedding_weight[np.array(index_sequences)]\n",
    "\n",
    "print input_sequences\n",
    "print '\\nInput sequence has shape', input_sequences.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we pass a mini batch of sequences to the LSTM layer, we will run through the each word vector of each sequence through series of time step. In each time step, we perform the following forward propagation:\n",
    "```python\n",
    "    def _forward_step(self, x, prev_hidden_state, prev_cell_state):\n",
    "        \"\"\"Forward pass for a single time step of the LSTM layer.\n",
    "\n",
    "        :param np.array x: Input data of shape (N, D)\n",
    "        :param np.array prev_hidden_state: Previous hidden state of shape (N, H)\n",
    "        :param np.array prev_cell_state: Previous cell state of shape (N, H)\n",
    "\n",
    "        Returns tuple:\n",
    "            - next_hidden_state: Next hidden state, of shape (N, H)\n",
    "            - next_cell_state: Next cell state, of shape (N, H)\n",
    "            - cache: Tuple of values needed for back-propagation\n",
    "        \"\"\"\n",
    "        _, H = prev_hidden_state.shape\n",
    "\n",
    "        # Compute activations\n",
    "        acts = np.dot(x, self.Wx) + np.dot(prev_hidden_state, self.Wh) + self.b\n",
    "\n",
    "        # Compute the internal gates\n",
    "        input_gate = sigmoid(acts[:, 0:H])\n",
    "        forget_gate = sigmoid(acts[:, H:2*H])\n",
    "        output_gate = sigmoid(acts[:, 2*H:3*H])\n",
    "        gain_gate = np.tanh(acts[:, 3*H:4*H])\n",
    "\n",
    "        # Compute next states\n",
    "        next_cell_state = forget_gate * prev_cell_state + input_gate * gain_gate\n",
    "        next_hidden_state = output_gate * np.tanh(next_cell_state)\n",
    "\n",
    "        # Cache the results\n",
    "        cache = {\n",
    "            'x': x,\n",
    "            'next-c': next_hidden_state,\n",
    "            'next-h': next_cell_state,\n",
    "            'i-gate': input_gate,\n",
    "            'f-gate': forget_gate,\n",
    "            'o-gate': output_gate,\n",
    "            'g-gate': gain_gate,\n",
    "            'prev-h': prev_hidden_state,\n",
    "            'prev-c': prev_cell_state\n",
    "        }\n",
    "\n",
    "        return next_hidden_state, next_cell_state, cache\n",
    "```\n",
    "\n",
    "The cache is necessary for back propagation later. The forward propagation of a LSTM layer can be thought as breaking a sequence into time steps and feed each time step to the above code snippet.\n",
    "```python\n",
    "    def forward(self, input_sequence, h0, Wx=None, Wh=None, b=None):\n",
    "        \"\"\"Forward pass for a LSTM layer over an entire sequence of data. \n",
    "        This assumes an input sequence composed of T vectors, each of dimension D. \n",
    "        The LSTM uses a hidden size of H, and it works over a mini-batch containing N sequences.\n",
    "\n",
    "        :param np.array input_sequence: Input data of shape (N, T, D)\n",
    "        :param np.array h0: Initial hidden state of shape (N, H)\n",
    "        :param np.array Wx: Optional input-to-hidden weight matrix, of shape (D, 4H)\n",
    "        :param np.array Wh: Optional hidden-to-hidden weight matrix, of shape (H, 4H)\n",
    "        :param np.array b: Optional bias vector, of shape (4H,)\n",
    "\n",
    "        Returns np.array:\n",
    "            Hidden state over time of shape (N, T, H)\n",
    "        \"\"\"\n",
    "        if Wx is not None and Wh is not None and b is not None:\n",
    "            self.Wx, self.Wh, self.b = Wx, Wh, b\n",
    "\n",
    "        N, T, D = input_sequence.shape\n",
    "        _, H = h0.shape\n",
    "\n",
    "        # Cache the inputs and create time series variables, \n",
    "        # i.e. hidden states over time and cell states over time.\n",
    "        self.input_sequence = input_sequence\n",
    "        self.h0 = h0\n",
    "\n",
    "        self.hidden_states_over_t = np.zeros((N, T, H))        \n",
    "        self.cell_states_over_t = np.zeros((N, T, H))\n",
    "        self.caches = dict()\n",
    "\n",
    "        # Run the sequence\n",
    "        prev_hidden_state = h0\n",
    "        prev_cell_state = np.zeros(h0.shape)\n",
    "        for t in range(T):\n",
    "            hidden_state, cell_state, self.caches[t] = self._forward_step(input_sequence[:, t, :],\n",
    "                                                                         prev_hidden_state,\n",
    "                                                                         prev_cell_state)\n",
    "            self.hidden_states_over_t[:, t, :] = hidden_state\n",
    "            self.cell_states_over_t[:, t, :] = cell_state\n",
    "\n",
    "            prev_hidden_state, prev_cell_state = hidden_state, cell_state\n",
    "\n",
    "        return self.hidden_states_over_t\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Back Propagation\n",
    "We are given the following upstream gradients:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial c_{t}} \\;,\\; \\frac{\\partial L}{\\partial h_{t}}\n",
    "$$\n",
    "\n",
    "The objective is to find `ifog` gate gradients and matrix weight gradients:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial i} \\;,\\;\n",
    "\\frac{\\partial L}{\\partial f} \\;,\\;\n",
    "\\frac{\\partial L}{\\partial o} \\;,\\;\n",
    "\\frac{\\partial L}{\\partial g} \\;,\\;\n",
    "\\frac{\\partial L}{\\partial W_{x}} \\;,\\;\n",
    "\\frac{\\partial L}{\\partial W_{h}} \\;,\\;\n",
    "\\frac{\\partial L}{\\partial b} \\;,\\;\n",
    "\\frac{\\partial L}{\\partial x} \\;,\\;\n",
    "\\frac{\\partial L}{\\partial h_{0}}\n",
    "$$\n",
    "\n",
    "Recall that `ifog` gates are declared as follows:\n",
    "$$\n",
    "\\begin{pmatrix} i \\\\ f \\\\ o \\\\ g \\end{pmatrix} = \n",
    "\\begin{pmatrix} \\sigma \\\\ \\sigma \\\\ \\sigma \\\\ tanh \\end{pmatrix}\n",
    "\\left( \\vec{x}W_{x} + \\vec{h}_{t-1}W_{h} + b\\right)\n",
    "$$\n",
    "\n",
    "The expected output of above calculation is a matrix of shape `(N, 4H)` where N is the mini-batch size and H is the hidden dimension. Using what we have above, we can compute the next cell state as follows:\n",
    "\n",
    "$$\n",
    "c_{t} = f \\cdot c_{t-1} + i \\cdot g\n",
    "$$\n",
    "\n",
    "With cell state, we can compute the next hidden state:\n",
    "\n",
    "$$\n",
    "h_{t} = o \\cdot tanh\\left(c_{t}\\right)\n",
    "$$\n",
    "\n",
    "### Compute Gradients\n",
    "Since we are given upstream gradients of loss with respect to output cell state and output hidden state, we will first compute gradients of hidden state with respect to cell state and gradient of new cell state with respect to old cell state.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial h_{t}}{\\partial c_{t}} = o \\cdot (1 - tanh^{2}(c_{t}))\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial c_{t}}{\\partial c_{t - 1}} = f\n",
    "$$\n",
    "\n",
    "Now we can calculate gradient of loss with respect to previous cell state, which is a sum of contributions from both upstream gradients.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial c_{t-1}} = \\frac{\\partial L}{\\partial c_{t}} \\frac{\\partial c_{t}}{\\partial c_{t-1}} +\n",
    "\\frac{\\partial L}{\\partial h_{t}} \\frac{\\partial h_{t}}{\\partial c_{t}} \\frac{\\partial c_{t}}{\\partial c_{t-1}}\n",
    "$$\n",
    "\n",
    "Proceed and compute gradients for `ifog` gates:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial i} = \\frac{\\partial L}{\\partial c_{t}} \\frac{\\partial c_{t}}{\\partial i} + \n",
    "\\frac{\\partial L}{\\partial h_{t}} \\frac{\\partial h_{t}}{\\partial c_{t}} \\frac{\\partial c_{t}}{\\partial i}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial f} = \\frac{\\partial L}{\\partial c_{t}} \\frac{\\partial c_{t}}{\\partial f} + \n",
    "\\frac{\\partial L}{\\partial h_{t}} \\frac{\\partial h_{t}}{\\partial c_{t}} \\frac{\\partial c_{t}}{\\partial f}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial o} = \\frac{\\partial L}{\\partial h_{t}} \\frac{\\partial h_{t}}{\\partial o}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial g} = \\frac{\\partial L}{\\partial c_{t}} \\frac{\\partial c_{t}}{\\partial g} + \n",
    "\\frac{\\partial L}{\\partial h_{t}} \\frac{\\partial h_{t}}{\\partial c_{t}} \\frac{\\partial c_{t}}{\\partial g}\n",
    "$$\n",
    "\n",
    "Continue to back-propagate and we are almost done! Compute the non-linearity for each of the `ifog` gates and then combine the gradients to form one combined matrix of shape `(N, 4H)`. We will call this our activiation matrix `A`. Then the following gradients can be obtained:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W_{x}} = \\frac{\\partial L}{\\partial A} \\frac{\\partial A}{\\partial W_{x}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W_{h}} = \\frac{\\partial L}{\\partial A} \\frac{\\partial A}{\\partial W_{h}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial b} = \\frac{\\partial L}{\\partial A} \\frac{\\partial A}{\\partial b}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial x} = \\frac{\\partial L}{\\partial A} \\frac{\\partial A}{\\partial x}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial h_{0}} = \\frac{\\partial L}{\\partial A} \\frac{\\partial A}{\\partial h_{0}}\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
