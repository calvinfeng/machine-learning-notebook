{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5af7f764-7a7d-4bea-ad8b-d71145464774",
   "metadata": {},
   "source": [
    "# Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f365d59-af8d-4a74-867a-237bc67c67c0",
   "metadata": {},
   "source": [
    "I may revisit this topic later. This is an out-of-fashion technique."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98d7072-1df9-49ba-9eac-22b37bdf021d",
   "metadata": {},
   "source": [
    "## Max Margin Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bac1650-4f46-45e9-909d-e879fd432e6b",
   "metadata": {},
   "source": [
    "The shortest distance between the observations and the threshold is called the margin. When we use the threshold that gives us the largest margin to make classifications, we are using a **Maximal Margin Classifier**.\n",
    "\n",
    "Maximal Margin Classifiers are super sensitive to outliers in the training data and makes them very ineffective.\n",
    "\n",
    "To make a threshold that is not so sensitive to outliers, we must allow misclassifications. When we allow misclassifactions, the distance between the observations and the threshold is called a **Soft Margin**.\n",
    "\n",
    "When we pick a threshold that is less sensitive to the training data and allows misclassification, it is giving us higher bias but in exchange reducing variance.\n",
    "\n",
    "We will use cross validation to determine how many misclassifications and observations to allow inside to allow inside of the Soft Margn to get the best classification results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdeaedc5-d9b0-475e-ab8f-af40fa7baa60",
   "metadata": {},
   "source": [
    "## Soft Margin Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a70aac7-5de6-41cd-aae5-2b3fd7ce4b0a",
   "metadata": {},
   "source": [
    "When we use a soft margin to determine the location of a threshold, then we are using a soft margin classifier also known as a **support vector classifier** to classify observations.\n",
    "\n",
    "The name support vector classifier comes from the fact that the observations on the edge and within the soft margin are called “support vectors”. \n",
    "\n",
    "When data are 2-dimensional, a support vector classifier is a line. When the data are 3-dimensional, the support vector classifier forms a plane."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ea4c2f-086e-49f3-8da8-e01bb1988a0e",
   "metadata": {},
   "source": [
    "![Soft vs Hard Margin](assets/hard-vs-soft-margin.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54118c81-e485-4254-8577-876c0fb64cba",
   "metadata": {},
   "source": [
    "Support vector classifiers generally don’t perform well with non linearly separatable data such as drug dosage. For example, we want to predict what is the effective dosage of a drug for a disease. It won’t cure the disease if dosage is too little or too large. There isn’t a threshold point that can cleanly separate these data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559ee995-f720-4119-a307-b53a9fbfb5e2",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407e7969-79a0-41e6-a11f-50e50c0c5c5f",
   "metadata": {},
   "source": [
    "If the support vector classifier cannot draw a threshold or cutoff point for classifying drug dosage, can we project this data into high dimensional space such that it becomes linearly separable?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d5bdcf-5ed4-4a57-8ab0-a42d672e8695",
   "metadata": {},
   "source": [
    "![Principle of SVM](assets/principle-of-svm.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1ee9e0-02fd-4fe3-a18b-a769d3d40325",
   "metadata": {},
   "source": [
    "1. Start with data in a relatively low dimension\n",
    "2. Move the data into a higher dimension\n",
    "3. Find a support vector classifier that separates the higher dimensional data into two groups"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175dbe12-3585-4f0e-8f9e-dbd27e98bdd5",
   "metadata": {},
   "source": [
    "## Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed89f4d-6f58-4bc9-b7f7-9ed573b3efb6",
   "metadata": {},
   "source": [
    "In order to make the mathematics possible, SVM use Kernel Functions to systematically find support vector classifiers in higher dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d21b94-cb6e-40ac-b8d9-0e28a8616d50",
   "metadata": {},
   "source": [
    "### Polynomial Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51757eaa-cd37-4109-9024-ac9e3aadc2e6",
   "metadata": {},
   "source": [
    "When d = 1, the polynomial kernel computes the relationships between each pair of observations in 1-dimension, and these relationships are used to find a support vector classifier. \n",
    "\n",
    "When d = 2, we get a 2nd dimension based on the squared of the original value. The polynomial kernel will then compute the 2-dimensional relationships between each pair of observations. The logic repeats for higher dimensions.\n",
    "\n",
    "In summary, the polynomial kernel systematically increases dimensions by setting d, the degree of the polynomial. The variable d is a hyperparameter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df8a5ed-5158-4638-800a-9026907b38f0",
   "metadata": {},
   "source": [
    "### Radial Basis Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76505079-2d71-4a44-be7b-7e8f38bacacc",
   "metadata": {},
   "source": [
    "The Radial kernel finds support vector classifiers in INFINITE dimensions. The radial kernel can behave like a weighted nearest neighbor model in the case of drug dosage prediction.\n",
    "\n",
    "Kernel functions only calculate the relationships between every pair of points as if they are in the higher dimensions; they don’t actually do the transformation. This trick, calculating the high dimensional relationships without actually transforming the data to the higher dimension, is called The Kernel Trick.\n",
    "\n",
    "The Kernel Trick reduces the amount of computation required for SVM by avoiding the math that transforms the data from low to high dimensions, and it makes calculating relationships in the infinite dimensions used by the Radial Kernel possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa7a61a-5796-49be-9e38-48ac4cfaa229",
   "metadata": {},
   "source": [
    "![Radial Kernel](assets/radial-kernel.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
