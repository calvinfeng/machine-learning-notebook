{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Batch Normalization\n",
    "## Unit Gaussian Activations\n",
    "\n",
    "So if you really want unit Gaussian activations, you can make them so by applying batch normalization to every layer. Let's consider a batch of activations at some layer, we can make each dimension (denoted by $k$) unit Gaussian by applying: \n",
    "\n",
    "$$\n",
    "\\hat{x}^{(k)} = \\frac{x^{(k)} - E[x^{(k)}]}{\\sqrt{Var[x^{(k)}]}}\n",
    "$$\n",
    "\n",
    "Each batch of training example has dimension `D`. Compute the empirical mean and variance independently for each dimension by using all the training data.\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.          0.96666667  0.83333333]\n",
      "[ 0.          0.00222222  0.05555556]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# We have three training activation examples and each example has a dimension of 3\n",
    "activations = np.array([[1, 0.9, 1],[1, 1, 1],[1, 1, 0.5]])\n",
    "\n",
    "print activations.mean(axis=0)\n",
    "print activations.var(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Batch normalization is usually inserted after fully connected or convolutional layers and before nonlinearity is applied. For the convolutional layer, we are basically going to have one mean and one standard deviation per activation map that we have. And then we are going to normalize across all of the examples in the batch of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Avoid Constraints by Learning\n",
    "\n",
    "If we have a tanh layer, we don't really want to constraint it to the linear regime. The act of normalization might force us to stay within the center, which is known as the linear regime. We want flexibility so ideally we should learn batch normalization as a paramter of the network. In other words, we should insert a parameter which can be learned to effectively cancel out batch normalization if the network sees fit.\n",
    "\n",
    "We will apply the following operation to each normalized vector:\n",
    "\n",
    "$$\n",
    "y^{(k)} = \\gamma^{(k)}\\hat{x}^{(k)} + \\beta^{(k)}\n",
    "$$\n",
    "\n",
    "Such that the network can learn\n",
    "\n",
    "$$\n",
    "\\gamma^{(k)} = \\sqrt{Var[x^{(k)}]} \\\\\n",
    "\\beta^{(k)} = E[x^{(k)}]\n",
    "$$\n",
    "\n",
    "And effectively recover the identity mapping as if you didn't have batch normalization, i.e. to cancel out the batch normalization if the network sees fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Summary\n",
    "\n",
    "**Inputs**: Values of $x$ over a mini-batch: **B** = $\\{x_{1}...x_{m}\\}$\n",
    "\n",
    "**Outputs**: $\\{y_{i} = BN_{\\gamma, \\beta}(x_{i})\\}$\n",
    "\n",
    "Find mini-batch mean:\n",
    "$$\n",
    "\\mu_{B} = \\frac{1}{m} \\sum^{m}_{i = 1} x_{i}\n",
    "$$\n",
    "\n",
    "Find mini-batch variance:\n",
    "$$\n",
    "\\sigma_{B}^{2} = \\frac{1}{m} \\sum^{m}_{i = 1} (x_{i} - \\mu_{B})^{2}\n",
    "$$\n",
    "\n",
    "Normalize:\n",
    "$$\n",
    "\\hat{x_{i}} = \\frac{x_{i} - \\mu_{B}}{\\sqrt{\\sigma_{B}^{2} + \\epsilon}}\n",
    "$$\n",
    "\n",
    "Scale and shift:\n",
    "$$\n",
    "y_{i} = \\gamma \\hat{x_{i}} + \\beta = BN_{\\gamma, \\beta}(x_{i})\n",
    "$$\n",
    "\n",
    "### Benefits\n",
    "* Improves gradient flow through the network\n",
    "* Allows higher learning rates\n",
    "* Reduces the strong dependence on initialization\n",
    "* Acts as a form of regularization in a funny way, and slightly reduces the need for dropout"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
