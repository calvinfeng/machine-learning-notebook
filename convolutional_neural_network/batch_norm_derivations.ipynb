{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Normalization\n",
    "Batch normalization is *invented* and widely popularized by the paper *Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift*. In deep neural network, activations between neural layers are extremely dependent on the parameter initialization, which in turn affects how outputs are backprop into each layer during training. Poor initialization can greatly affect how well a network is trained and how fast it can be trained. Batch normalization is a powerful technique for decoupling the weight updates from parameter initialization. Quoted from the paper, *batch normalization allows us to use much higher learning rates and be less careful about initialization.*\n",
    "\n",
    "Much of the derivation comes from the paper itself and also from Kevin Zakka's blog on Github."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notations\n",
    "* **BN** stands for batch normalization\n",
    "* $x$ is the input matrix/vector to the **BN** layer\n",
    "* $\\mu$ is the batch mean\n",
    "* $\\sigma^{2}$ is the batch variance\n",
    "* $\\epsilon$ is a small constant added to avoid dividing by zero\n",
    "* $\\hat{x}$ is the normalized input matrix/vector\n",
    "* $y$ is the linear transformation which scales $x$ by $\\gamma$ and $\\beta$\n",
    "* $f$ represents the next layer after **BN** layer, if we assume a forward pass ordering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Pass\n",
    "Forward pass is very easy intuitively and mathematically.\n",
    "\n",
    "First we find the mean across a mini-batch of training examples\n",
    "\n",
    "$$\n",
    "\\mu = \\frac{1}{m} \\sum^{m}_{i = 1} x_{i}\n",
    "$$\n",
    "\n",
    "Find the variance across the same mini-batch of training examples\n",
    "\n",
    "$$\n",
    "\\sigma^{2} = \\frac{1}{m} \\sum^{m}_{i = 1} (x_{i} - \\mu)^{2}\n",
    "$$\n",
    "\n",
    "And then apply normalization\n",
    "\n",
    "$$\n",
    "\\hat{x_{i}} = \\frac{x_{i} - \\mu}{\\sqrt{\\sigma^{2} + \\epsilon}}\n",
    "$$\n",
    "\n",
    "Finally, apply linear transformation with learned parameters to enable network to recover identity. In case we wonder why do we need to do this. \n",
    "> Note that simply normalizing each input of a layer may change what the layer can represent. For instance, normalizing the inputs of a sigmoid would constrain them to the linear regime of the nonlinearity. To address this, ew make sure that the transformation inserted in the network can represent the identity transform. \n",
    "$$\n",
    "y_{i} = \\gamma \\hat{x_{i}} + \\beta = BN_{\\gamma, \\beta}(x_{i})\n",
    "$$\n",
    "\n",
    "If $\\gamma$ is 1 and $beta$ is 0 then the linear transformation is an identity transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1.33226763e-16   1.22124533e-16   0.00000000e+00  -3.60822483e-17\n",
      "  -2.22044605e-17]\n",
      "[ 0.9998869   0.99986432  0.99977066  0.99991854  0.99989989]\n",
      "{'running_var': array([ 0.00884096,  0.00736936,  0.00435933,  0.01227432,  0.00998846]), 'running_mean': array([ 0.04333566,  0.05228123,  0.06205922,  0.05746892,  0.05503796])}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def batch_norm_forward(x, gamma, beta, bn_params):\n",
    "    eps = bn_params.get('eps', 1e-5)\n",
    "    momentum = bn_params.get('momentum', 0.9)\n",
    "    mode = bn_params.get('mode', 'train')\n",
    "    \n",
    "    N, D = x.shape\n",
    "    running_mean = bn_params.get('running_mean', np.zeros(D, dtype=x.dtype))\n",
    "    running_var = bn_params.get('running_var', np.zeros(D, dtype=x.dtype))\n",
    "    \n",
    "    y = None\n",
    "    if mode == 'train':\n",
    "        mean = x.mean(axis=0)\n",
    "        var = x.var(axis=0)\n",
    "        x_norm = (x - mean) / np.sqrt(var + eps)\n",
    "        y = x_norm * gamma + beta\n",
    "        \n",
    "        # Update running mean and running variance during training time\n",
    "        running_mean = momentum * running_mean + (1 - momentum) * mean\n",
    "        running_var = momentum * running_var + (1 - momentum) * var\n",
    "        \n",
    "    elif mode == 'test':\n",
    "        # Use running mean and runningvariance for making test predictions\n",
    "        x_norm = (x - running_mean) / np.sqrt(running_var + eps)\n",
    "        y = x_norm * gamma + beta\n",
    "    else:\n",
    "        raise ValueError('Invalid forward pass batch norm mode %s' % mode)\n",
    "    \n",
    "    bn_params['running_mean'] = running_mean\n",
    "    bn_params['running_var'] = running_var\n",
    "    \n",
    "    return y\n",
    "\n",
    "x = np.random.rand(5, 5)\n",
    "bn_params = {}\n",
    "y = batch_norm_forward(x, 1, 0, bn_params)\n",
    "print y.mean(axis=0)\n",
    "print y.var(axis=0)\n",
    "print bn_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward Pass\n",
    "Now here comes the hard part. We are given an upstream gradient, i.e. the gradient of loss function w.r.t to output of the batch normalization layer. \n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial y} = \\frac{\\partial L}{\\partial f} \\frac{\\partial f}{\\partial y}\n",
    "$$\n",
    "\n",
    "We need to find \n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\hat{x}}, \\; \\frac{\\partial L}{\\partial \\sigma^{2}}, \\; \\frac{\\partial L}{\\partial \\mu}, \\; \\frac{\\partial L}{\\partial x}, \\; \\frac{\\partial L}{\\partial \\gamma}, \\; and \\; \\frac{\\partial L}{\\partial \\beta} \n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Gradient of Normalized Input\n",
    "The derivative of $y$ with respect to $\\hat{x}$ is simple:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial y}{\\partial \\hat{x}} = \\gamma\n",
    "$$\n",
    "\n",
    "Thus, \n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\hat{x}} = \\frac{\\partial L}{\\partial y} \\gamma\n",
    "$$\n",
    "\n",
    "In Python:\n",
    "```python\n",
    "grad_x_norm = grad_y * gamma # Element-wise multiplication\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient of Gamma\n",
    "The derivative of $y$ with respect to $\\gamma$ is:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial y}{\\partial \\gamma} = \\hat{x}\n",
    "$$\n",
    "\n",
    "Thus,\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\gamma} = \\sum^{m}_{i=1}\\frac{\\partial L}{\\partial y_{i}} \\cdot \\hat{x}_{i}\n",
    "$$\n",
    "\n",
    "We need to perform a sum across all training examples in the mini-batch and squash the shape `(N, M)` to `(M,)`\n",
    "\n",
    "In Python:\n",
    "```python\n",
    "grad_gamma = (grad_y * x_norm).sum(axis=0)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient of Beta\n",
    "The derivative of $y$ with respect to $\\beta$ is:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial y}{\\partial \\beta} = 1\n",
    "$$\n",
    "\n",
    "Thus,\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\beta} = \\sum^{m}_{i=1}\\frac{\\partial L}{\\partial y_{i}}\n",
    "$$\n",
    "\n",
    "We need to perform a sum across all training examples in the mini-batch and squash the shape `(N, M)` to `(M,)`\n",
    "\n",
    "In Python:\n",
    "```python\n",
    "grad_beta = grad_y.sum(axis=0)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient of Variance\n",
    "The derivative of $\\hat{x}$ with respect to $\\sigma^{2}$ is:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\hat{x}}{\\partial \\sigma^{2}} = \\frac{-1}{2} (x - \\mu) (\\sigma^{2} + \\epsilon)^{-3/2}\n",
    "$$\n",
    "\n",
    "Thus,\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\sigma^{2}} = \\sum^{m}_{i=1} \\frac{\\partial L}{\\partial \\hat{x}_{i}} (\\frac{-1}{2}) (x_{i} - \\mu) (\\sigma^{2} + \\epsilon)^{-3/2}\n",
    "$$\n",
    "\n",
    "We need to perform a sum across all training examples in the mini-batch and squash the shape `(N, M)` to `(M,)`\n",
    "\n",
    "In Python:\n",
    "```python\n",
    "dvar = (-0.5) * (x - mean) * (var + eps)**(-3.0/2)\n",
    "grad_var = np.sum(grad_x_norm * dvar, axis=0)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient of Mean\n",
    "We are going to use chain rule to solve for this gradient:\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\mu} = \\frac{\\partial L}{\\partial \\hat{x}} \\cdot \\frac{\\partial \\hat{x}}{\\partial \\mu} + \\frac{\\partial L}{\\partial \\sigma^{2}} \\cdot \\frac{\\partial \\sigma^{2}}{\\partial \\mu}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\hat{x}}{\\partial \\mu}  = \\frac{-1}{\\sqrt{\\sigma^{2} + \\epsilon}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\sigma^{2}}{\\partial \\mu} = \\frac{-2}{m}\\sum_{i=1}^{m} (x_{i} - \\mu)\n",
    "$$\n",
    "\n",
    "Thus,\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\mu} =  \\sum_{i=1}^{m} \\frac{\\partial L}{\\partial \\hat{x}_{i}} \\frac{-1}{\\sqrt{\\sigma^{2} + \\epsilon}} + \\frac{\\partial L}{\\partial \\sigma^{2}} \\frac{-2}{m}\\sum_{i=1}^{m} (x_{i} - \\mu)\n",
    "$$\n",
    "\n",
    "In Python:\n",
    "```python\n",
    "dxnorm_dmean = -1 / np.sqrt(var + eps)\n",
    "dvar_dmean = np.sum((-2 / x.shape[0]) * (x - mean), axis=0)\n",
    "grad_mean = np.sum(grad_x_norm * dxnorm_dmean, axis=0) + grad_var * dvar_dmean\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Gradient of Input\n",
    "Use chain rule again to solve for the final gradient:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial x} =  \\frac{\\partial L}{\\partial \\hat{x}} \\cdot \\frac{\\partial \\hat{x}}{\\partial x} + \\frac{\\partial L}{\\partial \\sigma^{2}} \\cdot \\frac{\\partial \\sigma^{2}}{\\partial x} + \\frac{\\partial L}{\\partial \\mu} \\cdot \\frac{\\partial \\mu}{\\partial x}\n",
    "$$\n",
    "\n",
    "Now fill in the missing pieces:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\hat{x}}{\\partial x} = \\frac{1}{\\sqrt{\\sigma^{2} + \\epsilon}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\sigma^{2}}{\\partial x} = \\frac{2 (x - \\mu)}{m}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mu}{\\partial x} = \\frac{1}{m}\n",
    "$$\n",
    "\n",
    "Now we just plug and chuck\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial x_{i}} =  \\frac{\\partial L}{\\partial \\hat{x}_{i}} \\cdot \\frac{1}{\\sqrt{\\sigma^{2} + \\epsilon}} + \\frac{\\partial L}{\\partial \\sigma^{2}} \\cdot \\frac{2 (x_{i} - \\mu)}{m} + \\frac{\\partial L}{\\partial \\mu} \\cdot \\frac{1}{m}\n",
    "$$\n",
    "\n",
    "In Python:\n",
    "```python\n",
    "dxnorm_dx = 1 / np.sqrt(var + eps)\n",
    "dvar_dx = 2 * (x - mean)\n",
    "dmean_dx = 1 / x.shape[0]\n",
    "grad_x = grad_x_norm * dxnorm_dx + grad_var * dvar_dx + grad_mean * dmean_dx\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simplification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
