{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Convolution Operation\n",
    "*Using 3x3 filter applied on a single channel 4x4 image*\n",
    "![conv](diagrams/conv.gif)\n",
    "\n",
    "## The Basics & Forward Pass\n",
    "### Filter\n",
    "Given an 32x32 image with RGB channels, we can represent it as a tensor of shape `(32, 32, 3)` which is (height, width, channels). When we perform convolution, we need a filter that has the same channel depth as the image. For example, we can use a 5x5 filter which is of shape `(5, 5, 3)` and slide it across the image left to right, top to bottom with a stride of 1 to perform convolution. \n",
    "\n",
    "### Padding\n",
    "Question is, since we are starting from the top left corner, what if the filter goes index out of bound? We can use zero padding in this case. Thus, now we have four hyperparameters:\n",
    "\n",
    "#### No padding\n",
    "Filter size of 3x3, stride is 1 and with no padding\n",
    "![conv_no_padding](diagrams/conv_no_padding.gif)\n",
    "\n",
    "#### With padding and stride of 2\n",
    "Filter size of 3x3, stride is 2 and with padding\n",
    "![conv_padding](diagrams/conv_padding.gif)\n",
    "\n",
    "* `F`: number of filters\n",
    "* `Hf` or `Wf`: spatial extend of the filters, which is 5 in this case\n",
    "* `S`: stride size\n",
    "* `P`: amount of padding\n",
    "\n",
    "Ignore the example in the picture and use the original example, let's assume `padding=2` and `stride=1`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padded input for a given image on a given color channel\n",
      "\n",
      "[[ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.         -2.17304004 ..., -0.90392633  0.          0.        ]\n",
      " ..., \n",
      " [ 0.          0.         -0.90080681 ..., -0.48944593  0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "\n",
      "Expected output tensor to have shape (10, 1, 32, 32)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\"\"\"\n",
    "- Input tensor is x, of shape (N, C, H, W) which is channel first. Give it N = 10, that is 10 pictures.\n",
    "- Filter tensor is denoted as weight, of shape (F, C, Hf, Wf). Give it F = 1, that is 1 filter.\n",
    "- Bias tensor is a one-dimensional vector of shape (F,) essentially one constant per filter.\n",
    "- Output tensor is y, of shape (N, F, H_out, W_out)\n",
    "\"\"\"\n",
    "\n",
    "pad = 2\n",
    "stride = 1\n",
    "x = np.random.randn(10, 3, 32, 32)\n",
    "weight = np.random.randn(1, 3, 5, 5)\n",
    "b = np.random.randn(1,)\n",
    "\n",
    "x_pad = np.pad(x, pad_width=((0, 0), (0, 0,), (pad, pad), (pad, pad)), mode='constant', constant_values=0)\n",
    "print 'Padded input for a given image on a given color channel\\n'\n",
    "print x_pad[0][0]\n",
    "\n",
    "N, _, H, W = x.shape\n",
    "F, _, Hf, Wf = weight.shape\n",
    "\n",
    "H_out = int(1 + (H + 2 * pad - Hf) / stride)\n",
    "W_out = int(1 + (W + 2 * pad - Wf) / stride)\n",
    "\n",
    "y = np.zeros((N, F, H_out, W_out))\n",
    "\n",
    "print '\\nExpected output tensor to have shape %s' % str(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Output Dimension\n",
    "From the calculation, we can see that given a tensor of shape `(W, H, C)`, convolution produces a tensor of shape `(W_out, H_out, F)`. The depth of the output tensor is dependent on number of filters being applied to the input.\n",
    "\n",
    "$$\n",
    "W_{output} = 1 + \\frac{W - F + 2P}{S}\n",
    "$$\n",
    "\n",
    "$$\n",
    "H_{output} = 1 + \\frac{H - F + 2P}{S}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# I don't recommend doing this in Python, the performance is terribly bad. This is for educational purpose.\n",
    "for n in range(N):\n",
    "    for f in range(F):\n",
    "        for h in range(H_out):\n",
    "            for w in range(W_out):\n",
    "                i, j = h * stride, w * stride\n",
    "                conv_sum = np.sum(x_pad[n][:, i:i+Hf, j:j+Wf] * weight[f])\n",
    "                y[n, f, h, w] = conv_sum + b[f]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Convolution Backpropagation\n",
    "The backward pass of a convolution operation (for both the input and weight) is also a convolution, but with spatially flipped filters. It is easy to derive using 1 dimensional example. \n",
    "\n",
    "Let's say we have x of shape `(3, 2, 2)` that is a 2x2 image with 3 channels, and a filter of shape `(3, 1, 1)` which is a one-pixel filter; just imagine the filter as `[weight[0], weight[1], weight[2]]`. When we perform convolution using `stride=1`. We can see that:\n",
    "\n",
    "$$\n",
    "y_{i, j} = \\Sigma_{f} x_{i, j, f} * w_{f}\n",
    "$$\n",
    "\n",
    "```\n",
    "y[0][0] = x[0][0][0] * weight[0] + x[0][0][1] * weight[1] + x[0][0][1] * weight[2]\n",
    "y[0][1] = x[0][1][0] * weight[0] + x[0][1][1] * weight[1] + x[0][1][2] * weight[2] \n",
    "y[1][0] = x[1][0][0] * weight[0] + x[1][0][1] * weight[1] + x[1][0][2] * weight[2]\n",
    "y[1][1] = x[1][1][0] * weight[0] + x[1][1][1] * weight[1] + x[1][1][2] * weight[2] \n",
    "```\n",
    "\n",
    "When we calculate derivative of loss with respect to each weight:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial y}\\frac{\\partial y}{\\partial w_{f}} = \\Sigma_{i, j} x_{i, j, f} \\frac{\\partial L}{\\partial y}_{i, j} \n",
    "$$\n",
    "\n",
    "```\n",
    "grad_weight[0] = x[0][0][0] * grad_y[0][0][0] + x[0][1][0] * grad_y[0][1][0] + x[1][0][0] * grad_y[1][0][0] + x[1][1][0] * grad_y[1][1][0]\n",
    "\n",
    "grad_weight[1] = ...\n",
    "\n",
    "grad_weight[2] = ...\n",
    "```\n",
    "\n",
    "Notice how this is flipped? For forward propoagation, we iterate through the number of filters for each pair of `{i, j}`. For back propagation, we iterate through each pair of `{i, j}` for every filter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "grad_weight = np.zeros(weight.shape)\n",
    "grad_x = np.zeros(x.shape)\n",
    "grad_b = np.zeros(b.shape)\n",
    "grad_x_pad = np.zeros(x_pad.shape)\n",
    "grad_y = np.ones(y.shape)\n",
    "\n",
    "for n in range(N):\n",
    "    for f in range(F):\n",
    "        for h in range(H_out):\n",
    "            for w in range(W_out):\n",
    "                i, j = h * stride, w * stride\n",
    "                grad_weight[f] += x_pad[n][:, i:i+Hf, j:j+Wf] * grad_y[n, f, h, w]\n",
    "                grad_x_pad[n][:, i:i+Hf, j:j+Wf] += weight[f] * grad_y[n, f, h, w]\n",
    "\n",
    "# Get rid of padding \n",
    "grad_x = grad_x_pad[:, :, pad:pad+H, pad:pad+W]\n",
    "\n",
    "# Compute gradient of bias\n",
    "for f in range(F):\n",
    "    grad_b[f] = np.sum(grad_y[:, f, :, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Transpose Convolution / Upsampling\n",
    "So far the convolution we have seen actually downsample the image, i.e. creating an output that is smaller in spatial dimension than the input. When I say spatial dimension, I am referring to the width and height of the tensor input. However, there are times we need to upsample and there is an operation called **transpose convolution** that does exactly this. \n",
    "\n",
    "Given a 4x4 input, using a 3x3 filter with `stride=1` and `pad=1`, we should expect an output of 4x4. Similarly, if we increase the stride size to 2. We should expect an output of 2x2. Now, a transpose convolution does the opposite. Given an input of 2x2, we produce an output of 4x4 using 3x3 transpose filter with `stride=2` and `pad=1`. What it does is that it takes one element of the input and multiplies it to the filter matrix as a scalar multiplcation on the filter. This is also called **fractionally strided convolution**. \n",
    "\n",
    "![transpose_conv](diagrams/transpose_conv.gif)\n",
    "\n",
    "```\n",
    "y[0][0] = weight[0][0] * x[0][0]\n",
    "y[0][1] = weight[0][1] * x[0][0]\n",
    "y[0][2] = weight[0][2] * x[0][0]\n",
    "y[1][0] = weight[1][0] * x[0][0]\n",
    "and so on...\n",
    "``` \n",
    "\n",
    "### Overlaps?\n",
    "We simply sum the overlapping regions.\n",
    "\n",
    "![transpose_conv_overlap](diagrams/transpose_conv_overlap.png)\n",
    "\n",
    "### One-dimensional Example\n",
    "Given an input array of `[a, b]` and a filter array of `[x, y, z]`, using `stride=2`. We can see that the output should be `y = [ax, ay, az + bx, by, bz]`\n",
    "\n",
    "$$\n",
    "W_{output} = (W - 1)S + F - 2P\n",
    "$$\n",
    "\n",
    "$$\n",
    "H_{output} = (H - 1)S + F - 2P\n",
    "$$\n",
    "\n",
    "Let's extend the example to a bit more complicated. Given an input array of `[a, b, c, d]` and a filter of `[x, y, z]`. We can express a *convolution* using matrix multiplication. Using `pad=1` and `stride=1`, notice that we are sliding `[x, y, z]` by one step to the right per row in the matrix on the left hand side.\n",
    "\n",
    "$$\n",
    "\\begin{vmatrix}\n",
    "x & y & z & 0 & 0 & 0 \\\\\n",
    "0 & x & y & z & 0 & 0 \\\\\n",
    "0 & 0 & x & y & z & 0 \\\\\n",
    "0 & 0 & 0 & x & y & z \\\\\n",
    "\\end{vmatrix}\n",
    "\\cdot\n",
    "\\begin{vmatrix}\n",
    "0 \\\\ a \\\\ b \\\\ c \\\\ d \\\\ 0\n",
    "\\end{vmatrix}\n",
    "=\n",
    "\\begin{vmatrix}\n",
    "ax + bz \\\\\n",
    "ax + by + cz \\\\\n",
    "bx + cy + dz \\\\\n",
    "cx + dy\n",
    "\\end{vmatrix}\n",
    "= \n",
    "\\begin{vmatrix}\n",
    "a^{'} \\\\ b^{'} \\\\ c^{'} \\\\ d^{'}\n",
    "\\end{vmatrix}\n",
    "$$\n",
    "\n",
    "Now let's perform a *transpose convolution*, with `stride=1`. Padding rule in transpose convolution is different. We cannot arbitrarily insert padding. It must correspond to the previous padded input prior to convolution transformation. \n",
    "\n",
    "$$\n",
    "\\begin{vmatrix}\n",
    "x & 0 & 0 & 0 \\\\\n",
    "y & x & 0 & 0 \\\\\n",
    "z & y & x & 0 \\\\\n",
    "0 & z & y & x \\\\ \n",
    "0 & 0 & z & y \\\\\n",
    "0 & 0 & 0 & z\n",
    "\\end{vmatrix}\n",
    "\\cdot\n",
    "\\begin{vmatrix}\n",
    "a^{'} \\\\ b^{'} \\\\ c^{'} \\\\ d^{'}\n",
    "\\end{vmatrix}\n",
    "= \n",
    "\\begin{vmatrix}\n",
    "a^{'}x \\\\\n",
    "a^{'}y + b^{'}x \\\\\n",
    "a^{'}z + b^{'}y + c^{'}x \\\\\n",
    "b^{'}z + c^{'}y + d^{'}x \\\\\n",
    "d^{'}z + d^{'}y \\\\\n",
    "d^{'}y\n",
    "\\end{vmatrix}\n",
    "$$\n",
    "\n",
    "The primary takeaway for transpose convolution is that it **RESTORES** the dimension of an input that was previously downsampled."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
