{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "least-demand",
   "metadata": {},
   "source": [
    "# ML Model Evaluation Tools & Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "victorian-riding",
   "metadata": {},
   "source": [
    "[Source Part 1](https://towardsdatascience.com/20-popular-machine-learning-metrics-part-1-classification-regression-evaluation-metrics-1ca3e282a2ce)\n",
    "\n",
    "[Source Part 2](https://towardsdatascience.com/20-popular-machine-learning-metrics-part-2-ranking-statistical-metrics-22c3e5a937b6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advised-basketball",
   "metadata": {},
   "source": [
    "## Classification Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alternative-source",
   "metadata": {},
   "source": [
    "### Confusion Matrix\n",
    "\n",
    "Confusion matrix is a tabular visualization of the model predictions versus ground truth labels. It's often useful to look at a confusion matrix for a quick idea of the recall and precision rate.\n",
    "\n",
    "For example, we have 1000 non-cat images and 100 cat images. We feed it into a classification model and receive the following result.\n",
    "\n",
    "![Confusion Matrix](./assets/confusion.png)\n",
    "\n",
    "We will use this example and its confusion matrix to derive the evaluation metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "french-application",
   "metadata": {},
   "source": [
    "### Accuracy\n",
    "\n",
    "Accuracy is the number of correct predictions divided by the total number of predictions.\n",
    "\n",
    "$$\n",
    "\\text{Accuracy} = \\frac{TP + TN}{TP + FP + TN + FN} = \\frac{90 + 940}{1100} = 0.936\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "configured-california",
   "metadata": {},
   "source": [
    "### Precision\n",
    "\n",
    "If your class distribution is imbalanced (i.e. one class is more frequently appearing than others), accuracy is not a reliable metric. If the model consistently predicts all samples as the most frequent class, the model has high accuracy but it is deceiving. We need to use precision to understand the model's performance.\n",
    "\n",
    "$$\n",
    "\\text{Precision}_\\text{positive} = \\frac{TP}{TP + FP} = \\frac{90}{90+60} = 0.60\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Precision}_\\text{negative} = \\frac{TN}{TN + FN} = \\frac{940}{940+10} = 0.989\n",
    "$$\n",
    "\n",
    "We can see that the model is not performing well on detecting cat. As we optimize for precision, our model might become more \"conservative\" in what it considers to be a \"cat\". This will cause our recall score to drop (see next section)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "false-newport",
   "metadata": {},
   "source": [
    "### Recall\n",
    "\n",
    "Recall is the fraction of samples from a class which are correctly predicted by the model. For a cat image, how often does the model predict correctly? For a non-cat image, how often does the model predict correctly?\n",
    "\n",
    "$$\n",
    "\\text{Recall}_\\text{positive} = \\frac{TP}{TP + FN} = \\frac{90}{90+10} = 0.90\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Recall}_\\text{negative} = \\frac{TN}{TN + FP} = \\frac{940}{940+60} = 0.94\n",
    "$$\n",
    "\n",
    "High recall generally means that we try to minimize false negative by predicting more positive even if they are false positive. This will cause our precision to drop.\n",
    "\n",
    "> If the cost for a FP is low, e.g. detecting cancer for a patient, then we should optimize\n",
    "  for recall. This is because the cost for FN is high in this scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "geological-fraud",
   "metadata": {},
   "source": [
    "### F1 Score\n",
    "\n",
    "Depending on the application, it may need a higher priority for recall or precision. But there are many applications in which both recall and precision are important. Therefore, it is natural to think of a way to combine them into one single score.\n",
    "\n",
    "**F1** is the harmonic mean of precision and recall.\n",
    "\n",
    "$$\n",
    "\\text{F1} = \\frac{2 * \\text{Precision} * \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
    "$$\n",
    "\n",
    "There is always a trade-off between precision and recall of a model. If you want to make the precision high, you should expect to see a drop in recall, vice versa. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gorgeous-circuit",
   "metadata": {},
   "source": [
    "### Sensitivity and Specificity\n",
    "\n",
    "Sensitivity and specificity are just recalls for true positive and true negative.\n",
    "\n",
    "$$\n",
    "\\text{Sensitivity} = \\text{True Positive Rate} = \\frac{TP}{TP + FN}\n",
    "$$ \n",
    "\n",
    "$$\n",
    "\\text{Specificity} = \\text{True Negative Rate} = \\frac{TN}{TN + FP}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "split-horizon",
   "metadata": {},
   "source": [
    "### ROC Curve\n",
    "\n",
    "The **receiver operating characteristic** curve is a plot which shows the performance of a binary classifier as function of its cut-off threshold. It shows the true positive rate against the false positive rate for various threshold values.\n",
    "\n",
    "Classification models produce probabilities for samples as predictions. The models compare the output probability with some cut-off threshold to decide whether the output is positive or negative. For example, a model may predict `[0.45, 0.60, 0.70, 0.30]` for 4 sample images. \n",
    "\n",
    "- If `cut-off=0.5` then predicted labels are `[0, 1, 1, 0]`\n",
    "- If `cut-off=0.2` then predicted labels are `[1, 1, 1, 1]`\n",
    "- If `cut-off=0.8` then predicted labels are `[0, 0, 0, 0]`\n",
    "\n",
    "The cut-off rate will directly affect the precision and recall rates. The graph will look like the following.\n",
    "\n",
    "![ROC Curve](https://upload.wikimedia.org/wikipedia/commons/3/36/Roc-draft-xkcd-style.svg)\n",
    "\n",
    "ROC curve is a useful tool for picking the best cut-off threshold for the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oriental-spare",
   "metadata": {},
   "source": [
    "### AUC\n",
    "\n",
    "The **area under of the curve** is an aggregated measure of performance of a binary classifier on all possible threshold values (and therefore it is threshold invariant). AUC is an integral over all threshold values over the ROC curve. One way to interpreting AUC is _the probability that the model ranks a random positive example more highly than a random negative example_. A model whose predictions are 100% wrong has an AUC of 0.0, one whose predictions are 100% correct has an AUC of 1.0.\n",
    "\n",
    "![Area Under the Curve](https://developers.google.com/machine-learning/crash-course/images/AUC.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "activated-segment",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
