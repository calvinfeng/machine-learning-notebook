{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59f9f0a7",
   "metadata": {},
   "source": [
    "# Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7236186",
   "metadata": {},
   "source": [
    "![Transformer](assets/transformer.png)\n",
    "\n",
    "With multi-head attention out of the way, now we can stack these attention units together to create a transformer architecture. There are 2 phases, encoding and decoding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0a1c4f",
   "metadata": {},
   "source": [
    "## Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098d0062",
   "metadata": {},
   "source": [
    "We obtain the embeddings $X$ for an input sequence, using the same example from Andrew Ng's lecture i.e. \"Jane viste l'Afrique en septembre\". \n",
    "\n",
    "$X$, in conjunction with $Q$, $K$, and $V$, is used to compute the output of the heads. The output will be concatenated, summed with residuals, and normalized. The normalized outputs will then be fed into a non-linear unit such as a simple feedfoward neural network. The output of the neural network will be summed with residuals and normalized again. The encoder block will be repeated multiple times. Each tiem the output should have the same shape as $Q$, $K$, $V$ just like the inputs to each block.\n",
    "\n",
    "<img src=\"assets/encoder.png\" alt=\"Encoder\" width=\"400\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7366e259",
   "metadata": {},
   "source": [
    "## Decoding\n",
    "\n",
    "First the `SOS` start of sentence token is fed into the decoder generate the query matrix $Q$ while discarding the $K$ and $V$ matrix. The decoded sequence is always the \"query\" while the keys and values should be found in the encoded sequences. The decoding attention blocks will run `N` times just like the encoding attention blocks. \n",
    "\n",
    "<img src=\"assets/decoder.png\" alt=\"Encoder\" width=\"600\"/>\n",
    "\n",
    "The feed-forward network in the decoder should generate the next word in the seqeuence, i.e. `Jane` ideally. Then `SOS` and `Jane` are fed into the decoder blocks again and repeat the same logic until the `EOS` is reached."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854e9813",
   "metadata": {},
   "source": [
    "## PyTorch Example\n",
    "\n",
    "Source: https://pytorch.org/tutorials/beginner/transformer_tutorial.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
