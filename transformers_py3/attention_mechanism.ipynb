{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa37fa60",
   "metadata": {},
   "source": [
    "# Attention Mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d1909b",
   "metadata": {},
   "source": [
    "[Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473) is also known as the additive attention. Attention is an extension to encoder-decoder model which learns to align and translate jointly. Each time the model generates a word in a translation, it searches for a set of positions in a source sentence where the most relevant information is concentrated. The model then predicts a target word based on the context vectors associated with these source positions and all the previous generated target words.\n",
    "\n",
    "The models that came before this idea tried to encode the whole input sentence into a single fixed-length vector. The new attention mechanism encodes the input sentence into a sequence of vectors and chooses only a subset of these vectors adaptively during decoding phase."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196a53aa",
   "metadata": {},
   "source": [
    "## RNN Encoder-Decoder\n",
    "\n",
    "Suppose we have a RNN based encoder-decoder network, the encoder reads the input sentence, a sequence of vectors\n",
    "$x = (x_1, x_2, ..., x_{T})$ into a context vector $c$. For every step in the encoding phase, the network generates a hidden state vector at step `t`.\n",
    "\n",
    "$$\n",
    "h_t = f\\,(x_t, h_{t-1})\n",
    "$$\n",
    "\n",
    "The context vector is then generated from the sequence of hidden states.\n",
    "\n",
    "$$\n",
    "c = q\\,({h_1, h_2, ..., h_T})\n",
    "$$\n",
    "\n",
    "$f$ and $q$ are some nonlinear functions that represent the network. The decoder is trained to predict the next word $y_t$ given the context vector and all the previously predicted words $(y_1, y_2, ..., y_{t-1})$. In other words, the decoder defines a probability over the translation $y$ by decomposing the joint probability into the ordered conditionals.\n",
    "\n",
    "$$\n",
    "p\\,(y) = \\prod_{t=1}^T p\\,(y_t \\mid y_1, y_2, ..., y_{t-1}, c)\n",
    "$$\n",
    "\n",
    "The $y$ represents the whole translated sentence. Each conditional probability is modeled as\n",
    "\n",
    "$$\n",
    "p\\,(y_t \\mid y_1, y_2, ..., y_{t-1}, c) = g\\,(y_{t-1}, s_t, c)\n",
    "$$\n",
    "\n",
    "where $s_t$ is the hidden state of the RNN network.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4eb78e",
   "metadata": {},
   "source": [
    "## Learning to Align\n",
    "\n",
    "The paper proposes a new model architecture where it uses a bi-directional RNN as an encoder/decoder. Each conditional probability is defined as:\n",
    "\n",
    "$$\n",
    "p\\,(y_i \\mid y_1, y_2, ..., y_{i-1}, x) = g\\,(y_{i-1}, s_{i}, c_{i})\n",
    "$$\n",
    "\n",
    "$s_i$ is the hidden state at time `i` which is computed by\n",
    "\n",
    "$$\n",
    "s_i = f\\,(s_{i-1}, y_{i-1}, c_i)\n",
    "$$\n",
    "\n",
    "Notice that the probability is conditioned on a **distinct** context vector $c_i$ for each target word output. The context vector depends on a sequence of annotations $(h_1, h_2, ..., h_T)$ to which an encoder maps the input sentence. Each annotation $h_i$ contains information about the whole input sequence with a strong focus on the parts surrounding the `i`th word of the input sequence. These are effectively the hidden states of the encoder at time step `t`.\n",
    "\n",
    "The context vector is a weighted sum of these annotations.\n",
    "\n",
    "$$\n",
    "c_i = \\Sigma_{j=1}^{T} \\alpha_{ij}h_j\n",
    "$$\n",
    "\n",
    "The weight $\\alpha_{ij}$ for each annotation $h_j$ is computed by a softmax function.\n",
    "\n",
    "$$\n",
    "\\alpha_{ij} = \\frac{\\text{exp}\\,(e_{ij})}{ \\Sigma_{k=1}^T \\text{exp}\\, (e_{ik})}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "e_{ij} = a\\,(s_{i-1}, h_j)\n",
    "$$\n",
    "\n",
    "The $a$ is called an alignment model which scores ho wwell the inputs around position `j` and the output at position `i` match. $a$ is modeled as a feedforward fully connected neural network which is jointly trained with all the other components of the system.\n",
    "\n",
    "> So why use a bi-directional RNN?\n",
    "\n",
    "Annotation of each word should summarize not only the preceding words, but also the following words. The bi-directional allows such information to propagate from left to right and right to left.\n",
    "\n",
    "The forward RNN reads the input sequences from left to right and calculates a forward hidden state sequence. The backward RNN reads the input sequences from right to left and calculates a backward hidden state sequence. The annotation for each word is the concatenation of the forward and backward hidden states.\n",
    "\n",
    "> Why is it additive?\n",
    "\n",
    "Because the context vector is a weighted sum of annotations.\n",
    "\n",
    "![Bahdanau Attention Model](assets/bahdanau.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15398ca9",
   "metadata": {},
   "source": [
    "## Encoder and Decoder in PyTorch\n",
    "\n",
    "Source: [Practical PyTorch: Translation with a Sequence to Sequence Network and Attention](https://github.com/spro/practical-pytorch/blob/master/seq2seq-translation/seq2seq-translation.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c049dcbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_layers=1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers)\n",
    "        \n",
    "    def forward(self, word_inputs, hidden):\n",
    "        # Note: we run this all at once (over the whole input sequence)\n",
    "        seq_len = len(word_inputs)\n",
    "        embedded = self.embedding(word_inputs).view(seq_len, 1, -1)\n",
    "        output, hidden = self.gru(embedded, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        hidden = Variable(torch.zeros(self.n_layers, 1, self.hidden_size))\n",
    "        if USE_CUDA: hidden = hidden.cuda()\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bdd06ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, n_layers=1, dropout_p=0.1):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        \n",
    "        # Define parameters\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # Define layers\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.attn = GeneralAttn(hidden_size) # Attention unit should compute the softmaxed weights aka alpha.\n",
    "        self.gru = nn.GRU(hidden_size * 2, hidden_size, n_layers, dropout=dropout_p)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, word_input, last_hidden, encoder_outputs):\n",
    "        # Note that we will only be running forward for a single decoder time step, but will use all\n",
    "        # encoder outputs\n",
    "        \n",
    "        # Get the embedding of the current input word (last output word)\n",
    "        word_embedded = self.embedding(word_input).view(1, 1, -1) # S=1 x B x N\n",
    "        word_embedded = self.dropout(word_embedded)\n",
    "        \n",
    "        # Calculate attention weights and apply to encoder outputs (which are the hidden states h)\n",
    "        attn_weights = self.attn(last_hidden[-1], encoder_outputs) \n",
    "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1)) # B x 1 x N \n",
    "        \n",
    "        # Combine embedded input word and attended context, run through RNN\n",
    "        rnn_input = torch.cat((word_embedded, context), 2)\n",
    "        output, hidden = self.gru(rnn_input, last_hidden)\n",
    "        \n",
    "        # Final output layer\n",
    "        output = output.squeeze(0) # B x N\n",
    "        output = F.log_softmax(self.out(torch.cat((output, context), 1)))\n",
    "        \n",
    "        # Return final output, hidden state, and attention weights (for visualization)\n",
    "        return output, hidden, attn_weights"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
