{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec - The Skip-Gram Model\n",
    "\n",
    "The idea of word2vec is actually quite simple. We want to train a 2-layer neural network to perform a fake task. The weights in the hidden layer will become our embedding vectors for our vocabulary in the corpus.\n",
    "\n",
    "## Fake Task\n",
    "\n",
    "Given a specific word in the middle of a sentence, find the probability for every word in our vocabulary of being the nearby *word*.\n",
    "\n",
    "For example, I have a sentence.\n",
    "\n",
    "> Live as if you were to die tomorrow and learn as if you were to live forever.\n",
    "\n",
    "I pick *tomorrow* to be my center word. Now the neural network is supposed to tell me what is the probability of *die* being the nearby word of *tomorrow*. If it is properly trained, it should be close to one. Similarly if I ask the neural network, what is the probability of *live* being the nearby word of *tomorrow*, it should be close to zero.\n",
    "\n",
    "## Architecture\n",
    "\n",
    "![skip-gram-model](./assets/skip-gram-model.png)\n",
    "\n",
    "### Input\n",
    "\n",
    "The input is a word that gets translated into one-hot encoding. The encoding vector should have length of `V` where `V` represents the total vocabulary length. For example, if I have 10 unique words in my vocabulary, one of the words will be encoded as the following.\n",
    "\n",
    "    [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "    \n",
    "### Output\n",
    "\n",
    "Each output is a vector of same length `V`; it contains the probability for all the words that they are the nearby word of the input center word. The output vector will contain float ranging from 0 to 1. \n",
    "\n",
    "    [0.1, 0.23, 0.64, 0.45, 0, 0.523, 0.4, 0.9, 0.34, 0.85]\n",
    "\n",
    "### Lookup Table\n",
    "\n",
    "Our neural network has two layers and two sets of weights. Suppose we have 1000 words in our vocabulary and 300 is our feature dimension. The first set of weights from the hidden layer will be our word vector lookup table after we finish training.\n",
    "\n",
    "![word2vec-lookup-table](./assets/word2vec-lookup-table.png)\n",
    "\n",
    "### Implementation\n",
    "\n",
    "Now we are ready to actually create the model for performing such task. Let's define our word vector feature length to be `D=100`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  7.10131056e-13,   2.47594653e-06,   2.53290408e-12,\n",
       "          2.20996587e-06,   2.30141869e-01,   9.49824842e-02,\n",
       "          3.22236345e-05,   1.60437264e-02,   1.27058846e-05,\n",
       "          1.88410823e-01,   8.26042585e-13,   4.57722072e-08,\n",
       "          5.02238406e-08,   5.09131038e-05,   3.05286306e-08,\n",
       "          6.70482585e-18,   7.26135416e-10,   4.76799976e-03,\n",
       "          1.43687960e-12,   4.89209703e-03,   4.98483157e-06,\n",
       "          9.29606849e-03,   1.55444970e-17,   9.37455733e-10,\n",
       "          2.82616201e-12,   1.36136316e-04,   5.78575661e-14,\n",
       "          2.58469383e-02,   1.24313812e-12,   8.67508145e-14,\n",
       "          1.77377765e-22,   7.86738822e-15,   9.52655197e-14,\n",
       "          7.63781212e-13,   1.46723086e-02,   3.13115174e-13,\n",
       "          2.83290437e-04,   1.83244196e-05,   1.28662427e-16,\n",
       "          2.75454297e-15,   1.47283303e-15,   1.07521354e-12,\n",
       "          3.80998775e-03,   7.74221786e-11,   1.26412695e-04,\n",
       "          3.57877099e-06,   3.96060206e-01,   7.15494708e-11,\n",
       "          1.29623645e-12,   7.72861156e-20,   1.70229640e-11,\n",
       "          1.36012958e-07,   2.13267985e-12,   6.58457396e-03,\n",
       "          3.81377234e-03,   1.06339654e-08,   3.61402187e-06,\n",
       "          2.60113612e-15,   2.81161858e-14,   1.21710265e-19]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "corpus = \"\"\"\n",
    "The Porsche Boxster is a mid-engined two-seater roadster. It was Porsche's first road vehicle to be\n",
    "originally designed as a roadster since the 550 Spyder. The first-generation Boxster was introduced in\n",
    "late 1996; it was powered by a 2.5-litre flat six-cylinder engine. The design was heavily influenced by the 1992\n",
    "Boxster Concept. In 2000, the base model was upgraded to a 2.7-litre engine and the new Boxster S variant was\n",
    "introduced with a 3.2-litre engine. In 2003, styling and engine output was upgraded on both variants.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    if len(x.shape) > 1:\n",
    "        x = x - np.max(x, axis=1, keepdims=True)\n",
    "        denom = np.sum(np.exp(x), axis=1, keepdims=True)\n",
    "        x = np.exp(x) / denom\n",
    "    else:\n",
    "        x = x - np.max(x)\n",
    "        denom = np.sum(np.exp(x))\n",
    "        x = np.exp(x) / denom\n",
    "    \n",
    "    return x\n",
    "    \n",
    "    \n",
    "class Model(object):\n",
    "    def __init__(self, corpus, feature_dim):\n",
    "        self.add_corpus(corpus)\n",
    "        self.V = len(self.lookup_table)\n",
    "        self.D = feature_dim\n",
    "        self.w1 = np.random.randn(self.V, self.D)\n",
    "        self.w2 = np.random.randn(self.D, self.V)\n",
    "    \n",
    "    def add_corpus(self, corpus):\n",
    "        self.lookup_table = dict()\n",
    "        idx = 0\n",
    "        for word in set(corpus.split()):\n",
    "            self.lookup_table[word] = idx\n",
    "            idx+=1\n",
    "    \n",
    "    def forward(self, word):\n",
    "        if self.lookup_table.get(word) is None:\n",
    "            return\n",
    "        \n",
    "        idx = self.lookup_table.get(word)\n",
    "        wordvec = np.array([0]*self.V).reshape(1, self.V)\n",
    "        wordvec[0][idx] = 1\n",
    "        \n",
    "        out1 = np.dot(wordvec, self.w1)\n",
    "        out2 = np.dot(out1, self.w2)\n",
    "        out3 = softmax(out2)\n",
    "        \n",
    "        return out3\n",
    "        \n",
    "        \n",
    "model = Model(corpus, 100)\n",
    "model.forward('Porsche')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "If we have a giant corpus with one million unique vocabulary words, and our feature dimension is defined to be 100. We will have a matrix that has 100 million entries. This is not going to fit in memory. We will also have serious trouble with computing the matrix multiplication."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
