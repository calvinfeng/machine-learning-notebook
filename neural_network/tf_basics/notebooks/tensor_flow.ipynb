{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# TensorFlow\n",
    "## Computational Graph\n",
    "Start with a simple architecture to outline how strikingly similar numpy and tensorflow are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "![computational graph](compu_graph.png)\n",
    "\n",
    "The following is forward prop and gradient computation in `numpy`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient x: [[ 0.76103773  0.12167502  0.44386323  0.33367433]\n",
      " [ 1.49407907 -0.20515826  0.3130677  -0.85409574]\n",
      " [-2.55298982  0.6536186   0.8644362  -0.74216502]]\n",
      "Graident y: [[ 1.76405235  0.40015721  0.97873798  2.2408932 ]\n",
      " [ 1.86755799 -0.97727788  0.95008842 -0.15135721]\n",
      " [-0.10321885  0.4105985   0.14404357  1.45427351]]\n",
      "Gradient z: [[ 1.  1.  1.  1.]\n",
      " [ 1.  1.  1.  1.]\n",
      " [ 1.  1.  1.  1.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "N, D = 3, 4\n",
    "\n",
    "x = np.random.randn(N, D)\n",
    "y = np.random.randn(N, D)\n",
    "z = np.random.randn(N, D)\n",
    "\n",
    "# Forward prop\n",
    "a = x * y\n",
    "b = a + z\n",
    "c = np.sum(b)\n",
    "\n",
    "# Backward prop\n",
    "grad_c = 1 # Gradient of c with respect to c \n",
    "grad_b = grad_c * np.ones((N, D))\n",
    "grad_a = grad_b.copy()\n",
    "grad_z = grad_b.copy() \n",
    "grad_x = grad_a * y\n",
    "grad_y = grad_a * x\n",
    "\n",
    "print \"Gradient x:\", grad_x\n",
    "print \"Graident y:\", grad_y\n",
    "print \"Gradient z:\", grad_z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The following is forward prop and gradient computation in `tf`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient x: [[ 0.76103771  0.12167501  0.44386324  0.33367434]\n",
      " [ 1.49407911 -0.20515826  0.3130677  -0.85409576]\n",
      " [-2.55298972  0.65361857  0.86443621 -0.74216503]]\n",
      "Graident y: [[ 1.76405239  0.40015721  0.97873801  2.24089313]\n",
      " [ 1.867558   -0.97727787  0.95008844 -0.1513572 ]\n",
      " [-0.10321885  0.41059852  0.14404356  1.45427346]]\n",
      "Gradient z: [[ 1.  1.  1.  1.]\n",
      " [ 1.  1.  1.  1.]\n",
      " [ 1.  1.  1.  1.]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "np.random.seed(0)\n",
    "\n",
    "N, D = 3, 4\n",
    "\n",
    "x = tf.placeholder(tf.float32)\n",
    "y = tf.placeholder(tf.float32)\n",
    "z = tf.placeholder(tf.float32)\n",
    "\n",
    "a = x * y\n",
    "b = a + z \n",
    "c = tf.reduce_sum(b)\n",
    "\n",
    "grad_x, grad_y, grad_z = tf.gradients(c, [x, y, z])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    values = {\n",
    "        x: np.random.randn(N, D),\n",
    "        y: np.random.randn(N, D),\n",
    "        z: np.random.randn(N, D),\n",
    "    }\n",
    "\n",
    "    out = sess.run([c, grad_x, grad_y, grad_z], feed_dict=values)\n",
    "    c_val, grad_x_val, grad_y_val, grad_z_val = out\n",
    "\n",
    "print \"Gradient x:\", grad_x_val\n",
    "print \"Graident y:\", grad_y_val\n",
    "print \"Gradient z:\", grad_z_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Silly Neural Network\n",
    "The following is an example of building a silly fully connected 2-layer neural network with TensorFlow. As you can see that the code isn't that much different from `numpy` as like the examples above. TensorFlow shares many similar API with `numpy`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N, D, H = 64, 1000, 100\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=(N, D))\n",
    "y = tf.placeholder(tf.float32, shape=(N, D))\n",
    "w1 = tf.placeholder(tf.float32, shape=(D, H))\n",
    "w2 = tf.placeholder(tf.float32, shape=(H, D))\n",
    "\n",
    "h = tf.maximum(tf.matmul(x, w1), 0)\n",
    "y_pred = tf.matmul(h, w2)\n",
    "diff = y_pred - y\n",
    "loss = tf.reduce_mean(tf.reduce_sum(diff ** 2, axis=1))\n",
    "\n",
    "grad_w1, grad_w2 = tf.gradients(loss, [w1, w2])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    values = {\n",
    "        x: np.random.randn(N, D),\n",
    "        y: np.random.randn(N, D),\n",
    "        w1: np.random.randn(D, H),\n",
    "        w2: np.random.randn(H, D),\n",
    "    }\n",
    "    \n",
    "    out = sess.run([loss, grad_w1, grad_w2], feed_dict=values)\n",
    "    loss_val, grad_w1_val, grad_w2_val = out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The workflow in `tf` is usually splitted into two. First, we define the computational graph and decide on what are the gradients we are looking for, e.g. `grad_w1, grad_w2 = tf.gradients(loss, [w1, w2])`.\n",
    "\n",
    "```python\n",
    "x = tf.placeholder(tf.float32, shape=(N, D))\n",
    "y = tf.placeholder(tf.float32, shape=(N, D))\n",
    "w1 = tf.placeholder(tf.float32, shape=(D, H))\n",
    "w2 = tf.placeholder(tf.float32, shape=(H, D))\n",
    "h = tf.maximum(tf.matmul(x, w1), 0)\n",
    "y_pred = tf.matmul(h, w2)\n",
    "diff = y_pred - y\n",
    "loss = tf.reduce_mean(tf.reduce_sum(diff ** 2, axis=1))\n",
    "grad_w1, grad_w2 = tf.gradients(loss, [w1, w2])\n",
    "```\n",
    "\n",
    "And then we run the graph many times and perform updates on them\n",
    "```python\n",
    "with tf.Session() as sess:\n",
    "    values = {\n",
    "        x: np.random.randn(N, D),\n",
    "        y: np.random.randn(N, D),\n",
    "        w1: np.random.randn(D, H),\n",
    "        w2: np.random.randn(H, D),\n",
    "    }\n",
    "    \n",
    "    out = sess.run([loss, grad_w1, grad_w2], feed_dict=values)\n",
    "    loss_val, grad_w1_val, grad_w2_val = out\n",
    "```\n",
    "\n",
    "Remember that whatever `placeholder` we used, we need to feed them with values using the key word argument `feed_dict` in the `run()` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bottleneck\n",
    "Notice that vanilla `numpy` only works with CPU, the output of `run()` is in numpy array. This creates a problem when we want to run our `tf` code on GPU because we are basically copying values from GPU to CPU and CPU back to GPU. This can create a bottleneck on huge dataset. \n",
    "\n",
    "There is a solution, instead of using `placeholder`, use `Variable`.\n",
    "\n",
    "```python\n",
    "w1 = tf.Variable(tf.random_normal((D, H)))\n",
    "w2 = tf.Variable(tf.random_normal((H, D)))\n",
    "``` \n",
    "\n",
    "These variables are values that live inside the computational graph and will persist throughout training. And then we need to specify how we'd like to update these variables per iteration through `session.Run()` in Tensorflow.\n",
    "\n",
    "```python\n",
    "grad_w1, grad_w2 = tf.gradients(loss, [w1, w2])\n",
    "new_w1 = w1.assign(w1 - learning_rate * grad_w1)\n",
    "new_w2 = w2.assign(w2 - learning_rate * grad_w2)\n",
    "```\n",
    "\n",
    "In order to actually compute and update these values, we need to assign them to a dummy node and tell tensor flow that we need the node to be computed in each iteration.\n",
    "\n",
    "```python\n",
    "weight_updates = tf.group(new_w1, new_w2)\n",
    "with tf.Session() as sess:\n",
    "    for t in range(50):\n",
    "        loss_val, _ = sess.run([loss, weight_updates], feed_dict=values)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF Optimizer\n",
    "Tensor flow actually gives us API to run gradient descent. The API lives inside `tf.train`. Basically what it does is similar to what's written above. It performs the updates by calling `assign` on the `tf.Variable`s and then group them into a dummy node and execute the computation per iteration.\n",
    "\n",
    "```python\n",
    "optimizer = tf.train.GradientDescentOptimizer(1e-5)\n",
    "weight_updates = optimizer.minimize(loss)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    values = { x: np.random.randn(N, D), y: np.random.randn(N, D) }\n",
    "    losses = []\n",
    "    for t in range(50):\n",
    "        loss_val, _ = sess.run([loss, weight_updates], feed_dict=values)\n",
    "        losses.append(loss_val)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF Loss\n",
    "If `tf` gives optimizier, it must also other conveninent functions like L1 and L2 losses. That's right, it does!\n",
    "\n",
    "```python\n",
    "loss = tf.losses.mean_squared_error(y_pred, y)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF Layers\n",
    "So far we have omitted all the nitty-gritty details of defining biases and performing xavier initialization for simplicity. It'd take a good amount of code to carefully piece everything together if we were to write them from scratch. And again, `tf` provides everything out of the box for us!\n",
    "\n",
    "```python \n",
    "init = tf.contrib.layers.xavier_initializer()\n",
    "h = tf.layers.dense(inputs=x, units=H, activation=tf.nn.relu, kernel_initializer=init)\n",
    "y_pred = tf.layers.dense(inputs=h, units=D, kernel_initializer=init)\n",
    "loss = tf.losses.mean_squared_error(y_pred, y)\n",
    "```\n",
    "\n",
    "This `tf.layers` library provides architectural setup for us so we don't have to create the layer manually. All the biases and weight initializations are set right out of the box!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras\n",
    "I guess I don't have to write much about this one, it's so popular these days. \n",
    "\n",
    "\n",
    "```python\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "N, D, H = 64, 1000, 100\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(input_dim=D, output_dim=H))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(input_dim=H, output_dim=D))\n",
    "\n",
    "optimizer = SGD(lr=1e0)\n",
    "model.compile(loss='mean_squared_error', optimizer=optimizer)\n",
    "\n",
    "x = np.random.randn(N, D)\n",
    "y = np.random.randn(N, D)\n",
    "history = model.fit(x, y, nb_epoch=50, batch_size=N, verbose=0)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
