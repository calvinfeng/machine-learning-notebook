{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3fb5ff1",
   "metadata": {},
   "source": [
    "# Nearest Neighbor Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d88c65f",
   "metadata": {},
   "source": [
    "## Define Nearest Neighbor Search\n",
    "\n",
    "Given a set of points ${x_1, ..., x_n} \\in \\mathbb{R}^d$, preprocess them into a data structure $X$ of size `polynomial(n, d)` in time `polynomial(n, d)` such that nearest neighbor queries can be performed in logarithmic time. In other words, given a search query point `q`, radius `r`, and $X$, one can return all $x_i$ such that\n",
    "\n",
    "$$\n",
    "\\left \\|\n",
    "{q - x_i} \\leq r\n",
    "\\right \\|\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79bff43",
   "metadata": {},
   "source": [
    "## Find Nearest Neighbor in 2D\n",
    "\n",
    "Let's assume that we have a random set of points `N = 100`. If we represent these points in a 2D feature space with $x_1 \\in [-1, 1]$ and $x_2 \\in [-1, 1]$. We want to find all the nearest neighbor within a circle of `radius=1`.  For the purpose of demsontration, I will perform a linear search to find all the exact matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e31b9ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "number of nearest neighbor within Euclidean distance 1.0: 41\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "N = 100\n",
    "\n",
    "def search_nn_by_dimension(num_points, dim):\n",
    "    points = np.random.randn(num_points, dim)\n",
    "    radius = 1\n",
    "    nearest = []\n",
    "    for p in points:\n",
    "        if np.sqrt(p.dot(p)) <= 1:\n",
    "            nearest.append(p)\n",
    "    return len(nearest)\n",
    "\n",
    "print(f\"\"\"\n",
    "number of nearest neighbor within Euclidean distance 1.0: {search_nn_by_dimension(N, 2)}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353fce79",
   "metadata": {},
   "source": [
    "What if I increase the dimension?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ecf3e9a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D=3 number of nearest neighbor within Euclidean distance 1.0: 21\n",
      "D=4 number of nearest neighbor within Euclidean distance 1.0: 12\n",
      "D=5 number of nearest neighbor within Euclidean distance 1.0: 5\n",
      "D=6 number of nearest neighbor within Euclidean distance 1.0: 2\n",
      "D=7 number of nearest neighbor within Euclidean distance 1.0: 0\n",
      "D=8 number of nearest neighbor within Euclidean distance 1.0: 0\n",
      "D=9 number of nearest neighbor within Euclidean distance 1.0: 0\n"
     ]
    }
   ],
   "source": [
    "for d in range(3, 10):\n",
    "    print(f\"D={d} number of nearest neighbor within \" +\n",
    "          f\"Euclidean distance 1.0: {search_nn_by_dimension(N, d)}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2017a863",
   "metadata": {},
   "source": [
    "As we increase the dimensionality of feature space, the data points are getting further apart. Nearest neighbor search within a radius `r` will start to reduce its effectiveness. In order to more accurately find nearest neighbor, the search radius must increase but it will also result in higher time complexity for the exact search.\n",
    "\n",
    "> The curse of dimensionality complicates nearest neighbor search in high dimensional space. It is not possible to quickly reject candidates by using the difference in one coordinate as a lower bound for a distance based on all the dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17de68c",
   "metadata": {},
   "source": [
    "## But Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c577f0a",
   "metadata": {},
   "source": [
    "Consider feature space as a hyper-cube, the volume of a hyper-cube is simply\n",
    "\n",
    "$$\n",
    "V_{\\text{hypercube}} = s^d\n",
    "$$\n",
    "\n",
    "where $d$ is dimension and $l$ is the length of the side. What about a hypercube? It involves a gamma function $\\Gamma$.\n",
    "\n",
    "$$\n",
    "V_{\\text{hypersphere}} = \\frac{\\pi^{d/2} r^d}{\\Gamma(\\frac{d}{2} + 1)}\n",
    "$$\n",
    "\n",
    "Using the previous example, let's assume we want to search within the radius of $\\frac{s}{2}$. The points are randomly scattered in the feature space, so the probability of finding a point inside the search radius is the ratio of hypersphere volume to hypercube volume.\n",
    "\n",
    "$$\n",
    "\\frac{V_{\\text{hypersphere}}}{V_{\\text{hypercube}}} = \\frac{\\pi^{d/2}}{2^d\\Gamma(\\frac{d}{2} + 1)}\n",
    "$$\n",
    "\n",
    "What does this mean? The probability of finding points inside the search radius decreases as the dimension increases! THe rate decreases exponentially too.\n",
    "\n",
    "> There are no known exact methods for finding nearest neighbors efficiently. As both the number of points increases and the number of dimensions increase, we fall victim to the curse of dimensionality. In high dimensions, all points are almost equally distant from each other. A good enough solution for many applications is to trade accuracy for efficiency. In approximately nearest neighbors (ANN), we build index structures that narrow down the search space. The implicit neighborhoods in such indexes also help reduce the problem of high dimensions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592aca4f",
   "metadata": {},
   "source": [
    "## Approximate Nearest Neighbor\n",
    "\n",
    "Resources\n",
    "\n",
    "- [ANNOY ANN Part 1](https://erikbern.com/2015/09/24/nearest-neighbor-methods-vector-models-part-1.html)\n",
    "- [ANNOY ANN Part 2](https://erikbern.com/2015/10/01/nearest-neighbors-and-vector-models-part-2-how-to-search-in-high-dimensional-spaces.html)\n",
    "\n",
    "\n",
    "If we care more about search speed, we need to give up accuracy for it. Let's look at a couple of approximate nearest neighbor search. Let's assume we have a set of trained embeddings for movies based on the MovieLens dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "45b5eb01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1682,)\n",
      "(1682, 64)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open('movies.pickle', 'rb') as f:\n",
    "    embeddings = pickle.load(f)\n",
    "\n",
    "print(embeddings['name'].shape)\n",
    "print(embeddings['vector'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd22954d",
   "metadata": {},
   "source": [
    "## Linear Search\n",
    "\n",
    "We can build a linear search with `faiss`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "68392c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "\n",
    "class BruteForceIndex():\n",
    "    def __init__(self, embeddings, labels):\n",
    "        self.dimension = embeddings.shape[1]\n",
    "        self.embeddings = embeddings.astype('float32')\n",
    "        self.labels = labels\n",
    "    \n",
    "    def build(self):\n",
    "        self.index = faiss.IndexFlatL2(self.dimension)\n",
    "        self.index.add(self.embeddings)\n",
    "    \n",
    "    def query(self, q, k=10):\n",
    "        return self.index.search(q, k)\n",
    "        # return distances, indices\n",
    "        \n",
    "index = BruteForceIndex(embeddings['vector'], embeddings['name'])\n",
    "index.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "454d94cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Toy Story (1995) as query\n",
      "\n",
      "- Toy Story (1995) with distance 0.0\n",
      "- Rock, The (1996) with distance 1.4708294868469238\n",
      "- Return of the Jedi (1983) with distance 1.6384427547454834\n",
      "- Willy Wonka and the Chocolate Factory (1971) with distance 1.7211445569992065\n",
      "- Phenomenon (1996) with distance 1.7770946025848389\n",
      "- Star Trek: First Contact (1996) with distance 1.8738785982131958\n",
      "- Star Wars (1977) with distance 1.8834713697433472\n",
      "- Hunchback of Notre Dame, The (1996) with distance 1.883762001991272\n",
      "- Birdcage, The (1996) with distance 1.9082722663879395\n",
      "- Mars Attacks! (1996) with distance 1.9544236660003662\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "print(f\"Query: {embeddings['name'][i]} as query\\n\")\n",
    "\n",
    "distances, indices = index.query(embeddings['vector'][i:i+1, :])\n",
    "for j in range(len(indices)):\n",
    "    candidates = embeddings['name'][indices[j]]\n",
    "    dists = distances[j]\n",
    "    for k in range(len(candidates)):\n",
    "        print(f\"- {candidates[k]} with distance {dists[k]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beeff1db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
