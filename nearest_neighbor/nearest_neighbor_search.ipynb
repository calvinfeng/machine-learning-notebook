{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3fb5ff1",
   "metadata": {},
   "source": [
    "# Nearest Neighbor Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d88c65f",
   "metadata": {},
   "source": [
    "## Define Nearest Neighbor Search\n",
    "\n",
    "Given a set of points ${x_1, ..., x_n} \\in \\mathbb{R}^d$, preprocess them into a data structure $X$ of size `polynomial(n, d)` in time `polynomial(n, d)` such that nearest neighbor queries can be performed in logarithmic time. In other words, given a search query point `q`, radius `r`, and $X$, one can return all $x_i$ such that\n",
    "\n",
    "$$\n",
    "\\left \\|\n",
    "{q - x_i} \\leq r\n",
    "\\right \\|\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79bff43",
   "metadata": {},
   "source": [
    "## Find Nearest Neighbor in 2D\n",
    "\n",
    "Let's assume that we have a random set of points `N = 100`. If we represent these points in a 2D feature space with $x_1 \\in [-1, 1]$ and $x_2 \\in [-1, 1]$. We want to find all the nearest neighbor within a circle of `radius=1`.  For the purpose of demsontration, I will perform a linear search to find all the exact matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e31b9ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "number of nearest neighbor within Euclidean distance 1.0: 41\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "N = 100\n",
    "\n",
    "def search_nn_by_dimension(num_points, dim):\n",
    "    points = np.random.randn(num_points, dim)\n",
    "    radius = 1\n",
    "    nearest = []\n",
    "    for p in points:\n",
    "        if np.sqrt(p.dot(p)) <= 1:\n",
    "            nearest.append(p)\n",
    "    return len(nearest)\n",
    "\n",
    "print(f\"\"\"\n",
    "number of nearest neighbor within Euclidean distance 1.0: {search_nn_by_dimension(N, 2)}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353fce79",
   "metadata": {},
   "source": [
    "What if I increase the dimension?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecf3e9a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D=3 number of nearest neighbor within Euclidean distance 1.0: 21\n",
      "D=4 number of nearest neighbor within Euclidean distance 1.0: 12\n",
      "D=5 number of nearest neighbor within Euclidean distance 1.0: 5\n",
      "D=6 number of nearest neighbor within Euclidean distance 1.0: 2\n",
      "D=7 number of nearest neighbor within Euclidean distance 1.0: 0\n",
      "D=8 number of nearest neighbor within Euclidean distance 1.0: 0\n",
      "D=9 number of nearest neighbor within Euclidean distance 1.0: 0\n"
     ]
    }
   ],
   "source": [
    "for d in range(3, 10):\n",
    "    print(f\"D={d} number of nearest neighbor within \" +\n",
    "          f\"Euclidean distance 1.0: {search_nn_by_dimension(N, d)}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2017a863",
   "metadata": {},
   "source": [
    "As we increase the dimensionality of feature space, the data points are getting further apart. Nearest neighbor search within a radius `r` will start to reduce its effectiveness. In order to more accurately find nearest neighbor, the search radius must increase but it will also result in higher time complexity for the exact search.\n",
    "\n",
    "> The curse of dimensionality complicates nearest neighbor search in high dimensional space. It is not possible to quickly reject candidates by using the difference in one coordinate as a lower bound for a distance based on all the dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17de68c",
   "metadata": {},
   "source": [
    "## But Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c577f0a",
   "metadata": {},
   "source": [
    "Consider feature space as a hyper-cube, the volume of a hyper-cube is simply\n",
    "\n",
    "$$\n",
    "V_{\\text{hypercube}} = s^d\n",
    "$$\n",
    "\n",
    "where $d$ is dimension and $l$ is the length of the side. What about a hypercube? It involves a gamma function $\\Gamma$.\n",
    "\n",
    "$$\n",
    "V_{\\text{hypersphere}} = \\frac{\\pi^{d/2} r^d}{\\Gamma(\\frac{d}{2} + 1)}\n",
    "$$\n",
    "\n",
    "Using the previous example, let's assume we want to search within the radius of $\\frac{s}{2}$. The points are randomly scattered in the feature space, so the probability of finding a point inside the search radius is the ratio of hypersphere volume to hypercube volume.\n",
    "\n",
    "$$\n",
    "\\frac{V_{\\text{hypersphere}}}{V_{\\text{hypercube}}} = \\frac{\\pi^{d/2}}{2^d\\Gamma(\\frac{d}{2} + 1)}\n",
    "$$\n",
    "\n",
    "What does this mean? The probability of finding points inside the search radius decreases as the dimension increases! THe rate decreases exponentially too.\n",
    "\n",
    "> There are no known exact methods for finding nearest neighbors efficiently. As both the number of points increases and the number of dimensions increase, we fall victim to the curse of dimensionality. In high dimensions, all points are almost equally distant from each other. A good enough solution for many applications is to trade accuracy for efficiency. In approximately nearest neighbors (ANN), we build index structures that narrow down the search space. The implicit neighborhoods in such indexes also help reduce the problem of high dimensions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592aca4f",
   "metadata": {},
   "source": [
    "## Approximate Nearest Neighbor\n",
    "\n",
    "Resources\n",
    "\n",
    "- [ANNOY ANN Part 1](https://erikbern.com/2015/09/24/nearest-neighbor-methods-vector-models-part-1.html)\n",
    "- [ANNOY ANN Part 2](https://erikbern.com/2015/10/01/nearest-neighbors-and-vector-models-part-2-how-to-search-in-high-dimensional-spaces.html)\n",
    "\n",
    "\n",
    "If we care more about search speed, we need to give up accuracy for it. We can preprocess the vectors into efficient indices by doing the following steps.\n",
    "\n",
    "1. Transform The Vector: Since the curse is on dimensionality, we can try to reduce the dimension with PCA.\n",
    "2. Encode The Vector: We can encode the vector into a data structure (e.g. tree, hash, quantization) for fast retrieval.\n",
    "3. Non-Exhaustive Search: We can avoid excessive searches with inverted files and neighborhood graphs.\n",
    "\n",
    "Let's look at a couple of approximate nearest neighbor search. Let's assume we have a set of trained embeddings for movies based on the MovieLens dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45b5eb01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1682,)\n",
      "(1682, 64)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open('movies.pickle', 'rb') as f:\n",
    "    embeddings = pickle.load(f)\n",
    "\n",
    "print(embeddings['name'].shape)\n",
    "print(embeddings['vector'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd22954d",
   "metadata": {},
   "source": [
    "## Linear Search\n",
    "\n",
    "We can build a linear search with `faiss`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68392c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "\n",
    "class BruteForceIndex():\n",
    "    def __init__(self, embeddings, labels):\n",
    "        self.dimension = embeddings.shape[1]\n",
    "        self.embeddings = embeddings.astype('float32')\n",
    "        self.labels = labels\n",
    "    \n",
    "    def build(self):\n",
    "        self.index = faiss.IndexFlatL2(self.dimension)\n",
    "        self.index.add(self.embeddings)\n",
    "    \n",
    "    def query(self, q, k=10):\n",
    "        q = q.reshape(1, self.dimension)\n",
    "        distances, indices = self.index.search(q, k)\n",
    "        candidates = []\n",
    "        for i in range(len(distances[0])):\n",
    "            dist = distances[0][i]\n",
    "            idx = indices[0][i]\n",
    "            candidates.append({\n",
    "                \"title\": self.labels[idx],\n",
    "                \"distance\": dist\n",
    "            })    \n",
    "        return candidates\n",
    "    \n",
    "\n",
    "index = BruteForceIndex(embeddings['vector'], embeddings['name'])\n",
    "index.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "454d94cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Toy Story (1995) as query\n",
      "\n",
      "Toy Story (1995) with distance 0.0\n",
      "Rock, The (1996) with distance 1.4708294868469238\n",
      "Return of the Jedi (1983) with distance 1.6384427547454834\n",
      "Willy Wonka and the Chocolate Factory (1971) with distance 1.7211445569992065\n",
      "Phenomenon (1996) with distance 1.7770946025848389\n",
      "Star Trek: First Contact (1996) with distance 1.8738785982131958\n",
      "Star Wars (1977) with distance 1.8834713697433472\n",
      "Hunchback of Notre Dame, The (1996) with distance 1.883762001991272\n",
      "Birdcage, The (1996) with distance 1.9082722663879395\n",
      "Mars Attacks! (1996) with distance 1.9544236660003662\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "print(f\"Query: {embeddings['name'][i]} as query\\n\")\n",
    "for candidate in index.query(embeddings['vector'][i]):\n",
    "    print(f\"{candidate['title']} with distance {candidate['distance']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e480b56",
   "metadata": {},
   "source": [
    "## ANNOY\n",
    "\n",
    "This is vector encoding using trees. Annoy uses forests to construct an index. Each tree is constructed by picking two points at random, split the space into two by their hyperplane. The tree keeps spliting into subspaces recursively until the points associated with a node is small enough.\n",
    "\n",
    "![Annoy Hyperplane](./assets/annoy-hyperplane.png)\n",
    "\n",
    "![Annoy Trees](./assets/annoy-trees.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2c364ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import annoy\n",
    "\n",
    "\n",
    "class AnnoyIndex():\n",
    "    def __init__(self, embeddings, labels):\n",
    "        self.dimension = embeddings.shape[1]\n",
    "        self.embeddings = embeddings.astype('float32')\n",
    "        self.labels = labels\n",
    "    \n",
    "    def build(self, num_trees=5):\n",
    "        self.index = annoy.AnnoyIndex(self.dimension, metric='angular')\n",
    "        for i, vec in enumerate(self.embeddings):\n",
    "            self.index.add_item(i, vec.tolist())\n",
    "        self.index.build(num_trees)\n",
    "    \n",
    "    def query(self, q, k=10):\n",
    "        indices = self.index.get_nns_by_vector(q.tolist(), k)\n",
    "        candidates = []\n",
    "        for i in indices:\n",
    "            candidates.append({\n",
    "                \"title\": self.labels[i]\n",
    "            })\n",
    "        return candidates\n",
    "\n",
    "\n",
    "index = AnnoyIndex(embeddings['vector'], embeddings['name'])\n",
    "index.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53d18949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Toy Story (1995) as query\n",
      "\n",
      "Toy Story (1995)\n",
      "Rock, The (1996)\n",
      "Return of the Jedi (1983)\n",
      "Star Wars (1977)\n",
      "Willy Wonka and the Chocolate Factory (1971)\n",
      "Star Trek: First Contact (1996)\n",
      "Fargo (1996)\n",
      "Twelve Monkeys (1995)\n",
      "Phenomenon (1996)\n",
      "Men in Black (1997)\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "print(f\"Query: {embeddings['name'][i]} as query\\n\")\n",
    "for candidate in index.query(embeddings['vector'][i]):\n",
    "    print(f\"{candidate['title']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23e64ef",
   "metadata": {},
   "source": [
    "### Pros\n",
    "\n",
    "- Index can be saved as files and passed around.\n",
    "- Parameters can be tuned to change accuracy/speed trade off.\n",
    "\n",
    "### Cons\n",
    "\n",
    "- No support for GPU processing.\n",
    "- No support for batch processing, every query has to be fed one by one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a43c85",
   "metadata": {},
   "source": [
    "## LSH\n",
    "\n",
    "Another approach is to use locality sensitive hashing. The hash function maps points that are nearby into the same bucket.\n",
    "\n",
    "![LSH](./assets/lsh.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d61c0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "\n",
    "class LSHIndex():\n",
    "    def __init__(self, embeddings, labels):\n",
    "        self.dimension = embeddings.shape[1]\n",
    "        self.embeddings = embeddings.astype('float32')\n",
    "        self.labels = labels\n",
    "        \n",
    "    def build(self, num_bits=128):\n",
    "        self.index = faiss.IndexLSH(self.dimension, num_bits)\n",
    "        self.index.add(self.embeddings)\n",
    "        \n",
    "    def query(self, q, k=10):\n",
    "        q = q.reshape(1, self.dimension)\n",
    "        distances, indices = self.index.search(q, k)\n",
    "        candidates = []\n",
    "        for i in range(len(distances[0])):\n",
    "            dist = distances[0][i]\n",
    "            idx = indices[0][i]\n",
    "            candidates.append({\n",
    "                \"title\": self.labels[idx],\n",
    "                \"distance\": dist\n",
    "            })    \n",
    "        return candidates\n",
    "\n",
    "\n",
    "index = LSHIndex(embeddings['vector'], embeddings['name'])\n",
    "index.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7082f0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Toy Story (1995) as query\n",
      "\n",
      "Toy Story (1995) with distance 0.0\n",
      "Rock, The (1996) with distance 26.0\n",
      "Star Wars (1977) with distance 27.0\n",
      "Return of the Jedi (1983) with distance 27.0\n",
      "Willy Wonka and the Chocolate Factory (1971) with distance 30.0\n",
      "Men in Black (1997) with distance 31.0\n",
      "Hunchback of Notre Dame, The (1996) with distance 33.0\n",
      "Fifth Element, The (1997) with distance 33.0\n",
      "Liar Liar (1997) with distance 33.0\n",
      "James and the Giant Peach (1996) with distance 34.0\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "print(f\"Query: {embeddings['name'][i]} as query\\n\")\n",
    "for candidate in index.query(embeddings['vector'][i]):\n",
    "    print(f\"{candidate['title']} with distance {candidate['distance']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4a2d28",
   "metadata": {},
   "source": [
    "### Pros\n",
    "\n",
    "- Accuracy of approximate search can be tuned without rebuilding the data structure.\n",
    "- Data distribution does not need to be sparse.\n",
    "\n",
    "### Cons\n",
    "\n",
    "- No support for GPU processing.\n",
    "- Requires a lot of RAM.\n",
    "- Algorithm might run slower than a linear scan."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318bcdbb",
   "metadata": {},
   "source": [
    "## Hierarchical Navigable Small World Graphs\n",
    "\n",
    "Networks with logrithmic or polylogarithmic scaling of the greedy graph routing are known as the navigable small world networks. In layman, a random graph have points equally distributed across a feature space while a small world graph tends to have points clustered. \n",
    "\n",
    "> A small world network is a type of mathematical graph in which most nodes are not neighbors of one another, but the neighbors of any given node are likely to be neighbors of each other and most nodes can be reached from every other node by a small number of hops of steps.\n",
    "\n",
    "This strongly resembles a social network. Friends are typically clustered together in one social group, but social groups are far apart from each other. This is what led to the famous 6-degree separation statement. \n",
    "\n",
    "A hierarchical small world graph is a multi-layered structure where each layer is a proximity graph. The search begins at entry point node in the highest layer and recursively perfrms a greedy graph traversal in each layer until it reaches a local minimum in the bottom most layer.\n",
    "\n",
    "![HNSW](./assets/hnsw.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "838180ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hnswlib\n",
    "\n",
    "class HNSWIndex():\n",
    "    def __init__(self, embeddings, labels):\n",
    "        self.dimension = embeddings.shape[1]\n",
    "        self.embeddings = embeddings.astype('float32')\n",
    "        self.labels = labels\n",
    "        \n",
    "    def build(self, m=100, ef_construction=10, ef=10):\n",
    "        self.index = hnswlib.Index(space='l2',\n",
    "                                    dim=self.dimension)\n",
    "        self.index.init_index(max_elements=len(self.embeddings),\n",
    "                              ef_construction=ef_construction,\n",
    "                              M=m)\n",
    "        self.index.set_ef(ef)\n",
    "        self.index.set_num_threads(4)\n",
    "        self.index.add_items(self.embeddings)\n",
    "        \n",
    "    def query(self, q, k=10):\n",
    "        q = q.reshape(1, self.dimension)\n",
    "        indices, distances = self.index.knn_query(q, k)\n",
    "        candidates = []\n",
    "        for i in range(len(distances[0])):\n",
    "            dist = distances[0][i]\n",
    "            idx = indices[0][i]\n",
    "            candidates.append({\n",
    "                \"title\": self.labels[idx],\n",
    "                \"distance\": dist\n",
    "            })    \n",
    "        return candidates\n",
    "    \n",
    "index = HNSWIndex(embeddings['vector'], embeddings['name'])\n",
    "index.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f6b65dc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Toy Story (1995) as query\n",
      "\n",
      "Toy Story (1995) with distance 0.0\n",
      "Rock, The (1996) with distance 1.470829725265503\n",
      "Return of the Jedi (1983) with distance 1.6384427547454834\n",
      "Willy Wonka and the Chocolate Factory (1971) with distance 1.721144437789917\n",
      "Phenomenon (1996) with distance 1.777094841003418\n",
      "Star Trek: First Contact (1996) with distance 1.8738782405853271\n",
      "Star Wars (1977) with distance 1.8834716081619263\n",
      "Hunchback of Notre Dame, The (1996) with distance 1.8837616443634033\n",
      "Birdcage, The (1996) with distance 1.90827214717865\n",
      "Mars Attacks! (1996) with distance 1.9544237852096558\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "print(f\"Query: {embeddings['name'][i]} as query\\n\")\n",
    "for candidate in index.query(embeddings['vector'][i]):\n",
    "    print(f\"{candidate['title']} with distance {candidate['distance']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bc1f57",
   "metadata": {},
   "source": [
    "The recall is 100% !!!\n",
    "\n",
    "### Pros\n",
    "\n",
    "- Parameters can be tuned to change accuracy/speed tradeoff.\n",
    "- Support batch queries.\n",
    "- It outperforms many rival algorithms on real-world datasets, look at the recall!\n",
    "\n",
    "### Cons\n",
    "\n",
    "- It requires a lot of RAM.\n",
    "- It is difficult to incrementally add more points due to the max size limit during initialization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
