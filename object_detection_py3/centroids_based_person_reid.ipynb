{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf06657d",
   "metadata": {},
   "source": [
    "# Centroids-based Person Re-identification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930c71a6",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Person re-identification is essentailly a person retrieval problem across non-overlapping cameras. Given a query image, find the same person in multiple footage coming from different cameras at different time. This is commonly applied in video survelliance system. The early research tended to focus on hand-crafted feature construction using body structures and poses with distance metric learning (e.g. find the two features with miminal Euclidean distance in feature space.) More recently, the features are done via embedding in deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a57c13",
   "metadata": {},
   "source": [
    "## Approach to Features\n",
    "\n",
    "- **Global Feature Representation Learning** extracts a global feature vector for each person image.\n",
    "- **Local Feature Representation Learning** learns part/region aggregated features, e.g. one embedding for face, one mebedding for upper torso, and etc...\n",
    "- **Auxiliary Feature Representation Learning** maps person image to annotated attributes such as gender, age, clothing color, and etc...\n",
    "- **Video Feature Representation Learning** factors in temporal information like how does a person move across the frame."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f461bd22",
   "metadata": {},
   "source": [
    "## Approach to Training Objectives\n",
    "\n",
    "- Identity Loss\n",
    "- Verification Loss\n",
    "- Triplet Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f8de87",
   "metadata": {},
   "source": [
    "### Identity Loss\n",
    "\n",
    "This treats re-identification as a classification problem using categorical cross entropy loss. Each person is considered a distinct class. Given an input image $x_i$ with label $y_i$, the model predicts probability of $x_i$ being recognized as class $y_i$. The loss function encodes probability with softmax.\n",
    "\n",
    "$$\n",
    "L = \\frac{-1}{N} \\Sigma^{N}_{i=1} \\text{log}(p(y_i \\mid x_i ))\n",
    "$$\n",
    "\n",
    "However this approach is hard to scale with the increasing number of identities in the dataset. There may be tens of millions of identities. The fully connected layer that holds the embeddings will be too big to fit in one GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f59ae0",
   "metadata": {},
   "source": [
    "### Verification Loss\n",
    "\n",
    "This is sometimes called the _pair-wise loss_. It optimizes the pairwise relationship, either with a constrastive loss or binary verification loss.\n",
    "\n",
    "The contrastive loss improves the relative pairwise distance between two non-matching sample, i.e. two different people.\n",
    "\n",
    "$$\n",
    "L(i, j) = (1 - \\delta_{ij}) \\cdot max(0, \\rho - d_{ij})^2 + \\delta_{ij}d^2_{ij}\n",
    "$$\n",
    "\n",
    "$d_{ij}$ represents the Euclidean distance between the embedding features of two inputs $x_i$ and $x_j$. $\\delta_{ij}$ is 1 when two inputs belong to the same identity, and 0 when two inputs belong to different identities. $\\rho$ is a margin parameter.\n",
    "\n",
    "In other words:\n",
    "\n",
    "> When two inputs belong to the same identity, the loss is measured by their distance away from each other. Lower the distance, lower the loss.\n",
    "\n",
    "> When two inputs belong to different identities, the loss is measured by how close is distance to margin. The loss is lower then the distance betweeo two features are close to $\\rho$, the margin.\n",
    "\n",
    "On the other hand, verification loss is a binary classifier that predicts whether two inputs belong to the same identity, with 0 means no and 1 means yes. This becomes a binary cross entropy loss.\n",
    "\n",
    "$$\n",
    "L(i, j) = \\left[ -\\delta_{ij} log (p(\\delta_{ij} \\mid f_{ij}) \\right] - \\left[(1 - \\delta_{ij}) log(1 - p(\\delta_{ij} \\mid f_{ij}))\\right]\n",
    "$$\n",
    "\n",
    "where $f_{ij}$ is called the differential feature.\n",
    "\n",
    "$$\n",
    "f_{ij} = (f_{i} - f_{j})^2\n",
    "$$\n",
    "\n",
    "The verification is often combined with identity loss to improve performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b427787b",
   "metadata": {},
   "source": [
    "###  Triplet Loss\n",
    "\n",
    "This treats re-identification as a retrieval ranking problem. The basic idea is that the distance between positive pair should be smaller than the negative pair by a pre-defined margin. A triplet contains one anchor sample $x_i$, one positive sample $x_j$, and one negative sample $x_k$. \n",
    "\n",
    "$$\n",
    "L(i, j, k) = max(\\rho + d_{ij} - d_{ik}, 0)\n",
    "$$\n",
    "\n",
    "> Loss is minimized when distance between i and j is very small, and distance between i and k is greater than margin.\n",
    "\n",
    "However if the model directly optimizes this loss function, a large portion of easy triplets will dominate the training process, such as two images that are vastly different. The model will result in limited discriminability, i.e. it can easily confuse two similar persons. The solution is to hand pick informative triplets which can be quite tedious."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25860ca4",
   "metadata": {},
   "source": [
    "# Centroids Triplet Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2431f8c1",
   "metadata": {},
   "source": [
    "In [On the Unreasonable Effectiveness of Centroids in Image Retrieval](https://arxiv.org/abs/2104.13643), the authors propose that triplet loss can be enhanced by comparising query to centroids of postive and negative samples instead of individual embeddings.\n",
    "\n",
    "There are few issues with the vanilla triplet loss.\n",
    "\n",
    "1. Hard negative sampling may lead to bad local minima, basically overfit to few hand-selected examples. (Refer to above)\n",
    "2. Hard negative sampling is computationally expensive.\n",
    "3. Triplet loss is prone to outliers and noisy labels.\n",
    "\n",
    "> To alleviate problems stemming from the point to point nature of Triplet Loss, changes to point-to-set/point-to-centroid formulations were proposed, where the distances are measured between a sample and a prototype/centroid representing a class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f996fd",
   "metadata": {},
   "source": [
    "$$\n",
    "L_{triplet} = (f_A - c_P )^2 - (f_A - c_N)^2 + \\rho\n",
    "$$\n",
    "\n",
    "$f_A$ is the embedding for anchor. $c_P$ is the centroid to positive class, $c_N$ is the centroid to negative class, and $rho$ is the margin, same as above.\n",
    "\n",
    "![CTL Model](./assets/ctl-model.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cddcd841",
   "metadata": {},
   "source": [
    "For each class in the batch, center loss computes the center of that particular class and the minimize the distance of each sample of the class from this center. It increases inter-class separability and intra-class compactness. [A Discriminative Feature Learning Approach for Deep Face Recognition](https://kpzhang93.github.io/papers/eccv2016.pdf) is a famous example."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
